# Generated from SparkSQL/SparkSQL.g4 by ANTLR 4.8
# encoding: utf-8
from antlr4 import *
from io import StringIO
import sys
if sys.version_info[1] > 5:
	from typing import TextIO
else:
	from typing.io import TextIO


def serializedATN():
    with StringIO() as buf:
        buf.write("\3\u608b\ua72a\u8133\ub9ed\u417c\u3be7\u7786\u5964\3\u0128")
        buf.write("\u0bb9\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7")
        buf.write("\4\b\t\b\4\t\t\t\4\n\t\n\4\13\t\13\4\f\t\f\4\r\t\r\4\16")
        buf.write("\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22\t\22\4\23\t\23")
        buf.write("\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31")
        buf.write("\t\31\4\32\t\32\4\33\t\33\4\34\t\34\4\35\t\35\4\36\t\36")
        buf.write("\4\37\t\37\4 \t \4!\t!\4\"\t\"\4#\t#\4$\t$\4%\t%\4&\t")
        buf.write("&\4\'\t\'\4(\t(\4)\t)\4*\t*\4+\t+\4,\t,\4-\t-\4.\t.\4")
        buf.write("/\t/\4\60\t\60\4\61\t\61\4\62\t\62\4\63\t\63\4\64\t\64")
        buf.write("\4\65\t\65\4\66\t\66\4\67\t\67\48\t8\49\t9\4:\t:\4;\t")
        buf.write(";\4<\t<\4=\t=\4>\t>\4?\t?\4@\t@\4A\tA\4B\tB\4C\tC\4D\t")
        buf.write("D\4E\tE\4F\tF\4G\tG\4H\tH\4I\tI\4J\tJ\4K\tK\4L\tL\4M\t")
        buf.write("M\4N\tN\4O\tO\4P\tP\4Q\tQ\4R\tR\4S\tS\4T\tT\4U\tU\4V\t")
        buf.write("V\4W\tW\4X\tX\4Y\tY\4Z\tZ\4[\t[\4\\\t\\\4]\t]\4^\t^\4")
        buf.write("_\t_\4`\t`\4a\ta\4b\tb\4c\tc\4d\td\4e\te\4f\tf\4g\tg\4")
        buf.write("h\th\4i\ti\4j\tj\4k\tk\4l\tl\4m\tm\4n\tn\4o\to\4p\tp\4")
        buf.write("q\tq\4r\tr\4s\ts\4t\tt\4u\tu\4v\tv\4w\tw\4x\tx\4y\ty\4")
        buf.write("z\tz\4{\t{\4|\t|\4}\t}\4~\t~\4\177\t\177\4\u0080\t\u0080")
        buf.write("\4\u0081\t\u0081\4\u0082\t\u0082\4\u0083\t\u0083\4\u0084")
        buf.write("\t\u0084\4\u0085\t\u0085\4\u0086\t\u0086\4\u0087\t\u0087")
        buf.write("\4\u0088\t\u0088\3\2\3\2\7\2\u0113\n\2\f\2\16\2\u0116")
        buf.write("\13\2\3\2\3\2\3\3\3\3\3\3\3\4\3\4\3\4\3\5\3\5\3\5\3\6")
        buf.write("\3\6\3\6\3\7\3\7\3\7\3\b\3\b\3\b\3\t\3\t\5\t\u012e\n\t")
        buf.write("\3\t\3\t\3\t\5\t\u0133\n\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t")
        buf.write("\u013b\n\t\3\t\3\t\3\t\3\t\3\t\3\t\7\t\u0143\n\t\f\t\16")
        buf.write("\t\u0146\13\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u0159\n\t\3\t\3\t\5\t")
        buf.write("\u015d\n\t\3\t\3\t\3\t\3\t\5\t\u0163\n\t\3\t\5\t\u0166")
        buf.write("\n\t\3\t\5\t\u0169\n\t\3\t\3\t\3\t\3\t\3\t\5\t\u0170\n")
        buf.write("\t\3\t\3\t\3\t\5\t\u0175\n\t\3\t\5\t\u0178\n\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\5\t\u017f\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\5\t\u018b\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\7\t\u0194\n\t\f\t\16\t\u0197\13\t\3\t\5\t\u019a\n\t\3")
        buf.write("\t\5\t\u019d\n\t\3\t\3\t\3\t\3\t\3\t\5\t\u01a4\n\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\7\t\u01af\n\t\f\t\16")
        buf.write("\t\u01b2\13\t\3\t\3\t\3\t\3\t\3\t\5\t\u01b9\n\t\3\t\3")
        buf.write("\t\3\t\5\t\u01be\n\t\3\t\5\t\u01c1\n\t\3\t\3\t\3\t\3\t")
        buf.write("\5\t\u01c7\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t")
        buf.write("\u01d2\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u0212\n")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u021b\n\t\3\t\3\t\5")
        buf.write("\t\u021f\n\t\3\t\3\t\3\t\3\t\5\t\u0225\n\t\3\t\3\t\5\t")
        buf.write("\u0229\n\t\3\t\3\t\3\t\5\t\u022e\n\t\3\t\3\t\3\t\3\t\5")
        buf.write("\t\u0234\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\5")
        buf.write("\t\u0240\n\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u0248\n\t\3\t")
        buf.write("\3\t\3\t\3\t\5\t\u024e\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\5\t\u025b\n\t\3\t\6\t\u025e\n\t\r\t\16")
        buf.write("\t\u025f\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\5\t\u0270\n\t\3\t\3\t\3\t\7\t\u0275\n\t\f\t")
        buf.write("\16\t\u0278\13\t\3\t\5\t\u027b\n\t\3\t\3\t\3\t\3\t\5\t")
        buf.write("\u0281\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\5\t\u0290\n\t\3\t\3\t\5\t\u0294\n\t\3\t\3\t\3")
        buf.write("\t\3\t\5\t\u029a\n\t\3\t\3\t\3\t\3\t\5\t\u02a0\n\t\3\t")
        buf.write("\5\t\u02a3\n\t\3\t\5\t\u02a6\n\t\3\t\3\t\3\t\3\t\5\t\u02ac")
        buf.write("\n\t\3\t\3\t\5\t\u02b0\n\t\3\t\3\t\3\t\3\t\3\t\3\t\7\t")
        buf.write("\u02b8\n\t\f\t\16\t\u02bb\13\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\5\t\u02c3\n\t\3\t\5\t\u02c6\n\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\5\t\u02cf\n\t\3\t\3\t\3\t\5\t\u02d4\n\t\3\t\3\t")
        buf.write("\3\t\3\t\5\t\u02da\n\t\3\t\3\t\3\t\3\t\3\t\5\t\u02e1\n")
        buf.write("\t\3\t\5\t\u02e4\n\t\3\t\3\t\3\t\3\t\5\t\u02ea\n\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\7\t\u02f3\n\t\f\t\16\t\u02f6")
        buf.write("\13\t\5\t\u02f8\n\t\3\t\3\t\5\t\u02fc\n\t\3\t\3\t\3\t")
        buf.write("\5\t\u0301\n\t\3\t\3\t\3\t\5\t\u0306\n\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\5\t\u030d\n\t\3\t\5\t\u0310\n\t\3\t\5\t\u0313\n")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\5\t\u031a\n\t\3\t\3\t\3\t\5\t\u031f")
        buf.write("\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u0328\n\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\5\t\u0330\n\t\3\t\3\t\3\t\3\t\5\t\u0336")
        buf.write("\n\t\3\t\5\t\u0339\n\t\3\t\5\t\u033c\n\t\3\t\3\t\3\t\3")
        buf.write("\t\5\t\u0342\n\t\3\t\3\t\5\t\u0346\n\t\3\t\3\t\5\t\u034a")
        buf.write("\n\t\3\t\3\t\5\t\u034e\n\t\5\t\u0350\n\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\5\t\u0358\n\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u0360")
        buf.write("\n\t\3\t\3\t\3\t\3\t\5\t\u0366\n\t\3\t\3\t\3\t\3\t\5\t")
        buf.write("\u036c\n\t\3\t\5\t\u036f\n\t\3\t\3\t\5\t\u0373\n\t\3\t")
        buf.write("\5\t\u0376\n\t\3\t\3\t\5\t\u037a\n\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\7\t\u0394\n\t\f\t\16\t\u0397")
        buf.write("\13\t\5\t\u0399\n\t\3\t\3\t\5\t\u039d\n\t\3\t\3\t\3\t")
        buf.write("\3\t\5\t\u03a3\n\t\3\t\5\t\u03a6\n\t\3\t\5\t\u03a9\n\t")
        buf.write("\3\t\3\t\3\t\3\t\5\t\u03af\n\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\5\t\u03b7\n\t\3\t\3\t\3\t\5\t\u03bc\n\t\3\t\3\t\3\t\3")
        buf.write("\t\5\t\u03c2\n\t\3\t\3\t\3\t\3\t\5\t\u03c8\n\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\7\t\u03d2\n\t\f\t\16\t\u03d5")
        buf.write("\13\t\5\t\u03d7\n\t\3\t\3\t\3\t\7\t\u03dc\n\t\f\t\16\t")
        buf.write("\u03df\13\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\7\t\u03ed\n\t\f\t\16\t\u03f0\13\t\3\t\3\t\3\t\3")
        buf.write("\t\7\t\u03f6\n\t\f\t\16\t\u03f9\13\t\5\t\u03fb\n\t\3\t")
        buf.write("\3\t\7\t\u03ff\n\t\f\t\16\t\u0402\13\t\3\t\3\t\3\t\3\t")
        buf.write("\7\t\u0408\n\t\f\t\16\t\u040b\13\t\3\t\3\t\7\t\u040f\n")
        buf.write("\t\f\t\16\t\u0412\13\t\5\t\u0414\n\t\3\n\3\n\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\5\13\u041e\n\13\3\13\3\13\5\13\u0422")
        buf.write("\n\13\3\13\3\13\3\13\3\13\3\13\5\13\u0429\n\13\3\13\3")
        buf.write("\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\5\13\u049d\n\13\3\13\3\13\3\13\3\13\3")
        buf.write("\13\3\13\5\13\u04a5\n\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\5\13\u04ad\n\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\5")
        buf.write("\13\u04b6\n\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13")
        buf.write("\5\13\u04c0\n\13\3\f\3\f\5\f\u04c4\n\f\3\f\5\f\u04c7\n")
        buf.write("\f\3\f\3\f\3\f\3\f\5\f\u04cd\n\f\3\f\3\f\3\r\3\r\5\r\u04d3")
        buf.write("\n\r\3\r\3\r\3\r\3\r\3\16\3\16\3\16\3\16\3\16\3\16\5\16")
        buf.write("\u04df\n\16\3\16\3\16\3\16\3\16\3\17\3\17\3\17\3\17\3")
        buf.write("\17\3\17\5\17\u04eb\n\17\3\17\3\17\3\17\5\17\u04f0\n\17")
        buf.write("\3\20\3\20\3\20\3\21\3\21\3\21\3\22\5\22\u04f9\n\22\3")
        buf.write("\22\3\22\3\22\3\23\3\23\3\23\5\23\u0501\n\23\3\23\3\23")
        buf.write("\3\23\3\23\3\23\5\23\u0508\n\23\5\23\u050a\n\23\3\23\3")
        buf.write("\23\3\23\5\23\u050f\n\23\3\23\3\23\5\23\u0513\n\23\3\23")
        buf.write("\3\23\3\23\5\23\u0518\n\23\3\23\3\23\3\23\5\23\u051d\n")
        buf.write("\23\3\23\3\23\3\23\5\23\u0522\n\23\3\23\5\23\u0525\n\23")
        buf.write("\3\23\3\23\3\23\5\23\u052a\n\23\3\23\3\23\5\23\u052e\n")
        buf.write("\23\3\23\3\23\3\23\5\23\u0533\n\23\5\23\u0535\n\23\3\24")
        buf.write("\3\24\5\24\u0539\n\24\3\25\3\25\3\25\3\25\3\25\7\25\u0540")
        buf.write("\n\25\f\25\16\25\u0543\13\25\3\25\3\25\3\26\3\26\3\26")
        buf.write("\5\26\u054a\n\26\3\27\3\27\3\30\3\30\3\30\3\30\3\30\5")
        buf.write("\30\u0553\n\30\3\31\3\31\3\31\7\31\u0558\n\31\f\31\16")
        buf.write("\31\u055b\13\31\3\32\3\32\3\32\3\32\7\32\u0561\n\32\f")
        buf.write("\32\16\32\u0564\13\32\3\33\3\33\5\33\u0568\n\33\3\33\5")
        buf.write("\33\u056b\n\33\3\33\3\33\3\33\3\33\3\34\3\34\3\34\3\35")
        buf.write("\3\35\3\35\3\35\3\35\3\35\3\35\3\35\3\35\3\35\7\35\u057e")
        buf.write("\n\35\f\35\16\35\u0581\13\35\3\36\3\36\3\36\3\36\7\36")
        buf.write("\u0587\n\36\f\36\16\36\u058a\13\36\3\36\3\36\3\37\3\37")
        buf.write("\5\37\u0590\n\37\3\37\5\37\u0593\n\37\3 \3 \3 \7 \u0598")
        buf.write("\n \f \16 \u059b\13 \3 \5 \u059e\n \3!\3!\3!\3!\5!\u05a4")
        buf.write("\n!\3\"\3\"\3\"\3\"\7\"\u05aa\n\"\f\"\16\"\u05ad\13\"")
        buf.write("\3\"\3\"\3#\3#\3#\3#\7#\u05b5\n#\f#\16#\u05b8\13#\3#\3")
        buf.write("#\3$\3$\3$\3$\3$\3$\5$\u05c2\n$\3%\3%\3%\3%\3%\5%\u05c9")
        buf.write("\n%\3&\3&\3&\3&\5&\u05cf\n&\3\'\3\'\3\'\3(\3(\3(\3(\3")
        buf.write("(\3(\6(\u05da\n(\r(\16(\u05db\3(\3(\3(\3(\3(\5(\u05e3")
        buf.write("\n(\3(\3(\3(\3(\3(\5(\u05ea\n(\3(\3(\3(\3(\3(\3(\3(\3")
        buf.write("(\3(\3(\5(\u05f6\n(\3(\3(\3(\3(\7(\u05fc\n(\f(\16(\u05ff")
        buf.write("\13(\3(\7(\u0602\n(\f(\16(\u0605\13(\5(\u0607\n(\3)\3")
        buf.write(")\3)\3)\3)\7)\u060e\n)\f)\16)\u0611\13)\5)\u0613\n)\3")
        buf.write(")\3)\3)\3)\3)\7)\u061a\n)\f)\16)\u061d\13)\5)\u061f\n")
        buf.write(")\3)\3)\3)\3)\3)\7)\u0626\n)\f)\16)\u0629\13)\5)\u062b")
        buf.write("\n)\3)\3)\3)\3)\3)\7)\u0632\n)\f)\16)\u0635\13)\5)\u0637")
        buf.write("\n)\3)\5)\u063a\n)\3)\3)\3)\5)\u063f\n)\5)\u0641\n)\3")
        buf.write("*\3*\3*\3+\3+\3+\3+\3+\3+\5+\u064c\n+\3+\3+\3+\3+\5+\u0652")
        buf.write("\n+\3+\7+\u0655\n+\f+\16+\u0658\13+\3,\3,\3,\3,\3,\3,")
        buf.write("\3,\3,\3,\5,\u0663\n,\3-\3-\5-\u0667\n-\3-\3-\5-\u066b")
        buf.write("\n-\3.\3.\6.\u066f\n.\r.\16.\u0670\3/\3/\5/\u0675\n/\3")
        buf.write("/\3/\3/\3/\7/\u067b\n/\f/\16/\u067e\13/\3/\5/\u0681\n")
        buf.write("/\3/\5/\u0684\n/\3/\5/\u0687\n/\3/\5/\u068a\n/\3/\3/\5")
        buf.write("/\u068e\n/\3\60\3\60\5\60\u0692\n\60\3\60\5\60\u0695\n")
        buf.write("\60\3\60\3\60\5\60\u0699\n\60\3\60\7\60\u069c\n\60\f\60")
        buf.write("\16\60\u069f\13\60\3\60\5\60\u06a2\n\60\3\60\5\60\u06a5")
        buf.write("\n\60\3\60\5\60\u06a8\n\60\3\60\5\60\u06ab\n\60\5\60\u06ad")
        buf.write("\n\60\3\61\3\61\3\61\3\61\3\61\3\61\3\61\3\61\3\61\3\61")
        buf.write("\5\61\u06b9\n\61\3\61\5\61\u06bc\n\61\3\61\3\61\5\61\u06c0")
        buf.write("\n\61\3\61\3\61\3\61\3\61\3\61\3\61\3\61\3\61\5\61\u06ca")
        buf.write("\n\61\3\61\3\61\5\61\u06ce\n\61\5\61\u06d0\n\61\3\61\5")
        buf.write("\61\u06d3\n\61\3\61\3\61\5\61\u06d7\n\61\3\62\3\62\7\62")
        buf.write("\u06db\n\62\f\62\16\62\u06de\13\62\3\62\5\62\u06e1\n\62")
        buf.write("\3\62\3\62\3\63\3\63\3\63\3\64\3\64\3\64\3\64\5\64\u06ec")
        buf.write("\n\64\3\64\3\64\3\64\3\65\3\65\3\65\3\65\3\65\5\65\u06f6")
        buf.write("\n\65\3\65\3\65\3\65\3\66\3\66\3\66\3\66\3\66\3\66\3\66")
        buf.write("\5\66\u0702\n\66\3\67\3\67\3\67\3\67\3\67\3\67\3\67\3")
        buf.write("\67\3\67\3\67\3\67\7\67\u070f\n\67\f\67\16\67\u0712\13")
        buf.write("\67\3\67\3\67\5\67\u0716\n\67\38\38\38\78\u071b\n8\f8")
        buf.write("\168\u071e\138\39\39\39\39\3:\3:\3:\3;\3;\3;\3<\3<\3<")
        buf.write("\5<\u072d\n<\3<\7<\u0730\n<\f<\16<\u0733\13<\3<\3<\3=")
        buf.write("\3=\3=\3=\3=\3=\7=\u073d\n=\f=\16=\u0740\13=\3=\3=\5=")
        buf.write("\u0744\n=\3>\3>\3>\3>\7>\u074a\n>\f>\16>\u074d\13>\3>")
        buf.write("\7>\u0750\n>\f>\16>\u0753\13>\3>\5>\u0756\n>\3?\3?\3?")
        buf.write("\3?\3?\7?\u075d\n?\f?\16?\u0760\13?\3?\3?\3?\3?\3?\3?")
        buf.write("\3?\3?\3?\3?\7?\u076c\n?\f?\16?\u076f\13?\3?\3?\5?\u0773")
        buf.write("\n?\3?\3?\3?\3?\3?\3?\3?\3?\7?\u077d\n?\f?\16?\u0780\13")
        buf.write("?\3?\3?\5?\u0784\n?\3@\3@\3@\3@\7@\u078a\n@\f@\16@\u078d")
        buf.write("\13@\5@\u078f\n@\3@\3@\5@\u0793\n@\3A\3A\3A\3A\3A\3A\3")
        buf.write("A\3A\3A\3A\7A\u079f\nA\fA\16A\u07a2\13A\3A\3A\3A\3B\3")
        buf.write("B\3B\3B\3B\7B\u07ac\nB\fB\16B\u07af\13B\3B\3B\5B\u07b3")
        buf.write("\nB\3C\3C\5C\u07b7\nC\3C\5C\u07ba\nC\3D\3D\3D\5D\u07bf")
        buf.write("\nD\3D\3D\3D\3D\3D\7D\u07c6\nD\fD\16D\u07c9\13D\5D\u07cb")
        buf.write("\nD\3D\3D\3D\5D\u07d0\nD\3D\3D\3D\7D\u07d5\nD\fD\16D\u07d8")
        buf.write("\13D\5D\u07da\nD\3E\3E\3F\3F\7F\u07e0\nF\fF\16F\u07e3")
        buf.write("\13F\3G\3G\3G\3G\5G\u07e9\nG\3G\3G\3G\3G\3G\5G\u07f0\n")
        buf.write("G\3H\5H\u07f3\nH\3H\3H\3H\5H\u07f8\nH\3H\5H\u07fb\nH\3")
        buf.write("H\3H\3H\5H\u0800\nH\3H\3H\5H\u0804\nH\3H\5H\u0807\nH\3")
        buf.write("H\5H\u080a\nH\3I\3I\3I\3I\5I\u0810\nI\3J\3J\3J\5J\u0815")
        buf.write("\nJ\3J\3J\3K\5K\u081a\nK\3K\3K\3K\3K\3K\3K\3K\3K\3K\3")
        buf.write("K\3K\3K\3K\3K\3K\3K\5K\u082c\nK\5K\u082e\nK\3K\5K\u0831")
        buf.write("\nK\3L\3L\3L\3L\3M\3M\3M\7M\u083a\nM\fM\16M\u083d\13M")
        buf.write("\3N\3N\3N\3N\7N\u0843\nN\fN\16N\u0846\13N\3N\3N\3O\3O")
        buf.write("\5O\u084c\nO\3P\3P\3P\3P\7P\u0852\nP\fP\16P\u0855\13P")
        buf.write("\3P\3P\3Q\3Q\5Q\u085b\nQ\3R\3R\5R\u085f\nR\3R\3R\3R\3")
        buf.write("R\3R\3R\5R\u0867\nR\3R\3R\3R\3R\3R\3R\5R\u086f\nR\3R\3")
        buf.write("R\3R\3R\5R\u0875\nR\3S\3S\3S\3S\7S\u087b\nS\fS\16S\u087e")
        buf.write("\13S\3S\3S\3T\3T\3T\3T\3T\7T\u0887\nT\fT\16T\u088a\13")
        buf.write("T\5T\u088c\nT\3T\3T\3T\3U\5U\u0892\nU\3U\3U\5U\u0896\n")
        buf.write("U\5U\u0898\nU\3V\3V\3V\3V\3V\3V\3V\5V\u08a1\nV\3V\3V\3")
        buf.write("V\3V\3V\3V\3V\3V\3V\3V\5V\u08ad\nV\5V\u08af\nV\3V\3V\3")
        buf.write("V\3V\3V\5V\u08b6\nV\3V\3V\3V\3V\3V\5V\u08bd\nV\3V\3V\3")
        buf.write("V\3V\5V\u08c3\nV\3V\3V\3V\3V\5V\u08c9\nV\5V\u08cb\nV\3")
        buf.write("W\3W\3W\7W\u08d0\nW\fW\16W\u08d3\13W\3X\3X\3X\7X\u08d8")
        buf.write("\nX\fX\16X\u08db\13X\3Y\3Y\3Y\5Y\u08e0\nY\3Y\3Y\3Z\3Z")
        buf.write("\3Z\5Z\u08e7\nZ\3Z\3Z\3[\3[\5[\u08ed\n[\3[\3[\5[\u08f1")
        buf.write("\n[\5[\u08f3\n[\3\\\3\\\3\\\7\\\u08f8\n\\\f\\\16\\\u08fb")
        buf.write("\13\\\3]\3]\3]\3]\7]\u0901\n]\f]\16]\u0904\13]\3]\3]\3")
        buf.write("^\3^\3^\3^\3^\3^\7^\u090e\n^\f^\16^\u0911\13^\3^\3^\5")
        buf.write("^\u0915\n^\3_\3_\5_\u0919\n_\3`\3`\3a\3a\3a\3a\3a\3a\3")
        buf.write("a\3a\3a\3a\5a\u0927\na\5a\u0929\na\3a\3a\3a\3a\3a\3a\7")
        buf.write("a\u0931\na\fa\16a\u0934\13a\3b\5b\u0937\nb\3b\3b\3b\3")
        buf.write("b\3b\3b\5b\u093f\nb\3b\3b\3b\3b\3b\7b\u0946\nb\fb\16b")
        buf.write("\u0949\13b\3b\3b\3b\5b\u094e\nb\3b\3b\3b\3b\3b\3b\5b\u0956")
        buf.write("\nb\3b\3b\3b\5b\u095b\nb\3b\3b\3b\3b\3b\3b\3b\3b\7b\u0965")
        buf.write("\nb\fb\16b\u0968\13b\3b\3b\5b\u096c\nb\3b\5b\u096f\nb")
        buf.write("\3b\3b\3b\3b\5b\u0975\nb\3b\3b\5b\u0979\nb\3b\3b\3b\5")
        buf.write("b\u097e\nb\3b\3b\3b\5b\u0983\nb\3b\3b\3b\5b\u0988\nb\3")
        buf.write("c\3c\3c\3c\5c\u098e\nc\3c\3c\3c\3c\3c\3c\3c\3c\3c\3c\3")
        buf.write("c\3c\3c\3c\3c\3c\3c\3c\3c\7c\u09a3\nc\fc\16c\u09a6\13")
        buf.write("c\3d\3d\3d\3d\6d\u09ac\nd\rd\16d\u09ad\3d\3d\5d\u09b2")
        buf.write("\nd\3d\3d\3d\3d\3d\6d\u09b9\nd\rd\16d\u09ba\3d\3d\5d\u09bf")
        buf.write("\nd\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\7d\u09cf")
        buf.write("\nd\fd\16d\u09d2\13d\5d\u09d4\nd\3d\3d\3d\3d\3d\3d\5d")
        buf.write("\u09dc\nd\3d\3d\3d\3d\3d\3d\3d\5d\u09e5\nd\3d\3d\3d\3")
        buf.write("d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\6d\u09fa")
        buf.write("\nd\rd\16d\u09fb\3d\3d\3d\3d\3d\3d\3d\3d\3d\5d\u0a07\n")
        buf.write("d\3d\3d\3d\7d\u0a0c\nd\fd\16d\u0a0f\13d\5d\u0a11\nd\3")
        buf.write("d\3d\3d\3d\3d\3d\3d\5d\u0a1a\nd\3d\3d\5d\u0a1e\nd\3d\3")
        buf.write("d\3d\3d\3d\3d\3d\3d\6d\u0a28\nd\rd\16d\u0a29\3d\3d\3d")
        buf.write("\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3")
        buf.write("d\3d\3d\5d\u0a43\nd\3d\3d\3d\3d\3d\5d\u0a4a\nd\3d\5d\u0a4d")
        buf.write("\nd\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\3d\5d\u0a5c\n")
        buf.write("d\3d\3d\5d\u0a60\nd\3d\3d\3d\3d\3d\3d\3d\3d\7d\u0a6a\n")
        buf.write("d\fd\16d\u0a6d\13d\3e\3e\3e\3e\3e\3e\3e\3e\6e\u0a77\n")
        buf.write("e\re\16e\u0a78\5e\u0a7b\ne\3f\3f\3g\3g\3h\3h\3i\3i\3j")
        buf.write("\3j\3j\5j\u0a88\nj\3k\3k\5k\u0a8c\nk\3l\3l\3l\6l\u0a91")
        buf.write("\nl\rl\16l\u0a92\3m\3m\3m\5m\u0a98\nm\3n\3n\3n\3n\3n\3")
        buf.write("o\5o\u0aa0\no\3o\3o\5o\u0aa4\no\3p\3p\3p\5p\u0aa9\np\3")
        buf.write("q\3q\3q\3q\3q\3q\3q\3q\3q\3q\3q\3q\3q\3q\3q\5q\u0aba\n")
        buf.write("q\3q\3q\5q\u0abe\nq\3q\3q\3q\3q\3q\7q\u0ac5\nq\fq\16q")
        buf.write("\u0ac8\13q\3q\5q\u0acb\nq\5q\u0acd\nq\3r\3r\3r\7r\u0ad2")
        buf.write("\nr\fr\16r\u0ad5\13r\3s\3s\3s\3s\5s\u0adb\ns\3s\5s\u0ade")
        buf.write("\ns\3s\5s\u0ae1\ns\3t\3t\3t\7t\u0ae6\nt\ft\16t\u0ae9\13")
        buf.write("t\3u\3u\3u\3u\5u\u0aef\nu\3u\5u\u0af2\nu\3v\3v\3v\7v\u0af7")
        buf.write("\nv\fv\16v\u0afa\13v\3w\3w\3w\3w\3w\5w\u0b01\nw\3w\5w")
        buf.write("\u0b04\nw\3x\3x\3x\3x\3x\3y\3y\3y\3y\7y\u0b0f\ny\fy\16")
        buf.write("y\u0b12\13y\3z\3z\3z\3z\3{\3{\3{\3{\3{\3{\3{\3{\3{\3{")
        buf.write("\3{\7{\u0b23\n{\f{\16{\u0b26\13{\3{\3{\3{\3{\3{\7{\u0b2d")
        buf.write("\n{\f{\16{\u0b30\13{\5{\u0b32\n{\3{\3{\3{\3{\3{\7{\u0b39")
        buf.write("\n{\f{\16{\u0b3c\13{\5{\u0b3e\n{\5{\u0b40\n{\3{\5{\u0b43")
        buf.write("\n{\3{\5{\u0b46\n{\3|\3|\3|\3|\3|\3|\3|\3|\3|\3|\3|\3")
        buf.write("|\3|\3|\3|\3|\5|\u0b58\n|\3}\3}\3}\3}\3}\3}\3}\5}\u0b61")
        buf.write("\n}\3~\3~\3~\7~\u0b66\n~\f~\16~\u0b69\13~\3\177\3\177")
        buf.write("\3\177\3\177\5\177\u0b6f\n\177\3\u0080\3\u0080\3\u0080")
        buf.write("\7\u0080\u0b74\n\u0080\f\u0080\16\u0080\u0b77\13\u0080")
        buf.write("\3\u0081\3\u0081\3\u0082\3\u0082\5\u0082\u0b7d\n\u0082")
        buf.write("\3\u0083\3\u0083\3\u0083\5\u0083\u0b82\n\u0083\3\u0084")
        buf.write("\3\u0084\3\u0085\5\u0085\u0b87\n\u0085\3\u0085\3\u0085")
        buf.write("\5\u0085\u0b8b\n\u0085\3\u0085\3\u0085\5\u0085\u0b8f\n")
        buf.write("\u0085\3\u0085\3\u0085\5\u0085\u0b93\n\u0085\3\u0085\3")
        buf.write("\u0085\5\u0085\u0b97\n\u0085\3\u0085\3\u0085\5\u0085\u0b9b")
        buf.write("\n\u0085\3\u0085\3\u0085\5\u0085\u0b9f\n\u0085\3\u0085")
        buf.write("\3\u0085\5\u0085\u0ba3\n\u0085\3\u0085\3\u0085\5\u0085")
        buf.write("\u0ba7\n\u0085\3\u0085\5\u0085\u0baa\n\u0085\3\u0086\3")
        buf.write("\u0086\3\u0086\3\u0086\3\u0086\3\u0086\3\u0086\5\u0086")
        buf.write("\u0bb3\n\u0086\3\u0087\3\u0087\3\u0088\3\u0088\3\u0088")
        buf.write("\n\u0395\u03d3\u03dd\u03ee\u03f7\u0400\u0409\u0410\6T")
        buf.write("\u00c0\u00c4\u00c6\u0089\2\4\6\b\n\f\16\20\22\24\26\30")
        buf.write("\32\34\36 \"$&(*,.\60\62\64\668:<>@BDFHJLNPRTVXZ\\^`b")
        buf.write("dfhjlnprtvxz|~\u0080\u0082\u0084\u0086\u0088\u008a\u008c")
        buf.write("\u008e\u0090\u0092\u0094\u0096\u0098\u009a\u009c\u009e")
        buf.write("\u00a0\u00a2\u00a4\u00a6\u00a8\u00aa\u00ac\u00ae\u00b0")
        buf.write("\u00b2\u00b4\u00b6\u00b8\u00ba\u00bc\u00be\u00c0\u00c2")
        buf.write("\u00c4\u00c6\u00c8\u00ca\u00cc\u00ce\u00d0\u00d2\u00d4")
        buf.write("\u00d6\u00d8\u00da\u00dc\u00de\u00e0\u00e2\u00e4\u00e6")
        buf.write("\u00e8\u00ea\u00ec\u00ee\u00f0\u00f2\u00f4\u00f6\u00f8")
        buf.write("\u00fa\u00fc\u00fe\u0100\u0102\u0104\u0106\u0108\u010a")
        buf.write("\u010c\u010e\2*\4\2BB\u00b3\u00b3\4\2\"\"\u00c1\u00c1")
        buf.write("\4\2AA\u0095\u0095\4\2ffrr\3\2-.\4\2\u00e1\u00e1\u0100")
        buf.write("\u0100\4\2\21\21%%\7\2**\66\66XXee\u008e\u008e\3\2FG\4")
        buf.write("\2XXee\4\2\u0099\u0099\u0119\u0119\4\2\16\16\u0088\u0088")
        buf.write("\4\2\u008a\u008a\u0119\u0119\5\2@@\u0094\u0094\u00cb\u00cb")
        buf.write("\5\2SS\u00d3\u00d3\u00f6\u00f6\4\2\31\31FF\4\2``\u0080")
        buf.write("\u0080\4\2\20\20KK\4\2\u011d\u011d\u011f\u011f\5\2\20")
        buf.write("\20\25\25\u00d7\u00d7\5\2[[\u00f0\u00f0\u00f8\u00f8\4")
        buf.write("\2\u010f\u0110\u0114\u0114\4\2MM\u0111\u0113\4\2\u010f")
        buf.write("\u0110\u0117\u0117\4\2;;==\3\2\u00df\u00e0\4\2\6\6ff\4")
        buf.write("\2\6\6bb\5\2\35\35\u0083\u0083\u00eb\u00eb\3\2\u0107\u010e")
        buf.write("\4\2MM\u010f\u0118\6\2\23\23rr\u0098\u0098\u00a0\u00a0")
        buf.write("\4\2[[\u00f0\u00f0\3\2\u010f\u0110\4\2LL\u00a9\u00a9\4")
        buf.write("\2\u00a1\u00a1\u00d8\u00d8\4\2aa\u00b0\u00b0\4\2NN\u00d2")
        buf.write("\u00d2\21\2\24\2488SSgguuyy~~\u0084\u0084\u0096\u0096")
        buf.write("\u009c\u009c\u00c3\u00c3\u00cd\u00cd\u00d3\u00d3\u00f6")
        buf.write("\u00f6\u00fe\u00fe\22\2\16\23\25\679RTfhtvxz}\177\u0083")
        buf.write("\u0085\u0095\u0097\u009b\u009d\u00c2\u00c4\u00cc\u00ce")
        buf.write("\u00d2\u00d4\u00f5\u00f7\u00fd\u00ff\u0106\2\u0d8d\2\u0110")
        buf.write("\3\2\2\2\4\u0119\3\2\2\2\6\u011c\3\2\2\2\b\u011f\3\2\2")
        buf.write("\2\n\u0122\3\2\2\2\f\u0125\3\2\2\2\16\u0128\3\2\2\2\20")
        buf.write("\u0413\3\2\2\2\22\u0415\3\2\2\2\24\u04bf\3\2\2\2\26\u04c1")
        buf.write("\3\2\2\2\30\u04d2\3\2\2\2\32\u04d8\3\2\2\2\34\u04e4\3")
        buf.write("\2\2\2\36\u04f1\3\2\2\2 \u04f4\3\2\2\2\"\u04f8\3\2\2\2")
        buf.write("$\u0534\3\2\2\2&\u0536\3\2\2\2(\u053a\3\2\2\2*\u0546\3")
        buf.write("\2\2\2,\u054b\3\2\2\2.\u0552\3\2\2\2\60\u0554\3\2\2\2")
        buf.write("\62\u055c\3\2\2\2\64\u0565\3\2\2\2\66\u0570\3\2\2\28\u057f")
        buf.write("\3\2\2\2:\u0582\3\2\2\2<\u058d\3\2\2\2>\u059d\3\2\2\2")
        buf.write("@\u05a3\3\2\2\2B\u05a5\3\2\2\2D\u05b0\3\2\2\2F\u05c1\3")
        buf.write("\2\2\2H\u05c8\3\2\2\2J\u05ca\3\2\2\2L\u05d0\3\2\2\2N\u0606")
        buf.write("\3\2\2\2P\u0612\3\2\2\2R\u0642\3\2\2\2T\u0645\3\2\2\2")
        buf.write("V\u0662\3\2\2\2X\u0664\3\2\2\2Z\u066c\3\2\2\2\\\u068d")
        buf.write("\3\2\2\2^\u06ac\3\2\2\2`\u06b8\3\2\2\2b\u06d8\3\2\2\2")
        buf.write("d\u06e4\3\2\2\2f\u06e7\3\2\2\2h\u06f0\3\2\2\2j\u0701\3")
        buf.write("\2\2\2l\u0715\3\2\2\2n\u0717\3\2\2\2p\u071f\3\2\2\2r\u0723")
        buf.write("\3\2\2\2t\u0726\3\2\2\2v\u0729\3\2\2\2x\u0743\3\2\2\2")
        buf.write("z\u0745\3\2\2\2|\u0783\3\2\2\2~\u0792\3\2\2\2\u0080\u0794")
        buf.write("\3\2\2\2\u0082\u07b2\3\2\2\2\u0084\u07b4\3\2\2\2\u0086")
        buf.write("\u07bb\3\2\2\2\u0088\u07db\3\2\2\2\u008a\u07dd\3\2\2\2")
        buf.write("\u008c\u07ef\3\2\2\2\u008e\u0809\3\2\2\2\u0090\u080f\3")
        buf.write("\2\2\2\u0092\u0811\3\2\2\2\u0094\u0830\3\2\2\2\u0096\u0832")
        buf.write("\3\2\2\2\u0098\u0836\3\2\2\2\u009a\u083e\3\2\2\2\u009c")
        buf.write("\u0849\3\2\2\2\u009e\u084d\3\2\2\2\u00a0\u0858\3\2\2\2")
        buf.write("\u00a2\u0874\3\2\2\2\u00a4\u0876\3\2\2\2\u00a6\u0881\3")
        buf.write("\2\2\2\u00a8\u0897\3\2\2\2\u00aa\u08ca\3\2\2\2\u00ac\u08cc")
        buf.write("\3\2\2\2\u00ae\u08d4\3\2\2\2\u00b0\u08df\3\2\2\2\u00b2")
        buf.write("\u08e6\3\2\2\2\u00b4\u08ea\3\2\2\2\u00b6\u08f4\3\2\2\2")
        buf.write("\u00b8\u08fc\3\2\2\2\u00ba\u0914\3\2\2\2\u00bc\u0918\3")
        buf.write("\2\2\2\u00be\u091a\3\2\2\2\u00c0\u0928\3\2\2\2\u00c2\u0987")
        buf.write("\3\2\2\2\u00c4\u098d\3\2\2\2\u00c6\u0a5f\3\2\2\2\u00c8")
        buf.write("\u0a7a\3\2\2\2\u00ca\u0a7c\3\2\2\2\u00cc\u0a7e\3\2\2\2")
        buf.write("\u00ce\u0a80\3\2\2\2\u00d0\u0a82\3\2\2\2\u00d2\u0a84\3")
        buf.write("\2\2\2\u00d4\u0a89\3\2\2\2\u00d6\u0a90\3\2\2\2\u00d8\u0a94")
        buf.write("\3\2\2\2\u00da\u0a99\3\2\2\2\u00dc\u0aa3\3\2\2\2\u00de")
        buf.write("\u0aa8\3\2\2\2\u00e0\u0acc\3\2\2\2\u00e2\u0ace\3\2\2\2")
        buf.write("\u00e4\u0ad6\3\2\2\2\u00e6\u0ae2\3\2\2\2\u00e8\u0aea\3")
        buf.write("\2\2\2\u00ea\u0af3\3\2\2\2\u00ec\u0afb\3\2\2\2\u00ee\u0b05")
        buf.write("\3\2\2\2\u00f0\u0b0a\3\2\2\2\u00f2\u0b13\3\2\2\2\u00f4")
        buf.write("\u0b45\3\2\2\2\u00f6\u0b57\3\2\2\2\u00f8\u0b60\3\2\2\2")
        buf.write("\u00fa\u0b62\3\2\2\2\u00fc\u0b6e\3\2\2\2\u00fe\u0b70\3")
        buf.write("\2\2\2\u0100\u0b78\3\2\2\2\u0102\u0b7c\3\2\2\2\u0104\u0b81")
        buf.write("\3\2\2\2\u0106\u0b83\3\2\2\2\u0108\u0ba9\3\2\2\2\u010a")
        buf.write("\u0bb2\3\2\2\2\u010c\u0bb4\3\2\2\2\u010e\u0bb6\3\2\2\2")
        buf.write("\u0110\u0114\5\20\t\2\u0111\u0113\7\3\2\2\u0112\u0111")
        buf.write("\3\2\2\2\u0113\u0116\3\2\2\2\u0114\u0112\3\2\2\2\u0114")
        buf.write("\u0115\3\2\2\2\u0115\u0117\3\2\2\2\u0116\u0114\3\2\2\2")
        buf.write("\u0117\u0118\7\2\2\3\u0118\3\3\2\2\2\u0119\u011a\5\u00b4")
        buf.write("[\2\u011a\u011b\7\2\2\3\u011b\5\3\2\2\2\u011c\u011d\5")
        buf.write("\u00b0Y\2\u011d\u011e\7\2\2\3\u011e\7\3\2\2\2\u011f\u0120")
        buf.write("\5\u00aeX\2\u0120\u0121\7\2\2\3\u0121\t\3\2\2\2\u0122")
        buf.write("\u0123\5\u00b2Z\2\u0123\u0124\7\2\2\3\u0124\13\3\2\2\2")
        buf.write("\u0125\u0126\5\u00e0q\2\u0126\u0127\7\2\2\3\u0127\r\3")
        buf.write("\2\2\2\u0128\u0129\5\u00e6t\2\u0129\u012a\7\2\2\3\u012a")
        buf.write("\17\3\2\2\2\u012b\u0414\5\"\22\2\u012c\u012e\5\62\32\2")
        buf.write("\u012d\u012c\3\2\2\2\u012d\u012e\3\2\2\2\u012e\u012f\3")
        buf.write("\2\2\2\u012f\u0414\5N(\2\u0130\u0132\7\u00fc\2\2\u0131")
        buf.write("\u0133\7\u0094\2\2\u0132\u0131\3\2\2\2\u0132\u0133\3\2")
        buf.write("\2\2\u0133\u0134\3\2\2\2\u0134\u0414\5\u00aeX\2\u0135")
        buf.write("\u0136\7\67\2\2\u0136\u013a\5,\27\2\u0137\u0138\7o\2\2")
        buf.write("\u0138\u0139\7\u0098\2\2\u0139\u013b\7U\2\2\u013a\u0137")
        buf.write("\3\2\2\2\u013a\u013b\3\2\2\2\u013b\u013c\3\2\2\2\u013c")
        buf.write("\u0144\5\u00aeX\2\u013d\u0143\5 \21\2\u013e\u0143\5\36")
        buf.write("\20\2\u013f\u0140\7\u0105\2\2\u0140\u0141\t\2\2\2\u0141")
        buf.write("\u0143\5:\36\2\u0142\u013d\3\2\2\2\u0142\u013e\3\2\2\2")
        buf.write("\u0142\u013f\3\2\2\2\u0143\u0146\3\2\2\2\u0144\u0142\3")
        buf.write("\2\2\2\u0144\u0145\3\2\2\2\u0145\u0414\3\2\2\2\u0146\u0144")
        buf.write("\3\2\2\2\u0147\u0148\7\21\2\2\u0148\u0149\5,\27\2\u0149")
        buf.write("\u014a\5\u00aeX\2\u014a\u014b\7\u00d2\2\2\u014b\u014c")
        buf.write("\t\2\2\2\u014c\u014d\5:\36\2\u014d\u0414\3\2\2\2\u014e")
        buf.write("\u014f\7\21\2\2\u014f\u0150\5,\27\2\u0150\u0151\5\u00ae")
        buf.write("X\2\u0151\u0152\7\u00d2\2\2\u0152\u0153\5\36\20\2\u0153")
        buf.write("\u0414\3\2\2\2\u0154\u0155\7N\2\2\u0155\u0158\5,\27\2")
        buf.write("\u0156\u0157\7o\2\2\u0157\u0159\7U\2\2\u0158\u0156\3\2")
        buf.write("\2\2\u0158\u0159\3\2\2\2\u0159\u015a\3\2\2\2\u015a\u015c")
        buf.write("\5\u00aeX\2\u015b\u015d\t\3\2\2\u015c\u015b\3\2\2\2\u015c")
        buf.write("\u015d\3\2\2\2\u015d\u0414\3\2\2\2\u015e\u015f\7\u00d5")
        buf.write("\2\2\u015f\u0162\t\4\2\2\u0160\u0161\t\5\2\2\u0161\u0163")
        buf.write("\5\u00aeX\2\u0162\u0160\3\2\2\2\u0162\u0163\3\2\2\2\u0163")
        buf.write("\u0168\3\2\2\2\u0164\u0166\7\u0085\2\2\u0165\u0164\3\2")
        buf.write("\2\2\u0165\u0166\3\2\2\2\u0166\u0167\3\2\2\2\u0167\u0169")
        buf.write("\7\u0119\2\2\u0168\u0165\3\2\2\2\u0168\u0169\3\2\2\2\u0169")
        buf.write("\u0414\3\2\2\2\u016a\u016f\5\26\f\2\u016b\u016c\7\4\2")
        buf.write("\2\u016c\u016d\5\u00e6t\2\u016d\u016e\7\5\2\2\u016e\u0170")
        buf.write("\3\2\2\2\u016f\u016b\3\2\2\2\u016f\u0170\3\2\2\2\u0170")
        buf.write("\u0171\3\2\2\2\u0171\u0172\5\66\34\2\u0172\u0177\58\35")
        buf.write("\2\u0173\u0175\7\30\2\2\u0174\u0173\3\2\2\2\u0174\u0175")
        buf.write("\3\2\2\2\u0175\u0176\3\2\2\2\u0176\u0178\5\"\22\2\u0177")
        buf.write("\u0174\3\2\2\2\u0177\u0178\3\2\2\2\u0178\u0414\3\2\2\2")
        buf.write("\u0179\u017e\5\26\f\2\u017a\u017b\7\4\2\2\u017b\u017c")
        buf.write("\5\u00e6t\2\u017c\u017d\7\5\2\2\u017d\u017f\3\2\2\2\u017e")
        buf.write("\u017a\3\2\2\2\u017e\u017f\3\2\2\2\u017f\u0195\3\2\2\2")
        buf.write("\u0180\u0194\5 \21\2\u0181\u0182\7\u00aa\2\2\u0182\u0183")
        buf.write("\7 \2\2\u0183\u0184\7\4\2\2\u0184\u0185\5\u00e6t\2\u0185")
        buf.write("\u0186\7\5\2\2\u0186\u018b\3\2\2\2\u0187\u0188\7\u00aa")
        buf.write("\2\2\u0188\u0189\7 \2\2\u0189\u018b\5\u0096L\2\u018a\u0181")
        buf.write("\3\2\2\2\u018a\u0187\3\2\2\2\u018b\u0194\3\2\2\2\u018c")
        buf.write("\u0194\5\32\16\2\u018d\u0194\5\34\17\2\u018e\u0194\5\u00aa")
        buf.write("V\2\u018f\u0194\5F$\2\u0190\u0194\5\36\20\2\u0191\u0192")
        buf.write("\7\u00e4\2\2\u0192\u0194\5:\36\2\u0193\u0180\3\2\2\2\u0193")
        buf.write("\u018a\3\2\2\2\u0193\u018c\3\2\2\2\u0193\u018d\3\2\2\2")
        buf.write("\u0193\u018e\3\2\2\2\u0193\u018f\3\2\2\2\u0193\u0190\3")
        buf.write("\2\2\2\u0193\u0191\3\2\2\2\u0194\u0197\3\2\2\2\u0195\u0193")
        buf.write("\3\2\2\2\u0195\u0196\3\2\2\2\u0196\u019c\3\2\2\2\u0197")
        buf.write("\u0195\3\2\2\2\u0198\u019a\7\30\2\2\u0199\u0198\3\2\2")
        buf.write("\2\u0199\u019a\3\2\2\2\u019a\u019b\3\2\2\2\u019b\u019d")
        buf.write("\5\"\22\2\u019c\u0199\3\2\2\2\u019c\u019d\3\2\2\2\u019d")
        buf.write("\u0414\3\2\2\2\u019e\u019f\7\67\2\2\u019f\u01a3\7\u00e1")
        buf.write("\2\2\u01a0\u01a1\7o\2\2\u01a1\u01a2\7\u0098\2\2\u01a2")
        buf.write("\u01a4\7U\2\2\u01a3\u01a0\3\2\2\2\u01a3\u01a4\3\2\2\2")
        buf.write("\u01a4\u01a5\3\2\2\2\u01a5\u01a6\5\u00b0Y\2\u01a6\u01a7")
        buf.write("\7\u0085\2\2\u01a7\u01b0\5\u00b0Y\2\u01a8\u01af\5\66\34")
        buf.write("\2\u01a9\u01af\5\u00aaV\2\u01aa\u01af\5F$\2\u01ab\u01af")
        buf.write("\5\36\20\2\u01ac\u01ad\7\u00e4\2\2\u01ad\u01af\5:\36\2")
        buf.write("\u01ae\u01a8\3\2\2\2\u01ae\u01a9\3\2\2\2\u01ae\u01aa\3")
        buf.write("\2\2\2\u01ae\u01ab\3\2\2\2\u01ae\u01ac\3\2\2\2\u01af\u01b2")
        buf.write("\3\2\2\2\u01b0\u01ae\3\2\2\2\u01b0\u01b1\3\2\2\2\u01b1")
        buf.write("\u0414\3\2\2\2\u01b2\u01b0\3\2\2\2\u01b3\u01b8\5\30\r")
        buf.write("\2\u01b4\u01b5\7\4\2\2\u01b5\u01b6\5\u00e6t\2\u01b6\u01b7")
        buf.write("\7\5\2\2\u01b7\u01b9\3\2\2\2\u01b8\u01b4\3\2\2\2\u01b8")
        buf.write("\u01b9\3\2\2\2\u01b9\u01ba\3\2\2\2\u01ba\u01bb\5\66\34")
        buf.write("\2\u01bb\u01c0\58\35\2\u01bc\u01be\7\30\2\2\u01bd\u01bc")
        buf.write("\3\2\2\2\u01bd\u01be\3\2\2\2\u01be\u01bf\3\2\2\2\u01bf")
        buf.write("\u01c1\5\"\22\2\u01c0\u01bd\3\2\2\2\u01c0\u01c1\3\2\2")
        buf.write("\2\u01c1\u0414\3\2\2\2\u01c2\u01c3\7\22\2\2\u01c3\u01c4")
        buf.write("\7\u00e1\2\2\u01c4\u01c6\5\u00aeX\2\u01c5\u01c7\5(\25")
        buf.write("\2\u01c6\u01c5\3\2\2\2\u01c6\u01c7\3\2\2\2\u01c7\u01c8")
        buf.write("\3\2\2\2\u01c8\u01c9\7\63\2\2\u01c9\u01d1\7\u00db\2\2")
        buf.write("\u01ca\u01d2\5\u0102\u0082\2\u01cb\u01cc\7b\2\2\u01cc")
        buf.write("\u01cd\7.\2\2\u01cd\u01d2\5\u0098M\2\u01ce\u01cf\7b\2")
        buf.write("\2\u01cf\u01d0\7\20\2\2\u01d0\u01d2\7.\2\2\u01d1\u01ca")
        buf.write("\3\2\2\2\u01d1\u01cb\3\2\2\2\u01d1\u01ce\3\2\2\2\u01d1")
        buf.write("\u01d2\3\2\2\2\u01d2\u0414\3\2\2\2\u01d3\u01d4\7\21\2")
        buf.write("\2\u01d4\u01d5\7\u00e1\2\2\u01d5\u01d6\5\u00aeX\2\u01d6")
        buf.write("\u01d7\7\16\2\2\u01d7\u01d8\t\6\2\2\u01d8\u01d9\5\u00e2")
        buf.write("r\2\u01d9\u0414\3\2\2\2\u01da\u01db\7\21\2\2\u01db\u01dc")
        buf.write("\7\u00e1\2\2\u01dc\u01dd\5\u00aeX\2\u01dd\u01de\7\16\2")
        buf.write("\2\u01de\u01df\t\6\2\2\u01df\u01e0\7\4\2\2\u01e0\u01e1")
        buf.write("\5\u00e2r\2\u01e1\u01e2\7\5\2\2\u01e2\u0414\3\2\2\2\u01e3")
        buf.write("\u01e4\7\21\2\2\u01e4\u01e5\7\u00e1\2\2\u01e5\u01e6\5")
        buf.write("\u00aeX\2\u01e6\u01e7\7\u00bd\2\2\u01e7\u01e8\7-\2\2\u01e8")
        buf.write("\u01e9\5\u00aeX\2\u01e9\u01ea\7\u00e9\2\2\u01ea\u01eb")
        buf.write("\5\u0100\u0081\2\u01eb\u0414\3\2\2\2\u01ec\u01ed\7\21")
        buf.write("\2\2\u01ed\u01ee\7\u00e1\2\2\u01ee\u01ef\5\u00aeX\2\u01ef")
        buf.write("\u01f0\7N\2\2\u01f0\u01f1\t\6\2\2\u01f1\u01f2\7\4\2\2")
        buf.write("\u01f2\u01f3\5\u00acW\2\u01f3\u01f4\7\5\2\2\u01f4\u0414")
        buf.write("\3\2\2\2\u01f5\u01f6\7\21\2\2\u01f6\u01f7\7\u00e1\2\2")
        buf.write("\u01f7\u01f8\5\u00aeX\2\u01f8\u01f9\7N\2\2\u01f9\u01fa")
        buf.write("\t\6\2\2\u01fa\u01fb\5\u00acW\2\u01fb\u0414\3\2\2\2\u01fc")
        buf.write("\u01fd\7\21\2\2\u01fd\u01fe\t\7\2\2\u01fe\u01ff\5\u00ae")
        buf.write("X\2\u01ff\u0200\7\u00bd\2\2\u0200\u0201\7\u00e9\2\2\u0201")
        buf.write("\u0202\5\u00aeX\2\u0202\u0414\3\2\2\2\u0203\u0204\7\21")
        buf.write("\2\2\u0204\u0205\t\7\2\2\u0205\u0206\5\u00aeX\2\u0206")
        buf.write("\u0207\7\u00d2\2\2\u0207\u0208\7\u00e4\2\2\u0208\u0209")
        buf.write("\5:\36\2\u0209\u0414\3\2\2\2\u020a\u020b\7\21\2\2\u020b")
        buf.write("\u020c\t\7\2\2\u020c\u020d\5\u00aeX\2\u020d\u020e\7\u00fa")
        buf.write("\2\2\u020e\u0211\7\u00e4\2\2\u020f\u0210\7o\2\2\u0210")
        buf.write("\u0212\7U\2\2\u0211\u020f\3\2\2\2\u0211\u0212\3\2\2\2")
        buf.write("\u0212\u0213\3\2\2\2\u0213\u0214\5:\36\2\u0214\u0414\3")
        buf.write("\2\2\2\u0215\u0216\7\21\2\2\u0216\u0217\7\u00e1\2\2\u0217")
        buf.write("\u0218\5\u00aeX\2\u0218\u021a\t\b\2\2\u0219\u021b\7-\2")
        buf.write("\2\u021a\u0219\3\2\2\2\u021a\u021b\3\2\2\2\u021b\u021c")
        buf.write("\3\2\2\2\u021c\u021e\5\u00aeX\2\u021d\u021f\5\u010a\u0086")
        buf.write("\2\u021e\u021d\3\2\2\2\u021e\u021f\3\2\2\2\u021f\u0414")
        buf.write("\3\2\2\2\u0220\u0221\7\21\2\2\u0221\u0222\7\u00e1\2\2")
        buf.write("\u0222\u0224\5\u00aeX\2\u0223\u0225\5(\25\2\u0224\u0223")
        buf.write("\3\2\2\2\u0224\u0225\3\2\2\2\u0225\u0226\3\2\2\2\u0226")
        buf.write("\u0228\7%\2\2\u0227\u0229\7-\2\2\u0228\u0227\3\2\2\2\u0228")
        buf.write("\u0229\3\2\2\2\u0229\u022a\3\2\2\2\u022a\u022b\5\u00ae")
        buf.write("X\2\u022b\u022d\5\u00e8u\2\u022c\u022e\5\u00dep\2\u022d")
        buf.write("\u022c\3\2\2\2\u022d\u022e\3\2\2\2\u022e\u0414\3\2\2\2")
        buf.write("\u022f\u0230\7\21\2\2\u0230\u0231\7\u00e1\2\2\u0231\u0233")
        buf.write("\5\u00aeX\2\u0232\u0234\5(\25\2\u0233\u0232\3\2\2\2\u0233")
        buf.write("\u0234\3\2\2\2\u0234\u0235\3\2\2\2\u0235\u0236\7\u00bf")
        buf.write("\2\2\u0236\u0237\7.\2\2\u0237\u0238\7\4\2\2\u0238\u0239")
        buf.write("\5\u00e2r\2\u0239\u023a\7\5\2\2\u023a\u0414\3\2\2\2\u023b")
        buf.write("\u023c\7\21\2\2\u023c\u023d\7\u00e1\2\2\u023d\u023f\5")
        buf.write("\u00aeX\2\u023e\u0240\5(\25\2\u023f\u023e\3\2\2\2\u023f")
        buf.write("\u0240\3\2\2\2\u0240\u0241\3\2\2\2\u0241\u0242\7\u00d2")
        buf.write("\2\2\u0242\u0243\7\u00cf\2\2\u0243\u0247\7\u0119\2\2\u0244")
        buf.write("\u0245\7\u0105\2\2\u0245\u0246\7\u00d0\2\2\u0246\u0248")
        buf.write("\5:\36\2\u0247\u0244\3\2\2\2\u0247\u0248\3\2\2\2\u0248")
        buf.write("\u0414\3\2\2\2\u0249\u024a\7\21\2\2\u024a\u024b\7\u00e1")
        buf.write("\2\2\u024b\u024d\5\u00aeX\2\u024c\u024e\5(\25\2\u024d")
        buf.write("\u024c\3\2\2\2\u024d\u024e\3\2\2\2\u024e\u024f\3\2\2\2")
        buf.write("\u024f\u0250\7\u00d2\2\2\u0250\u0251\7\u00d0\2\2\u0251")
        buf.write("\u0252\5:\36\2\u0252\u0414\3\2\2\2\u0253\u0254\7\21\2")
        buf.write("\2\u0254\u0255\t\7\2\2\u0255\u0256\5\u00aeX\2\u0256\u025a")
        buf.write("\7\16\2\2\u0257\u0258\7o\2\2\u0258\u0259\7\u0098\2\2\u0259")
        buf.write("\u025b\7U\2\2\u025a\u0257\3\2\2\2\u025a\u025b\3\2\2\2")
        buf.write("\u025b\u025d\3\2\2\2\u025c\u025e\5&\24\2\u025d\u025c\3")
        buf.write("\2\2\2\u025e\u025f\3\2\2\2\u025f\u025d\3\2\2\2\u025f\u0260")
        buf.write("\3\2\2\2\u0260\u0414\3\2\2\2\u0261\u0262\7\21\2\2\u0262")
        buf.write("\u0263\7\u00e1\2\2\u0263\u0264\5\u00aeX\2\u0264\u0265")
        buf.write("\5(\25\2\u0265\u0266\7\u00bd\2\2\u0266\u0267\7\u00e9\2")
        buf.write("\2\u0267\u0268\5(\25\2\u0268\u0414\3\2\2\2\u0269\u026a")
        buf.write("\7\21\2\2\u026a\u026b\t\7\2\2\u026b\u026c\5\u00aeX\2\u026c")
        buf.write("\u026f\7N\2\2\u026d\u026e\7o\2\2\u026e\u0270\7U\2\2\u026f")
        buf.write("\u026d\3\2\2\2\u026f\u0270\3\2\2\2\u0270\u0271\3\2\2\2")
        buf.write("\u0271\u0276\5(\25\2\u0272\u0273\7\6\2\2\u0273\u0275\5")
        buf.write("(\25\2\u0274\u0272\3\2\2\2\u0275\u0278\3\2\2\2\u0276\u0274")
        buf.write("\3\2\2\2\u0276\u0277\3\2\2\2\u0277\u027a\3\2\2\2\u0278")
        buf.write("\u0276\3\2\2\2\u0279\u027b\7\u00b4\2\2\u027a\u0279\3\2")
        buf.write("\2\2\u027a\u027b\3\2\2\2\u027b\u0414\3\2\2\2\u027c\u027d")
        buf.write("\7\21\2\2\u027d\u027e\7\u00e1\2\2\u027e\u0280\5\u00ae")
        buf.write("X\2\u027f\u0281\5(\25\2\u0280\u027f\3\2\2\2\u0280\u0281")
        buf.write("\3\2\2\2\u0281\u0282\3\2\2\2\u0282\u0283\7\u00d2\2\2\u0283")
        buf.write("\u0284\5\36\20\2\u0284\u0414\3\2\2\2\u0285\u0286\7\21")
        buf.write("\2\2\u0286\u0287\7\u00e1\2\2\u0287\u0288\5\u00aeX\2\u0288")
        buf.write("\u0289\7\u00b9\2\2\u0289\u028a\7\u00ab\2\2\u028a\u0414")
        buf.write("\3\2\2\2\u028b\u028c\7N\2\2\u028c\u028f\7\u00e1\2\2\u028d")
        buf.write("\u028e\7o\2\2\u028e\u0290\7U\2\2\u028f\u028d\3\2\2\2\u028f")
        buf.write("\u0290\3\2\2\2\u0290\u0291\3\2\2\2\u0291\u0293\5\u00ae")
        buf.write("X\2\u0292\u0294\7\u00b4\2\2\u0293\u0292\3\2\2\2\u0293")
        buf.write("\u0294\3\2\2\2\u0294\u0414\3\2\2\2\u0295\u0296\7N\2\2")
        buf.write("\u0296\u0299\7\u0100\2\2\u0297\u0298\7o\2\2\u0298\u029a")
        buf.write("\7U\2\2\u0299\u0297\3\2\2\2\u0299\u029a\3\2\2\2\u029a")
        buf.write("\u029b\3\2\2\2\u029b\u0414\5\u00aeX\2\u029c\u029f\7\67")
        buf.write("\2\2\u029d\u029e\7\u00a0\2\2\u029e\u02a0\7\u00bf\2\2\u029f")
        buf.write("\u029d\3\2\2\2\u029f\u02a0\3\2\2\2\u02a0\u02a5\3\2\2\2")
        buf.write("\u02a1\u02a3\7j\2\2\u02a2\u02a1\3\2\2\2\u02a2\u02a3\3")
        buf.write("\2\2\2\u02a3\u02a4\3\2\2\2\u02a4\u02a6\7\u00e5\2\2\u02a5")
        buf.write("\u02a2\3\2\2\2\u02a5\u02a6\3\2\2\2\u02a6\u02a7\3\2\2\2")
        buf.write("\u02a7\u02ab\7\u0100\2\2\u02a8\u02a9\7o\2\2\u02a9\u02aa")
        buf.write("\7\u0098\2\2\u02aa\u02ac\7U\2\2\u02ab\u02a8\3\2\2\2\u02ab")
        buf.write("\u02ac\3\2\2\2\u02ac\u02ad\3\2\2\2\u02ad\u02af\5\u00ae")
        buf.write("X\2\u02ae\u02b0\5\u009eP\2\u02af\u02ae\3\2\2\2\u02af\u02b0")
        buf.write("\3\2\2\2\u02b0\u02b9\3\2\2\2\u02b1\u02b8\5 \21\2\u02b2")
        buf.write("\u02b3\7\u00aa\2\2\u02b3\u02b4\7\u009c\2\2\u02b4\u02b8")
        buf.write("\5\u0096L\2\u02b5\u02b6\7\u00e4\2\2\u02b6\u02b8\5:\36")
        buf.write("\2\u02b7\u02b1\3\2\2\2\u02b7\u02b2\3\2\2\2\u02b7\u02b5")
        buf.write("\3\2\2\2\u02b8\u02bb\3\2\2\2\u02b9\u02b7\3\2\2\2\u02b9")
        buf.write("\u02ba\3\2\2\2\u02ba\u02bc\3\2\2\2\u02bb\u02b9\3\2\2\2")
        buf.write("\u02bc\u02bd\7\30\2\2\u02bd\u02be\5\"\22\2\u02be\u0414")
        buf.write("\3\2\2\2\u02bf\u02c2\7\67\2\2\u02c0\u02c1\7\u00a0\2\2")
        buf.write("\u02c1\u02c3\7\u00bf\2\2\u02c2\u02c0\3\2\2\2\u02c2\u02c3")
        buf.write("\3\2\2\2\u02c3\u02c5\3\2\2\2\u02c4\u02c6\7j\2\2\u02c5")
        buf.write("\u02c4\3\2\2\2\u02c5\u02c6\3\2\2\2\u02c6\u02c7\3\2\2\2")
        buf.write("\u02c7\u02c8\7\u00e5\2\2\u02c8\u02c9\7\u0100\2\2\u02c9")
        buf.write("\u02ce\5\u00b0Y\2\u02ca\u02cb\7\4\2\2\u02cb\u02cc\5\u00e6")
        buf.write("t\2\u02cc\u02cd\7\5\2\2\u02cd\u02cf\3\2\2\2\u02ce\u02ca")
        buf.write("\3\2\2\2\u02ce\u02cf\3\2\2\2\u02cf\u02d0\3\2\2\2\u02d0")
        buf.write("\u02d3\5\66\34\2\u02d1\u02d2\7\u009f\2\2\u02d2\u02d4\5")
        buf.write(":\36\2\u02d3\u02d1\3\2\2\2\u02d3\u02d4\3\2\2\2\u02d4\u0414")
        buf.write("\3\2\2\2\u02d5\u02d6\7\21\2\2\u02d6\u02d7\7\u0100\2\2")
        buf.write("\u02d7\u02d9\5\u00aeX\2\u02d8\u02da\7\30\2\2\u02d9\u02d8")
        buf.write("\3\2\2\2\u02d9\u02da\3\2\2\2\u02da\u02db\3\2\2\2\u02db")
        buf.write("\u02dc\5\"\22\2\u02dc\u0414\3\2\2\2\u02dd\u02e0\7\67\2")
        buf.write("\2\u02de\u02df\7\u00a0\2\2\u02df\u02e1\7\u00bf\2\2\u02e0")
        buf.write("\u02de\3\2\2\2\u02e0\u02e1\3\2\2\2\u02e1\u02e3\3\2\2\2")
        buf.write("\u02e2\u02e4\7\u00e5\2\2\u02e3\u02e2\3\2\2\2\u02e3\u02e4")
        buf.write("\3\2\2\2\u02e4\u02e5\3\2\2\2\u02e5\u02e9\7h\2\2\u02e6")
        buf.write("\u02e7\7o\2\2\u02e7\u02e8\7\u0098\2\2\u02e8\u02ea\7U\2")
        buf.write("\2\u02e9\u02e6\3\2\2\2\u02e9\u02ea\3\2\2\2\u02ea\u02eb")
        buf.write("\3\2\2\2\u02eb\u02ec\5\u00aeX\2\u02ec\u02ed\7\30\2\2\u02ed")
        buf.write("\u02f7\7\u0119\2\2\u02ee\u02ef\7\u00fe\2\2\u02ef\u02f4")
        buf.write("\5L\'\2\u02f0\u02f1\7\6\2\2\u02f1\u02f3\5L\'\2\u02f2\u02f0")
        buf.write("\3\2\2\2\u02f3\u02f6\3\2\2\2\u02f4\u02f2\3\2\2\2\u02f4")
        buf.write("\u02f5\3\2\2\2\u02f5\u02f8\3\2\2\2\u02f6\u02f4\3\2\2\2")
        buf.write("\u02f7\u02ee\3\2\2\2\u02f7\u02f8\3\2\2\2\u02f8\u0414\3")
        buf.write("\2\2\2\u02f9\u02fb\7N\2\2\u02fa\u02fc\7\u00e5\2\2\u02fb")
        buf.write("\u02fa\3\2\2\2\u02fb\u02fc\3\2\2\2\u02fc\u02fd\3\2\2\2")
        buf.write("\u02fd\u0300\7h\2\2\u02fe\u02ff\7o\2\2\u02ff\u0301\7U")
        buf.write("\2\2\u0300\u02fe\3\2\2\2\u0300\u0301\3\2\2\2\u0301\u0302")
        buf.write("\3\2\2\2\u0302\u0414\5\u00aeX\2\u0303\u0305\7V\2\2\u0304")
        buf.write("\u0306\t\t\2\2\u0305\u0304\3\2\2\2\u0305\u0306\3\2\2\2")
        buf.write("\u0306\u0307\3\2\2\2\u0307\u0414\5\20\t\2\u0308\u0309")
        buf.write("\7\u00d5\2\2\u0309\u030c\7\u00e2\2\2\u030a\u030b\t\5\2")
        buf.write("\2\u030b\u030d\5\u00aeX\2\u030c\u030a\3\2\2\2\u030c\u030d")
        buf.write("\3\2\2\2\u030d\u0312\3\2\2\2\u030e\u0310\7\u0085\2\2\u030f")
        buf.write("\u030e\3\2\2\2\u030f\u0310\3\2\2\2\u0310\u0311\3\2\2\2")
        buf.write("\u0311\u0313\7\u0119\2\2\u0312\u030f\3\2\2\2\u0312\u0313")
        buf.write("\3\2\2\2\u0313\u0414\3\2\2\2\u0314\u0315\7\u00d5\2\2\u0315")
        buf.write("\u0316\7\u00e1\2\2\u0316\u0319\7X\2\2\u0317\u0318\t\5")
        buf.write("\2\2\u0318\u031a\5\u00aeX\2\u0319\u0317\3\2\2\2\u0319")
        buf.write("\u031a\3\2\2\2\u031a\u031b\3\2\2\2\u031b\u031c\7\u0085")
        buf.write("\2\2\u031c\u031e\7\u0119\2\2\u031d\u031f\5(\25\2\u031e")
        buf.write("\u031d\3\2\2\2\u031e\u031f\3\2\2\2\u031f\u0414\3\2\2\2")
        buf.write("\u0320\u0321\7\u00d5\2\2\u0321\u0322\7\u00e4\2\2\u0322")
        buf.write("\u0327\5\u00aeX\2\u0323\u0324\7\4\2\2\u0324\u0325\5> ")
        buf.write("\2\u0325\u0326\7\5\2\2\u0326\u0328\3\2\2\2\u0327\u0323")
        buf.write("\3\2\2\2\u0327\u0328\3\2\2\2\u0328\u0414\3\2\2\2\u0329")
        buf.write("\u032a\7\u00d5\2\2\u032a\u032b\7.\2\2\u032b\u032c\t\5")
        buf.write("\2\2\u032c\u032f\5\u00aeX\2\u032d\u032e\t\5\2\2\u032e")
        buf.write("\u0330\5\u00aeX\2\u032f\u032d\3\2\2\2\u032f\u0330\3\2")
        buf.write("\2\2\u0330\u0414\3\2\2\2\u0331\u0332\7\u00d5\2\2\u0332")
        buf.write("\u0335\7\u0101\2\2\u0333\u0334\t\5\2\2\u0334\u0336\5\u00ae")
        buf.write("X\2\u0335\u0333\3\2\2\2\u0335\u0336\3\2\2\2\u0336\u033b")
        buf.write("\3\2\2\2\u0337\u0339\7\u0085\2\2\u0338\u0337\3\2\2\2\u0338")
        buf.write("\u0339\3\2\2\2\u0339\u033a\3\2\2\2\u033a\u033c\7\u0119")
        buf.write("\2\2\u033b\u0338\3\2\2\2\u033b\u033c\3\2\2\2\u033c\u0414")
        buf.write("\3\2\2\2\u033d\u033e\7\u00d5\2\2\u033e\u033f\7\u00ab\2")
        buf.write("\2\u033f\u0341\5\u00aeX\2\u0340\u0342\5(\25\2\u0341\u0340")
        buf.write("\3\2\2\2\u0341\u0342\3\2\2\2\u0342\u0414\3\2\2\2\u0343")
        buf.write("\u0345\7\u00d5\2\2\u0344\u0346\5\u0102\u0082\2\u0345\u0344")
        buf.write("\3\2\2\2\u0345\u0346\3\2\2\2\u0346\u0347\3\2\2\2\u0347")
        buf.write("\u034f\7i\2\2\u0348\u034a\7\u0085\2\2\u0349\u0348\3\2")
        buf.write("\2\2\u0349\u034a\3\2\2\2\u034a\u034d\3\2\2\2\u034b\u034e")
        buf.write("\5\u00aeX\2\u034c\u034e\7\u0119\2\2\u034d\u034b\3\2\2")
        buf.write("\2\u034d\u034c\3\2\2\2\u034e\u0350\3\2\2\2\u034f\u0349")
        buf.write("\3\2\2\2\u034f\u0350\3\2\2\2\u0350\u0414\3\2\2\2\u0351")
        buf.write("\u0352\7\u00d5\2\2\u0352\u0353\7\67\2\2\u0353\u0354\7")
        buf.write("\u00e1\2\2\u0354\u0357\5\u00aeX\2\u0355\u0356\7\30\2\2")
        buf.write("\u0356\u0358\7\u00cf\2\2\u0357\u0355\3\2\2\2\u0357\u0358")
        buf.write("\3\2\2\2\u0358\u0414\3\2\2\2\u0359\u035a\7\u00d5\2\2\u035a")
        buf.write("\u035b\7:\2\2\u035b\u0414\7\u0094\2\2\u035c\u035d\t\n")
        buf.write("\2\2\u035d\u035f\7h\2\2\u035e\u0360\7X\2\2\u035f\u035e")
        buf.write("\3\2\2\2\u035f\u0360\3\2\2\2\u0360\u0361\3\2\2\2\u0361")
        buf.write("\u0414\5.\30\2\u0362\u0363\t\n\2\2\u0363\u0365\5,\27\2")
        buf.write("\u0364\u0366\7X\2\2\u0365\u0364\3\2\2\2\u0365\u0366\3")
        buf.write("\2\2\2\u0366\u0367\3\2\2\2\u0367\u0368\5\u00aeX\2\u0368")
        buf.write("\u0414\3\2\2\2\u0369\u036b\t\n\2\2\u036a\u036c\7\u00e1")
        buf.write("\2\2\u036b\u036a\3\2\2\2\u036b\u036c\3\2\2\2\u036c\u036e")
        buf.write("\3\2\2\2\u036d\u036f\t\13\2\2\u036e\u036d\3\2\2\2\u036e")
        buf.write("\u036f\3\2\2\2\u036f\u0370\3\2\2\2\u0370\u0372\5\u00ae")
        buf.write("X\2\u0371\u0373\5(\25\2\u0372\u0371\3\2\2\2\u0372\u0373")
        buf.write("\3\2\2\2\u0373\u0375\3\2\2\2\u0374\u0376\5\60\31\2\u0375")
        buf.write("\u0374\3\2\2\2\u0375\u0376\3\2\2\2\u0376\u0414\3\2\2\2")
        buf.write("\u0377\u0379\t\n\2\2\u0378\u037a\7\u00b5\2\2\u0379\u0378")
        buf.write("\3\2\2\2\u0379\u037a\3\2\2\2\u037a\u037b\3\2\2\2\u037b")
        buf.write("\u0414\5\"\22\2\u037c\u037d\7/\2\2\u037d\u037e\7\u009c")
        buf.write("\2\2\u037e\u037f\5,\27\2\u037f\u0380\5\u00aeX\2\u0380")
        buf.write("\u0381\7|\2\2\u0381\u0382\t\f\2\2\u0382\u0414\3\2\2\2")
        buf.write("\u0383\u0384\7/\2\2\u0384\u0385\7\u009c\2\2\u0385\u0386")
        buf.write("\7\u00e1\2\2\u0386\u0387\5\u00aeX\2\u0387\u0388\7|\2\2")
        buf.write("\u0388\u0389\t\f\2\2\u0389\u0414\3\2\2\2\u038a\u038b\7")
        buf.write("\u00bc\2\2\u038b\u038c\7\u00e1\2\2\u038c\u0414\5\u00ae")
        buf.write("X\2\u038d\u038e\7\u00bc\2\2\u038e\u038f\7h\2\2\u038f\u0414")
        buf.write("\5\u00aeX\2\u0390\u0398\7\u00bc\2\2\u0391\u0399\7\u0119")
        buf.write("\2\2\u0392\u0394\13\2\2\2\u0393\u0392\3\2\2\2\u0394\u0397")
        buf.write("\3\2\2\2\u0395\u0396\3\2\2\2\u0395\u0393\3\2\2\2\u0396")
        buf.write("\u0399\3\2\2\2\u0397\u0395\3\2\2\2\u0398\u0391\3\2\2\2")
        buf.write("\u0398\u0395\3\2\2\2\u0399\u0414\3\2\2\2\u039a\u039c\7")
        buf.write("!\2\2\u039b\u039d\7\u0082\2\2\u039c\u039b\3\2\2\2\u039c")
        buf.write("\u039d\3\2\2\2\u039d\u039e\3\2\2\2\u039e\u039f\7\u00e1")
        buf.write("\2\2\u039f\u03a2\5\u00aeX\2\u03a0\u03a1\7\u009f\2\2\u03a1")
        buf.write("\u03a3\5:\36\2\u03a2\u03a0\3\2\2\2\u03a2\u03a3\3\2\2\2")
        buf.write("\u03a3\u03a8\3\2\2\2\u03a4\u03a6\7\30\2\2\u03a5\u03a4")
        buf.write("\3\2\2\2\u03a5\u03a6\3\2\2\2\u03a6\u03a7\3\2\2\2\u03a7")
        buf.write("\u03a9\5\"\22\2\u03a8\u03a5\3\2\2\2\u03a8\u03a9\3\2\2")
        buf.write("\2\u03a9\u0414\3\2\2\2\u03aa\u03ab\7\u00f5\2\2\u03ab\u03ae")
        buf.write("\7\u00e1\2\2\u03ac\u03ad\7o\2\2\u03ad\u03af\7U\2\2\u03ae")
        buf.write("\u03ac\3\2\2\2\u03ae\u03af\3\2\2\2\u03af\u03b0\3\2\2\2")
        buf.write("\u03b0\u0414\5\u00aeX\2\u03b1\u03b2\7\'\2\2\u03b2\u0414")
        buf.write("\7!\2\2\u03b3\u03b4\7\u0089\2\2\u03b4\u03b6\7?\2\2\u03b5")
        buf.write("\u03b7\7\u008a\2\2\u03b6\u03b5\3\2\2\2\u03b6\u03b7\3\2")
        buf.write("\2\2\u03b7\u03b8\3\2\2\2\u03b8\u03b9\7v\2\2\u03b9\u03bb")
        buf.write("\7\u0119\2\2\u03ba\u03bc\7\u00a8\2\2\u03bb\u03ba\3\2\2")
        buf.write("\2\u03bb\u03bc\3\2\2\2\u03bc\u03bd\3\2\2\2\u03bd\u03be")
        buf.write("\7{\2\2\u03be\u03bf\7\u00e1\2\2\u03bf\u03c1\5\u00aeX\2")
        buf.write("\u03c0\u03c2\5(\25\2\u03c1\u03c0\3\2\2\2\u03c1\u03c2\3")
        buf.write("\2\2\2\u03c2\u0414\3\2\2\2\u03c3\u03c4\7\u00f1\2\2\u03c4")
        buf.write("\u03c5\7\u00e1\2\2\u03c5\u03c7\5\u00aeX\2\u03c6\u03c8")
        buf.write("\5(\25\2\u03c7\u03c6\3\2\2\2\u03c7\u03c8\3\2\2\2\u03c8")
        buf.write("\u0414\3\2\2\2\u03c9\u03ca\7\u0093\2\2\u03ca\u03cb\7\u00be")
        buf.write("\2\2\u03cb\u03cc\7\u00e1\2\2\u03cc\u0414\5\u00aeX\2\u03cd")
        buf.write("\u03ce\t\r\2\2\u03ce\u03d6\5\u0102\u0082\2\u03cf\u03d7")
        buf.write("\7\u0119\2\2\u03d0\u03d2\13\2\2\2\u03d1\u03d0\3\2\2\2")
        buf.write("\u03d2\u03d5\3\2\2\2\u03d3\u03d4\3\2\2\2\u03d3\u03d1\3")
        buf.write("\2\2\2\u03d4\u03d7\3\2\2\2\u03d5\u03d3\3\2\2\2\u03d6\u03cf")
        buf.write("\3\2\2\2\u03d6\u03d3\3\2\2\2\u03d7\u0414\3\2\2\2\u03d8")
        buf.write("\u03d9\7\u00d2\2\2\u03d9\u03dd\7\u00c5\2\2\u03da\u03dc")
        buf.write("\13\2\2\2\u03db\u03da\3\2\2\2\u03dc\u03df\3\2\2\2\u03dd")
        buf.write("\u03de\3\2\2\2\u03dd\u03db\3\2\2\2\u03de\u0414\3\2\2\2")
        buf.write("\u03df\u03dd\3\2\2\2\u03e0\u03e1\7\u00d2\2\2\u03e1\u03e2")
        buf.write("\7\u00e8\2\2\u03e2\u03e3\7\u0106\2\2\u03e3\u0414\5\u00d2")
        buf.write("j\2\u03e4\u03e5\7\u00d2\2\2\u03e5\u03e6\7\u00e8\2\2\u03e6")
        buf.write("\u03e7\7\u0106\2\2\u03e7\u0414\t\16\2\2\u03e8\u03e9\7")
        buf.write("\u00d2\2\2\u03e9\u03ea\7\u00e8\2\2\u03ea\u03ee\7\u0106")
        buf.write("\2\2\u03eb\u03ed\13\2\2\2\u03ec\u03eb\3\2\2\2\u03ed\u03f0")
        buf.write("\3\2\2\2\u03ee\u03ef\3\2\2\2\u03ee\u03ec\3\2\2\2\u03ef")
        buf.write("\u0414\3\2\2\2\u03f0\u03ee\3\2\2\2\u03f1\u03f2\7\u00d2")
        buf.write("\2\2\u03f2\u03fa\5\22\n\2\u03f3\u03f7\7\u0107\2\2\u03f4")
        buf.write("\u03f6\13\2\2\2\u03f5\u03f4\3\2\2\2\u03f6\u03f9\3\2\2")
        buf.write("\2\u03f7\u03f8\3\2\2\2\u03f7\u03f5\3\2\2\2\u03f8\u03fb")
        buf.write("\3\2\2\2\u03f9\u03f7\3\2\2\2\u03fa\u03f3\3\2\2\2\u03fa")
        buf.write("\u03fb\3\2\2\2\u03fb\u0414\3\2\2\2\u03fc\u0400\7\u00d2")
        buf.write("\2\2\u03fd\u03ff\13\2\2\2\u03fe\u03fd\3\2\2\2\u03ff\u0402")
        buf.write("\3\2\2\2\u0400\u0401\3\2\2\2\u0400\u03fe\3\2\2\2\u0401")
        buf.write("\u0414\3\2\2\2\u0402\u0400\3\2\2\2\u0403\u0404\7\u00c0")
        buf.write("\2\2\u0404\u0414\5\22\n\2\u0405\u0409\7\u00c0\2\2\u0406")
        buf.write("\u0408\13\2\2\2\u0407\u0406\3\2\2\2\u0408\u040b\3\2\2")
        buf.write("\2\u0409\u040a\3\2\2\2\u0409\u0407\3\2\2\2\u040a\u0414")
        buf.write("\3\2\2\2\u040b\u0409\3\2\2\2\u040c\u0410\5\24\13\2\u040d")
        buf.write("\u040f\13\2\2\2\u040e\u040d\3\2\2\2\u040f\u0412\3\2\2")
        buf.write("\2\u0410\u0411\3\2\2\2\u0410\u040e\3\2\2\2\u0411\u0414")
        buf.write("\3\2\2\2\u0412\u0410\3\2\2\2\u0413\u012b\3\2\2\2\u0413")
        buf.write("\u012d\3\2\2\2\u0413\u0130\3\2\2\2\u0413\u0135\3\2\2\2")
        buf.write("\u0413\u0147\3\2\2\2\u0413\u014e\3\2\2\2\u0413\u0154\3")
        buf.write("\2\2\2\u0413\u015e\3\2\2\2\u0413\u016a\3\2\2\2\u0413\u0179")
        buf.write("\3\2\2\2\u0413\u019e\3\2\2\2\u0413\u01b3\3\2\2\2\u0413")
        buf.write("\u01c2\3\2\2\2\u0413\u01d3\3\2\2\2\u0413\u01da\3\2\2\2")
        buf.write("\u0413\u01e3\3\2\2\2\u0413\u01ec\3\2\2\2\u0413\u01f5\3")
        buf.write("\2\2\2\u0413\u01fc\3\2\2\2\u0413\u0203\3\2\2\2\u0413\u020a")
        buf.write("\3\2\2\2\u0413\u0215\3\2\2\2\u0413\u0220\3\2\2\2\u0413")
        buf.write("\u022f\3\2\2\2\u0413\u023b\3\2\2\2\u0413\u0249\3\2\2\2")
        buf.write("\u0413\u0253\3\2\2\2\u0413\u0261\3\2\2\2\u0413\u0269\3")
        buf.write("\2\2\2\u0413\u027c\3\2\2\2\u0413\u0285\3\2\2\2\u0413\u028b")
        buf.write("\3\2\2\2\u0413\u0295\3\2\2\2\u0413\u029c\3\2\2\2\u0413")
        buf.write("\u02bf\3\2\2\2\u0413\u02d5\3\2\2\2\u0413\u02dd\3\2\2\2")
        buf.write("\u0413\u02f9\3\2\2\2\u0413\u0303\3\2\2\2\u0413\u0308\3")
        buf.write("\2\2\2\u0413\u0314\3\2\2\2\u0413\u0320\3\2\2\2\u0413\u0329")
        buf.write("\3\2\2\2\u0413\u0331\3\2\2\2\u0413\u033d\3\2\2\2\u0413")
        buf.write("\u0343\3\2\2\2\u0413\u0351\3\2\2\2\u0413\u0359\3\2\2\2")
        buf.write("\u0413\u035c\3\2\2\2\u0413\u0362\3\2\2\2\u0413\u0369\3")
        buf.write("\2\2\2\u0413\u0377\3\2\2\2\u0413\u037c\3\2\2\2\u0413\u0383")
        buf.write("\3\2\2\2\u0413\u038a\3\2\2\2\u0413\u038d\3\2\2\2\u0413")
        buf.write("\u0390\3\2\2\2\u0413\u039a\3\2\2\2\u0413\u03aa\3\2\2\2")
        buf.write("\u0413\u03b1\3\2\2\2\u0413\u03b3\3\2\2\2\u0413\u03c3\3")
        buf.write("\2\2\2\u0413\u03c9\3\2\2\2\u0413\u03cd\3\2\2\2\u0413\u03d8")
        buf.write("\3\2\2\2\u0413\u03e0\3\2\2\2\u0413\u03e4\3\2\2\2\u0413")
        buf.write("\u03e8\3\2\2\2\u0413\u03f1\3\2\2\2\u0413\u03fc\3\2\2\2")
        buf.write("\u0413\u0403\3\2\2\2\u0413\u0405\3\2\2\2\u0413\u040c\3")
        buf.write("\2\2\2\u0414\21\3\2\2\2\u0415\u0416\5\u0106\u0084\2\u0416")
        buf.write("\23\3\2\2\2\u0417\u0418\7\67\2\2\u0418\u04c0\7\u00c5\2")
        buf.write("\2\u0419\u041a\7N\2\2\u041a\u04c0\7\u00c5\2\2\u041b\u041d")
        buf.write("\7k\2\2\u041c\u041e\7\u00c5\2\2\u041d\u041c\3\2\2\2\u041d")
        buf.write("\u041e\3\2\2\2\u041e\u04c0\3\2\2\2\u041f\u0421\7\u00c2")
        buf.write("\2\2\u0420\u0422\7\u00c5\2\2\u0421\u0420\3\2\2\2\u0421")
        buf.write("\u0422\3\2\2\2\u0422\u04c0\3\2\2\2\u0423\u0424\7\u00d5")
        buf.write("\2\2\u0424\u04c0\7k\2\2\u0425\u0426\7\u00d5\2\2\u0426")
        buf.write("\u0428\7\u00c5\2\2\u0427\u0429\7k\2\2\u0428\u0427\3\2")
        buf.write("\2\2\u0428\u0429\3\2\2\2\u0429\u04c0\3\2\2\2\u042a\u042b")
        buf.write("\7\u00d5\2\2\u042b\u04c0\7\u00b2\2\2\u042c\u042d\7\u00d5")
        buf.write("\2\2\u042d\u04c0\7\u00c6\2\2\u042e\u042f\7\u00d5\2\2\u042f")
        buf.write("\u0430\7:\2\2\u0430\u04c0\7\u00c6\2\2\u0431\u0432\7W\2")
        buf.write("\2\u0432\u04c0\7\u00e1\2\2\u0433\u0434\7q\2\2\u0434\u04c0")
        buf.write("\7\u00e1\2\2\u0435\u0436\7\u00d5\2\2\u0436\u04c0\7\62")
        buf.write("\2\2\u0437\u0438\7\u00d5\2\2\u0438\u0439\7\67\2\2\u0439")
        buf.write("\u04c0\7\u00e1\2\2\u043a\u043b\7\u00d5\2\2\u043b\u04c0")
        buf.write("\7\u00ed\2\2\u043c\u043d\7\u00d5\2\2\u043d\u04c0\7t\2")
        buf.write("\2\u043e\u043f\7\u00d5\2\2\u043f\u04c0\7\u008d\2\2\u0440")
        buf.write("\u0441\7\67\2\2\u0441\u04c0\7s\2\2\u0442\u0443\7N\2\2")
        buf.write("\u0443\u04c0\7s\2\2\u0444\u0445\7\21\2\2\u0445\u04c0\7")
        buf.write("s\2\2\u0446\u0447\7\u008c\2\2\u0447\u04c0\7\u00e1\2\2")
        buf.write("\u0448\u0449\7\u008c\2\2\u0449\u04c0\7@\2\2\u044a\u044b")
        buf.write("\7\u00f9\2\2\u044b\u04c0\7\u00e1\2\2\u044c\u044d\7\u00f9")
        buf.write("\2\2\u044d\u04c0\7@\2\2\u044e\u044f\7\67\2\2\u044f\u0450")
        buf.write("\7\u00e5\2\2\u0450\u04c0\7\u008f\2\2\u0451\u0452\7N\2")
        buf.write("\2\u0452\u0453\7\u00e5\2\2\u0453\u04c0\7\u008f\2\2\u0454")
        buf.write("\u0455\7\21\2\2\u0455\u0456\7\u00e1\2\2\u0456\u0457\5")
        buf.write("\u00b0Y\2\u0457\u0458\7\u0098\2\2\u0458\u0459\7)\2\2\u0459")
        buf.write("\u04c0\3\2\2\2\u045a\u045b\7\21\2\2\u045b\u045c\7\u00e1")
        buf.write("\2\2\u045c\u045d\5\u00b0Y\2\u045d\u045e\7)\2\2\u045e\u045f")
        buf.write("\7 \2\2\u045f\u04c0\3\2\2\2\u0460\u0461\7\21\2\2\u0461")
        buf.write("\u0462\7\u00e1\2\2\u0462\u0463\5\u00b0Y\2\u0463\u0464")
        buf.write("\7\u0098\2\2\u0464\u0465\7\u00d9\2\2\u0465\u04c0\3\2\2")
        buf.write("\2\u0466\u0467\7\21\2\2\u0467\u0468\7\u00e1\2\2\u0468")
        buf.write("\u0469\5\u00b0Y\2\u0469\u046a\7\u00d6\2\2\u046a\u046b")
        buf.write("\7 \2\2\u046b\u04c0\3\2\2\2\u046c\u046d\7\21\2\2\u046d")
        buf.write("\u046e\7\u00e1\2\2\u046e\u046f\5\u00b0Y\2\u046f\u0470")
        buf.write("\7\u0098\2\2\u0470\u0471\7\u00d6\2\2\u0471\u04c0\3\2\2")
        buf.write("\2\u0472\u0473\7\21\2\2\u0473\u0474\7\u00e1\2\2\u0474")
        buf.write("\u0475\5\u00b0Y\2\u0475\u0476\7\u0098\2\2\u0476\u0477")
        buf.write("\7\u00dc\2\2\u0477\u0478\7\30\2\2\u0478\u0479\7I\2\2\u0479")
        buf.write("\u04c0\3\2\2\2\u047a\u047b\7\21\2\2\u047b\u047c\7\u00e1")
        buf.write("\2\2\u047c\u047d\5\u00b0Y\2\u047d\u047e\7\u00d2\2\2\u047e")
        buf.write("\u047f\7\u00d6\2\2\u047f\u0480\7\u008b\2\2\u0480\u04c0")
        buf.write("\3\2\2\2\u0481\u0482\7\21\2\2\u0482\u0483\7\u00e1\2\2")
        buf.write("\u0483\u0484\5\u00b0Y\2\u0484\u0485\7T\2\2\u0485\u0486")
        buf.write("\7\u00a9\2\2\u0486\u04c0\3\2\2\2\u0487\u0488\7\21\2\2")
        buf.write("\u0488\u0489\7\u00e1\2\2\u0489\u048a\5\u00b0Y\2\u048a")
        buf.write("\u048b\7\26\2\2\u048b\u048c\7\u00a9\2\2\u048c\u04c0\3")
        buf.write("\2\2\2\u048d\u048e\7\21\2\2\u048e\u048f\7\u00e1\2\2\u048f")
        buf.write("\u0490\5\u00b0Y\2\u0490\u0491\7\u00f3\2\2\u0491\u0492")
        buf.write("\7\u00a9\2\2\u0492\u04c0\3\2\2\2\u0493\u0494\7\21\2\2")
        buf.write("\u0494\u0495\7\u00e1\2\2\u0495\u0496\5\u00b0Y\2\u0496")
        buf.write("\u0497\7\u00ea\2\2\u0497\u04c0\3\2\2\2\u0498\u0499\7\21")
        buf.write("\2\2\u0499\u049a\7\u00e1\2\2\u049a\u049c\5\u00b0Y\2\u049b")
        buf.write("\u049d\5(\25\2\u049c\u049b\3\2\2\2\u049c\u049d\3\2\2\2")
        buf.write("\u049d\u049e\3\2\2\2\u049e\u049f\7\61\2\2\u049f\u04c0")
        buf.write("\3\2\2\2\u04a0\u04a1\7\21\2\2\u04a1\u04a2\7\u00e1\2\2")
        buf.write("\u04a2\u04a4\5\u00b0Y\2\u04a3\u04a5\5(\25\2\u04a4\u04a3")
        buf.write("\3\2\2\2\u04a4\u04a5\3\2\2\2\u04a5\u04a6\3\2\2\2\u04a6")
        buf.write("\u04a7\7\64\2\2\u04a7\u04c0\3\2\2\2\u04a8\u04a9\7\21\2")
        buf.write("\2\u04a9\u04aa\7\u00e1\2\2\u04aa\u04ac\5\u00b0Y\2\u04ab")
        buf.write("\u04ad\5(\25\2\u04ac\u04ab\3\2\2\2\u04ac\u04ad\3\2\2\2")
        buf.write("\u04ad\u04ae\3\2\2\2\u04ae\u04af\7\u00d2\2\2\u04af\u04b0")
        buf.write("\7_\2\2\u04b0\u04c0\3\2\2\2\u04b1\u04b2\7\21\2\2\u04b2")
        buf.write("\u04b3\7\u00e1\2\2\u04b3\u04b5\5\u00b0Y\2\u04b4\u04b6")
        buf.write("\5(\25\2\u04b5\u04b4\3\2\2\2\u04b5\u04b6\3\2\2\2\u04b6")
        buf.write("\u04b7\3\2\2\2\u04b7\u04b8\7\u00bf\2\2\u04b8\u04b9\7.")
        buf.write("\2\2\u04b9\u04c0\3\2\2\2\u04ba\u04bb\7\u00da\2\2\u04bb")
        buf.write("\u04c0\7\u00ec\2\2\u04bc\u04c0\7\60\2\2\u04bd\u04c0\7")
        buf.write("\u00c7\2\2\u04be\u04c0\7H\2\2\u04bf\u0417\3\2\2\2\u04bf")
        buf.write("\u0419\3\2\2\2\u04bf\u041b\3\2\2\2\u04bf\u041f\3\2\2\2")
        buf.write("\u04bf\u0423\3\2\2\2\u04bf\u0425\3\2\2\2\u04bf\u042a\3")
        buf.write("\2\2\2\u04bf\u042c\3\2\2\2\u04bf\u042e\3\2\2\2\u04bf\u0431")
        buf.write("\3\2\2\2\u04bf\u0433\3\2\2\2\u04bf\u0435\3\2\2\2\u04bf")
        buf.write("\u0437\3\2\2\2\u04bf\u043a\3\2\2\2\u04bf\u043c\3\2\2\2")
        buf.write("\u04bf\u043e\3\2\2\2\u04bf\u0440\3\2\2\2\u04bf\u0442\3")
        buf.write("\2\2\2\u04bf\u0444\3\2\2\2\u04bf\u0446\3\2\2\2\u04bf\u0448")
        buf.write("\3\2\2\2\u04bf\u044a\3\2\2\2\u04bf\u044c\3\2\2\2\u04bf")
        buf.write("\u044e\3\2\2\2\u04bf\u0451\3\2\2\2\u04bf\u0454\3\2\2\2")
        buf.write("\u04bf\u045a\3\2\2\2\u04bf\u0460\3\2\2\2\u04bf\u0466\3")
        buf.write("\2\2\2\u04bf\u046c\3\2\2\2\u04bf\u0472\3\2\2\2\u04bf\u047a")
        buf.write("\3\2\2\2\u04bf\u0481\3\2\2\2\u04bf\u0487\3\2\2\2\u04bf")
        buf.write("\u048d\3\2\2\2\u04bf\u0493\3\2\2\2\u04bf\u0498\3\2\2\2")
        buf.write("\u04bf\u04a0\3\2\2\2\u04bf\u04a8\3\2\2\2\u04bf\u04b1\3")
        buf.write("\2\2\2\u04bf\u04ba\3\2\2\2\u04bf\u04bc\3\2\2\2\u04bf\u04bd")
        buf.write("\3\2\2\2\u04bf\u04be\3\2\2\2\u04c0\25\3\2\2\2\u04c1\u04c3")
        buf.write("\7\67\2\2\u04c2\u04c4\7\u00e5\2\2\u04c3\u04c2\3\2\2\2")
        buf.write("\u04c3\u04c4\3\2\2\2\u04c4\u04c6\3\2\2\2\u04c5\u04c7\7")
        buf.write("Y\2\2\u04c6\u04c5\3\2\2\2\u04c6\u04c7\3\2\2\2\u04c7\u04c8")
        buf.write("\3\2\2\2\u04c8\u04cc\7\u00e1\2\2\u04c9\u04ca\7o\2\2\u04ca")
        buf.write("\u04cb\7\u0098\2\2\u04cb\u04cd\7U\2\2\u04cc\u04c9\3\2")
        buf.write("\2\2\u04cc\u04cd\3\2\2\2\u04cd\u04ce\3\2\2\2\u04ce\u04cf")
        buf.write("\5\u00aeX\2\u04cf\27\3\2\2\2\u04d0\u04d1\7\67\2\2\u04d1")
        buf.write("\u04d3\7\u00a0\2\2\u04d2\u04d0\3\2\2\2\u04d2\u04d3\3\2")
        buf.write("\2\2\u04d3\u04d4\3\2\2\2\u04d4\u04d5\7\u00bf\2\2\u04d5")
        buf.write("\u04d6\7\u00e1\2\2\u04d6\u04d7\5\u00aeX\2\u04d7\31\3\2")
        buf.write("\2\2\u04d8\u04d9\7)\2\2\u04d9\u04da\7 \2\2\u04da\u04de")
        buf.write("\5\u0096L\2\u04db\u04dc\7\u00d9\2\2\u04dc\u04dd\7 \2\2")
        buf.write("\u04dd\u04df\5\u009aN\2\u04de\u04db\3\2\2\2\u04de\u04df")
        buf.write("\3\2\2\2\u04df\u04e0\3\2\2\2\u04e0\u04e1\7{\2\2\u04e1")
        buf.write("\u04e2\7\u011d\2\2\u04e2\u04e3\7\37\2\2\u04e3\33\3\2\2")
        buf.write("\2\u04e4\u04e5\7\u00d6\2\2\u04e5\u04e6\7 \2\2\u04e6\u04e7")
        buf.write("\5\u0096L\2\u04e7\u04ea\7\u009c\2\2\u04e8\u04eb\5B\"\2")
        buf.write("\u04e9\u04eb\5D#\2\u04ea\u04e8\3\2\2\2\u04ea\u04e9\3\2")
        buf.write("\2\2\u04eb\u04ef\3\2\2\2\u04ec\u04ed\7\u00dc\2\2\u04ed")
        buf.write("\u04ee\7\30\2\2\u04ee\u04f0\7I\2\2\u04ef\u04ec\3\2\2\2")
        buf.write("\u04ef\u04f0\3\2\2\2\u04f0\35\3\2\2\2\u04f1\u04f2\7\u008b")
        buf.write("\2\2\u04f2\u04f3\7\u0119\2\2\u04f3\37\3\2\2\2\u04f4\u04f5")
        buf.write("\7/\2\2\u04f5\u04f6\7\u0119\2\2\u04f6!\3\2\2\2\u04f7\u04f9")
        buf.write("\5\62\32\2\u04f8\u04f7\3\2\2\2\u04f8\u04f9\3\2\2\2\u04f9")
        buf.write("\u04fa\3\2\2\2\u04fa\u04fb\5T+\2\u04fb\u04fc\5P)\2\u04fc")
        buf.write("#\3\2\2\2\u04fd\u04fe\7x\2\2\u04fe\u0500\7\u00a8\2\2\u04ff")
        buf.write("\u0501\7\u00e1\2\2\u0500\u04ff\3\2\2\2\u0500\u0501\3\2")
        buf.write("\2\2\u0501\u0502\3\2\2\2\u0502\u0509\5\u00aeX\2\u0503")
        buf.write("\u0507\5(\25\2\u0504\u0505\7o\2\2\u0505\u0506\7\u0098")
        buf.write("\2\2\u0506\u0508\7U\2\2\u0507\u0504\3\2\2\2\u0507\u0508")
        buf.write("\3\2\2\2\u0508\u050a\3\2\2\2\u0509\u0503\3\2\2\2\u0509")
        buf.write("\u050a\3\2\2\2\u050a\u0535\3\2\2\2\u050b\u050c\7x\2\2")
        buf.write("\u050c\u050e\7{\2\2\u050d\u050f\7\u00e1\2\2\u050e\u050d")
        buf.write("\3\2\2\2\u050e\u050f\3\2\2\2\u050f\u0510\3\2\2\2\u0510")
        buf.write("\u0512\5\u00aeX\2\u0511\u0513\5(\25\2\u0512\u0511\3\2")
        buf.write("\2\2\u0512\u0513\3\2\2\2\u0513\u0517\3\2\2\2\u0514\u0515")
        buf.write("\7o\2\2\u0515\u0516\7\u0098\2\2\u0516\u0518\7U\2\2\u0517")
        buf.write("\u0514\3\2\2\2\u0517\u0518\3\2\2\2\u0518\u0535\3\2\2\2")
        buf.write("\u0519\u051a\7x\2\2\u051a\u051c\7\u00a8\2\2\u051b\u051d")
        buf.write("\7\u008a\2\2\u051c\u051b\3\2\2\2\u051c\u051d\3\2\2\2\u051d")
        buf.write("\u051e\3\2\2\2\u051e\u051f\7J\2\2\u051f\u0521\7\u0119")
        buf.write("\2\2\u0520\u0522\5\u00aaV\2\u0521\u0520\3\2\2\2\u0521")
        buf.write("\u0522\3\2\2\2\u0522\u0524\3\2\2\2\u0523\u0525\5F$\2\u0524")
        buf.write("\u0523\3\2\2\2\u0524\u0525\3\2\2\2\u0525\u0535\3\2\2\2")
        buf.write("\u0526\u0527\7x\2\2\u0527\u0529\7\u00a8\2\2\u0528\u052a")
        buf.write("\7\u008a\2\2\u0529\u0528\3\2\2\2\u0529\u052a\3\2\2\2\u052a")
        buf.write("\u052b\3\2\2\2\u052b\u052d\7J\2\2\u052c\u052e\7\u0119")
        buf.write("\2\2\u052d\u052c\3\2\2\2\u052d\u052e\3\2\2\2\u052e\u052f")
        buf.write("\3\2\2\2\u052f\u0532\5\66\34\2\u0530\u0531\7\u009f\2\2")
        buf.write("\u0531\u0533\5:\36\2\u0532\u0530\3\2\2\2\u0532\u0533\3")
        buf.write("\2\2\2\u0533\u0535\3\2\2\2\u0534\u04fd\3\2\2\2\u0534\u050b")
        buf.write("\3\2\2\2\u0534\u0519\3\2\2\2\u0534\u0526\3\2\2\2\u0535")
        buf.write("%\3\2\2\2\u0536\u0538\5(\25\2\u0537\u0539\5\36\20\2\u0538")
        buf.write("\u0537\3\2\2\2\u0538\u0539\3\2\2\2\u0539\'\3\2\2\2\u053a")
        buf.write("\u053b\7\u00a9\2\2\u053b\u053c\7\4\2\2\u053c\u0541\5*")
        buf.write("\26\2\u053d\u053e\7\6\2\2\u053e\u0540\5*\26\2\u053f\u053d")
        buf.write("\3\2\2\2\u0540\u0543\3\2\2\2\u0541\u053f\3\2\2\2\u0541")
        buf.write("\u0542\3\2\2\2\u0542\u0544\3\2\2\2\u0543\u0541\3\2\2\2")
        buf.write("\u0544\u0545\7\5\2\2\u0545)\3\2\2\2\u0546\u0549\5\u0102")
        buf.write("\u0082\2\u0547\u0548\7\u0107\2\2\u0548\u054a\5\u00c8e")
        buf.write("\2\u0549\u0547\3\2\2\2\u0549\u054a\3\2\2\2\u054a+\3\2")
        buf.write("\2\2\u054b\u054c\t\17\2\2\u054c-\3\2\2\2\u054d\u0553\5")
        buf.write("\u00fe\u0080\2\u054e\u0553\7\u0119\2\2\u054f\u0553\5\u00ca")
        buf.write("f\2\u0550\u0553\5\u00ccg\2\u0551\u0553\5\u00ceh\2\u0552")
        buf.write("\u054d\3\2\2\2\u0552\u054e\3\2\2\2\u0552\u054f\3\2\2\2")
        buf.write("\u0552\u0550\3\2\2\2\u0552\u0551\3\2\2\2\u0553/\3\2\2")
        buf.write("\2\u0554\u0559\5\u0102\u0082\2\u0555\u0556\7\7\2\2\u0556")
        buf.write("\u0558\5\u0102\u0082\2\u0557\u0555\3\2\2\2\u0558\u055b")
        buf.write("\3\2\2\2\u0559\u0557\3\2\2\2\u0559\u055a\3\2\2\2\u055a")
        buf.write("\61\3\2\2\2\u055b\u0559\3\2\2\2\u055c\u055d\7\u0105\2")
        buf.write("\2\u055d\u0562\5\64\33\2\u055e\u055f\7\6\2\2\u055f\u0561")
        buf.write("\5\64\33\2\u0560\u055e\3\2\2\2\u0561\u0564\3\2\2\2\u0562")
        buf.write("\u0560\3\2\2\2\u0562\u0563\3\2\2\2\u0563\63\3\2\2\2\u0564")
        buf.write("\u0562\3\2\2\2\u0565\u0567\5\u0100\u0081\2\u0566\u0568")
        buf.write("\5\u0096L\2\u0567\u0566\3\2\2\2\u0567\u0568\3\2\2\2\u0568")
        buf.write("\u056a\3\2\2\2\u0569\u056b\7\30\2\2\u056a\u0569\3\2\2")
        buf.write("\2\u056a\u056b\3\2\2\2\u056b\u056c\3\2\2\2\u056c\u056d")
        buf.write("\7\4\2\2\u056d\u056e\5\"\22\2\u056e\u056f\7\5\2\2\u056f")
        buf.write("\65\3\2\2\2\u0570\u0571\7\u00fe\2\2\u0571\u0572\5\u00ae")
        buf.write("X\2\u0572\67\3\2\2\2\u0573\u0574\7\u009f\2\2\u0574\u057e")
        buf.write("\5:\36\2\u0575\u0576\7\u00aa\2\2\u0576\u0577\7 \2\2\u0577")
        buf.write("\u057e\5\u00b8]\2\u0578\u057e\5\32\16\2\u0579\u057e\5")
        buf.write("\36\20\2\u057a\u057e\5 \21\2\u057b\u057c\7\u00e4\2\2\u057c")
        buf.write("\u057e\5:\36\2\u057d\u0573\3\2\2\2\u057d\u0575\3\2\2\2")
        buf.write("\u057d\u0578\3\2\2\2\u057d\u0579\3\2\2\2\u057d\u057a\3")
        buf.write("\2\2\2\u057d\u057b\3\2\2\2\u057e\u0581\3\2\2\2\u057f\u057d")
        buf.write("\3\2\2\2\u057f\u0580\3\2\2\2\u05809\3\2\2\2\u0581\u057f")
        buf.write("\3\2\2\2\u0582\u0583\7\4\2\2\u0583\u0588\5<\37\2\u0584")
        buf.write("\u0585\7\6\2\2\u0585\u0587\5<\37\2\u0586\u0584\3\2\2\2")
        buf.write("\u0587\u058a\3\2\2\2\u0588\u0586\3\2\2\2\u0588\u0589\3")
        buf.write("\2\2\2\u0589\u058b\3\2\2\2\u058a\u0588\3\2\2\2\u058b\u058c")
        buf.write("\7\5\2\2\u058c;\3\2\2\2\u058d\u0592\5> \2\u058e\u0590")
        buf.write("\7\u0107\2\2\u058f\u058e\3\2\2\2\u058f\u0590\3\2\2\2\u0590")
        buf.write("\u0591\3\2\2\2\u0591\u0593\5@!\2\u0592\u058f\3\2\2\2\u0592")
        buf.write("\u0593\3\2\2\2\u0593=\3\2\2\2\u0594\u0599\5\u0102\u0082")
        buf.write("\2\u0595\u0596\7\7\2\2\u0596\u0598\5\u0102\u0082\2\u0597")
        buf.write("\u0595\3\2\2\2\u0598\u059b\3\2\2\2\u0599\u0597\3\2\2\2")
        buf.write("\u0599\u059a\3\2\2\2\u059a\u059e\3\2\2\2\u059b\u0599\3")
        buf.write("\2\2\2\u059c\u059e\7\u0119\2\2\u059d\u0594\3\2\2\2\u059d")
        buf.write("\u059c\3\2\2\2\u059e?\3\2\2\2\u059f\u05a4\7\u011d\2\2")
        buf.write("\u05a0\u05a4\7\u011f\2\2\u05a1\u05a4\5\u00d0i\2\u05a2")
        buf.write("\u05a4\7\u0119\2\2\u05a3\u059f\3\2\2\2\u05a3\u05a0\3\2")
        buf.write("\2\2\u05a3\u05a1\3\2\2\2\u05a3\u05a2\3\2\2\2\u05a4A\3")
        buf.write("\2\2\2\u05a5\u05a6\7\4\2\2\u05a6\u05ab\5\u00c8e\2\u05a7")
        buf.write("\u05a8\7\6\2\2\u05a8\u05aa\5\u00c8e\2\u05a9\u05a7\3\2")
        buf.write("\2\2\u05aa\u05ad\3\2\2\2\u05ab\u05a9\3\2\2\2\u05ab\u05ac")
        buf.write("\3\2\2\2\u05ac\u05ae\3\2\2\2\u05ad\u05ab\3\2\2\2\u05ae")
        buf.write("\u05af\7\5\2\2\u05afC\3\2\2\2\u05b0\u05b1\7\4\2\2\u05b1")
        buf.write("\u05b6\5B\"\2\u05b2\u05b3\7\6\2\2\u05b3\u05b5\5B\"\2\u05b4")
        buf.write("\u05b2\3\2\2\2\u05b5\u05b8\3\2\2\2\u05b6\u05b4\3\2\2\2")
        buf.write("\u05b6\u05b7\3\2\2\2\u05b7\u05b9\3\2\2\2\u05b8\u05b6\3")
        buf.write("\2\2\2\u05b9\u05ba\7\5\2\2\u05baE\3\2\2\2\u05bb\u05bc")
        buf.write("\7\u00dc\2\2\u05bc\u05bd\7\30\2\2\u05bd\u05c2\5H%\2\u05be")
        buf.write("\u05bf\7\u00dc\2\2\u05bf\u05c0\7 \2\2\u05c0\u05c2\5J&")
        buf.write("\2\u05c1\u05bb\3\2\2\2\u05c1\u05be\3\2\2\2\u05c2G\3\2")
        buf.write("\2\2\u05c3\u05c4\7w\2\2\u05c4\u05c5\7\u0119\2\2\u05c5")
        buf.write("\u05c6\7\u00a4\2\2\u05c6\u05c9\7\u0119\2\2\u05c7\u05c9")
        buf.write("\5\u0102\u0082\2\u05c8\u05c3\3\2\2\2\u05c8\u05c7\3\2\2")
        buf.write("\2\u05c9I\3\2\2\2\u05ca\u05ce\7\u0119\2\2\u05cb\u05cc")
        buf.write("\7\u0105\2\2\u05cc\u05cd\7\u00d0\2\2\u05cd\u05cf\5:\36")
        buf.write("\2\u05ce\u05cb\3\2\2\2\u05ce\u05cf\3\2\2\2\u05cfK\3\2")
        buf.write("\2\2\u05d0\u05d1\5\u0102\u0082\2\u05d1\u05d2\7\u0119\2")
        buf.write("\2\u05d2M\3\2\2\2\u05d3\u05d4\5$\23\2\u05d4\u05d5\5T+")
        buf.write("\2\u05d5\u05d6\5P)\2\u05d6\u0607\3\2\2\2\u05d7\u05d9\5")
        buf.write("z>\2\u05d8\u05da\5R*\2\u05d9\u05d8\3\2\2\2\u05da\u05db")
        buf.write("\3\2\2\2\u05db\u05d9\3\2\2\2\u05db\u05dc\3\2\2\2\u05dc")
        buf.write("\u0607\3\2\2\2\u05dd\u05de\7D\2\2\u05de\u05df\7f\2\2\u05df")
        buf.write("\u05e0\5\u00aeX\2\u05e0\u05e2\5\u00a8U\2\u05e1\u05e3\5")
        buf.write("r:\2\u05e2\u05e1\3\2\2\2\u05e2\u05e3\3\2\2\2\u05e3\u0607")
        buf.write("\3\2\2\2\u05e4\u05e5\7\u00fb\2\2\u05e5\u05e6\5\u00aeX")
        buf.write("\2\u05e6\u05e7\5\u00a8U\2\u05e7\u05e9\5d\63\2\u05e8\u05ea")
        buf.write("\5r:\2\u05e9\u05e8\3\2\2\2\u05e9\u05ea\3\2\2\2\u05ea\u0607")
        buf.write("\3\2\2\2\u05eb\u05ec\7\u0092\2\2\u05ec\u05ed\7{\2\2\u05ed")
        buf.write("\u05ee\5\u00aeX\2\u05ee\u05ef\5\u00a8U\2\u05ef\u05f5\7")
        buf.write("\u00fe\2\2\u05f0\u05f6\5\u00aeX\2\u05f1\u05f2\7\4\2\2")
        buf.write("\u05f2\u05f3\5\"\22\2\u05f3\u05f4\7\5\2\2\u05f4\u05f6")
        buf.write("\3\2\2\2\u05f5\u05f0\3\2\2\2\u05f5\u05f1\3\2\2\2\u05f6")
        buf.write("\u05f7\3\2\2\2\u05f7\u05f8\5\u00a8U\2\u05f8\u05f9\7\u009c")
        buf.write("\2\2\u05f9\u05fd\5\u00c0a\2\u05fa\u05fc\5f\64\2\u05fb")
        buf.write("\u05fa\3\2\2\2\u05fc\u05ff\3\2\2\2\u05fd\u05fb\3\2\2\2")
        buf.write("\u05fd\u05fe\3\2\2\2\u05fe\u0603\3\2\2\2\u05ff\u05fd\3")
        buf.write("\2\2\2\u0600\u0602\5h\65\2\u0601\u0600\3\2\2\2\u0602\u0605")
        buf.write("\3\2\2\2\u0603\u0601\3\2\2\2\u0603\u0604\3\2\2\2\u0604")
        buf.write("\u0607\3\2\2\2\u0605\u0603\3\2\2\2\u0606\u05d3\3\2\2\2")
        buf.write("\u0606\u05d7\3\2\2\2\u0606\u05dd\3\2\2\2\u0606\u05e4\3")
        buf.write("\2\2\2\u0606\u05eb\3\2\2\2\u0607O\3\2\2\2\u0608\u0609")
        buf.write("\7\u00a1\2\2\u0609\u060a\7 \2\2\u060a\u060f\5X-\2\u060b")
        buf.write("\u060c\7\6\2\2\u060c\u060e\5X-\2\u060d\u060b\3\2\2\2\u060e")
        buf.write("\u0611\3\2\2\2\u060f\u060d\3\2\2\2\u060f\u0610\3\2\2\2")
        buf.write("\u0610\u0613\3\2\2\2\u0611\u060f\3\2\2\2\u0612\u0608\3")
        buf.write("\2\2\2\u0612\u0613\3\2\2\2\u0613\u061e\3\2\2\2\u0614\u0615")
        buf.write("\7(\2\2\u0615\u0616\7 \2\2\u0616\u061b\5\u00be`\2\u0617")
        buf.write("\u0618\7\6\2\2\u0618\u061a\5\u00be`\2\u0619\u0617\3\2")
        buf.write("\2\2\u061a\u061d\3\2\2\2\u061b\u0619\3\2\2\2\u061b\u061c")
        buf.write("\3\2\2\2\u061c\u061f\3\2\2\2\u061d\u061b\3\2\2\2\u061e")
        buf.write("\u0614\3\2\2\2\u061e\u061f\3\2\2\2\u061f\u062a\3\2\2\2")
        buf.write("\u0620\u0621\7L\2\2\u0621\u0622\7 \2\2\u0622\u0627\5\u00be")
        buf.write("`\2\u0623\u0624\7\6\2\2\u0624\u0626\5\u00be`\2\u0625\u0623")
        buf.write("\3\2\2\2\u0626\u0629\3\2\2\2\u0627\u0625\3\2\2\2\u0627")
        buf.write("\u0628\3\2\2\2\u0628\u062b\3\2\2\2\u0629\u0627\3\2\2\2")
        buf.write("\u062a\u0620\3\2\2\2\u062a\u062b\3\2\2\2\u062b\u0636\3")
        buf.write("\2\2\2\u062c\u062d\7\u00d8\2\2\u062d\u062e\7 \2\2\u062e")
        buf.write("\u0633\5X-\2\u062f\u0630\7\6\2\2\u0630\u0632\5X-\2\u0631")
        buf.write("\u062f\3\2\2\2\u0632\u0635\3\2\2\2\u0633\u0631\3\2\2\2")
        buf.write("\u0633\u0634\3\2\2\2\u0634\u0637\3\2\2\2\u0635\u0633\3")
        buf.write("\2\2\2\u0636\u062c\3\2\2\2\u0636\u0637\3\2\2\2\u0637\u0639")
        buf.write("\3\2\2\2\u0638\u063a\5\u00f0y\2\u0639\u0638\3\2\2\2\u0639")
        buf.write("\u063a\3\2\2\2\u063a\u0640\3\2\2\2\u063b\u063e\7\u0086")
        buf.write("\2\2\u063c\u063f\7\20\2\2\u063d\u063f\5\u00be`\2\u063e")
        buf.write("\u063c\3\2\2\2\u063e\u063d\3\2\2\2\u063f\u0641\3\2\2\2")
        buf.write("\u0640\u063b\3\2\2\2\u0640\u0641\3\2\2\2\u0641Q\3\2\2")
        buf.write("\2\u0642\u0643\5$\23\2\u0643\u0644\5\\/\2\u0644S\3\2\2")
        buf.write("\2\u0645\u0646\b+\1\2\u0646\u0647\5V,\2\u0647\u0656\3")
        buf.write("\2\2\2\u0648\u0649\f\4\2\2\u0649\u064b\7y\2\2\u064a\u064c")
        buf.write("\5\u0088E\2\u064b\u064a\3\2\2\2\u064b\u064c\3\2\2\2\u064c")
        buf.write("\u064d\3\2\2\2\u064d\u0655\5T+\5\u064e\u064f\f\3\2\2\u064f")
        buf.write("\u0651\t\20\2\2\u0650\u0652\5\u0088E\2\u0651\u0650\3\2")
        buf.write("\2\2\u0651\u0652\3\2\2\2\u0652\u0653\3\2\2\2\u0653\u0655")
        buf.write("\5T+\4\u0654\u0648\3\2\2\2\u0654\u064e\3\2\2\2\u0655\u0658")
        buf.write("\3\2\2\2\u0656\u0654\3\2\2\2\u0656\u0657\3\2\2\2\u0657")
        buf.write("U\3\2\2\2\u0658\u0656\3\2\2\2\u0659\u0663\5^\60\2\u065a")
        buf.write("\u0663\5Z.\2\u065b\u065c\7\u00e1\2\2\u065c\u0663\5\u00ae")
        buf.write("X\2\u065d\u0663\5\u00a4S\2\u065e\u065f\7\4\2\2\u065f\u0660")
        buf.write("\5\"\22\2\u0660\u0661\7\5\2\2\u0661\u0663\3\2\2\2\u0662")
        buf.write("\u0659\3\2\2\2\u0662\u065a\3\2\2\2\u0662\u065b\3\2\2\2")
        buf.write("\u0662\u065d\3\2\2\2\u0662\u065e\3\2\2\2\u0663W\3\2\2")
        buf.write("\2\u0664\u0666\5\u00be`\2\u0665\u0667\t\21\2\2\u0666\u0665")
        buf.write("\3\2\2\2\u0666\u0667\3\2\2\2\u0667\u066a\3\2\2\2\u0668")
        buf.write("\u0669\7\u009a\2\2\u0669\u066b\t\22\2\2\u066a\u0668\3")
        buf.write("\2\2\2\u066a\u066b\3\2\2\2\u066bY\3\2\2\2\u066c\u066e")
        buf.write("\5z>\2\u066d\u066f\5\\/\2\u066e\u066d\3\2\2\2\u066f\u0670")
        buf.write("\3\2\2\2\u0670\u066e\3\2\2\2\u0670\u0671\3\2\2\2\u0671")
        buf.write("[\3\2\2\2\u0672\u0674\5`\61\2\u0673\u0675\5r:\2\u0674")
        buf.write("\u0673\3\2\2\2\u0674\u0675\3\2\2\2\u0675\u0676\3\2\2\2")
        buf.write("\u0676\u0677\5P)\2\u0677\u068e\3\2\2\2\u0678\u067c\5b")
        buf.write("\62\2\u0679\u067b\5\u0086D\2\u067a\u0679\3\2\2\2\u067b")
        buf.write("\u067e\3\2\2\2\u067c\u067a\3\2\2\2\u067c\u067d\3\2\2\2")
        buf.write("\u067d\u0680\3\2\2\2\u067e\u067c\3\2\2\2\u067f\u0681\5")
        buf.write("r:\2\u0680\u067f\3\2\2\2\u0680\u0681\3\2\2\2\u0681\u0683")
        buf.write("\3\2\2\2\u0682\u0684\5|?\2\u0683\u0682\3\2\2\2\u0683\u0684")
        buf.write("\3\2\2\2\u0684\u0686\3\2\2\2\u0685\u0687\5t;\2\u0686\u0685")
        buf.write("\3\2\2\2\u0686\u0687\3\2\2\2\u0687\u0689\3\2\2\2\u0688")
        buf.write("\u068a\5\u00f0y\2\u0689\u0688\3\2\2\2\u0689\u068a\3\2")
        buf.write("\2\2\u068a\u068b\3\2\2\2\u068b\u068c\5P)\2\u068c\u068e")
        buf.write("\3\2\2\2\u068d\u0672\3\2\2\2\u068d\u0678\3\2\2\2\u068e")
        buf.write("]\3\2\2\2\u068f\u0691\5`\61\2\u0690\u0692\5z>\2\u0691")
        buf.write("\u0690\3\2\2\2\u0691\u0692\3\2\2\2\u0692\u0694\3\2\2\2")
        buf.write("\u0693\u0695\5r:\2\u0694\u0693\3\2\2\2\u0694\u0695\3\2")
        buf.write("\2\2\u0695\u06ad\3\2\2\2\u0696\u0698\5b\62\2\u0697\u0699")
        buf.write("\5z>\2\u0698\u0697\3\2\2\2\u0698\u0699\3\2\2\2\u0699\u069d")
        buf.write("\3\2\2\2\u069a\u069c\5\u0086D\2\u069b\u069a\3\2\2\2\u069c")
        buf.write("\u069f\3\2\2\2\u069d\u069b\3\2\2\2\u069d\u069e\3\2\2\2")
        buf.write("\u069e\u06a1\3\2\2\2\u069f\u069d\3\2\2\2\u06a0\u06a2\5")
        buf.write("r:\2\u06a1\u06a0\3\2\2\2\u06a1\u06a2\3\2\2\2\u06a2\u06a4")
        buf.write("\3\2\2\2\u06a3\u06a5\5|?\2\u06a4\u06a3\3\2\2\2\u06a4\u06a5")
        buf.write("\3\2\2\2\u06a5\u06a7\3\2\2\2\u06a6\u06a8\5t;\2\u06a7\u06a6")
        buf.write("\3\2\2\2\u06a7\u06a8\3\2\2\2\u06a8\u06aa\3\2\2\2\u06a9")
        buf.write("\u06ab\5\u00f0y\2\u06aa\u06a9\3\2\2\2\u06aa\u06ab\3\2")
        buf.write("\2\2\u06ab\u06ad\3\2\2\2\u06ac\u068f\3\2\2\2\u06ac\u0696")
        buf.write("\3\2\2\2\u06ad_\3\2\2\2\u06ae\u06af\7\u00cc\2\2\u06af")
        buf.write("\u06b0\7\u00ee\2\2\u06b0\u06b1\7\4\2\2\u06b1\u06b2\5\u00b6")
        buf.write("\\\2\u06b2\u06b3\7\5\2\2\u06b3\u06b9\3\2\2\2\u06b4\u06b5")
        buf.write("\7\u0090\2\2\u06b5\u06b9\5\u00b6\\\2\u06b6\u06b7\7\u00ba")
        buf.write("\2\2\u06b7\u06b9\5\u00b6\\\2\u06b8\u06ae\3\2\2\2\u06b8")
        buf.write("\u06b4\3\2\2\2\u06b8\u06b6\3\2\2\2\u06b9\u06bb\3\2\2\2")
        buf.write("\u06ba\u06bc\5\u00aaV\2\u06bb\u06ba\3\2\2\2\u06bb\u06bc")
        buf.write("\3\2\2\2\u06bc\u06bf\3\2\2\2\u06bd\u06be\7\u00b8\2\2\u06be")
        buf.write("\u06c0\7\u0119\2\2\u06bf\u06bd\3\2\2\2\u06bf\u06c0\3\2")
        buf.write("\2\2\u06c0\u06c1\3\2\2\2\u06c1\u06c2\7\u00fe\2\2\u06c2")
        buf.write("\u06cf\7\u0119\2\2\u06c3\u06cd\7\30\2\2\u06c4\u06ce\5")
        buf.write("\u0098M\2\u06c5\u06ce\5\u00e6t\2\u06c6\u06c9\7\4\2\2\u06c7")
        buf.write("\u06ca\5\u0098M\2\u06c8\u06ca\5\u00e6t\2\u06c9\u06c7\3")
        buf.write("\2\2\2\u06c9\u06c8\3\2\2\2\u06ca\u06cb\3\2\2\2\u06cb\u06cc")
        buf.write("\7\5\2\2\u06cc\u06ce\3\2\2\2\u06cd\u06c4\3\2\2\2\u06cd")
        buf.write("\u06c5\3\2\2\2\u06cd\u06c6\3\2\2\2\u06ce\u06d0\3\2\2\2")
        buf.write("\u06cf\u06c3\3\2\2\2\u06cf\u06d0\3\2\2\2\u06d0\u06d2\3")
        buf.write("\2\2\2\u06d1\u06d3\5\u00aaV\2\u06d2\u06d1\3\2\2\2\u06d2")
        buf.write("\u06d3\3\2\2\2\u06d3\u06d6\3\2\2\2\u06d4\u06d5\7\u00b7")
        buf.write("\2\2\u06d5\u06d7\7\u0119\2\2\u06d6\u06d4\3\2\2\2\u06d6")
        buf.write("\u06d7\3\2\2\2\u06d7a\3\2\2\2\u06d8\u06dc\7\u00cc\2\2")
        buf.write("\u06d9\u06db\5v<\2\u06da\u06d9\3\2\2\2\u06db\u06de\3\2")
        buf.write("\2\2\u06dc\u06da\3\2\2\2\u06dc\u06dd\3\2\2\2\u06dd\u06e0")
        buf.write("\3\2\2\2\u06de\u06dc\3\2\2\2\u06df\u06e1\5\u0088E\2\u06e0")
        buf.write("\u06df\3\2\2\2\u06e0\u06e1\3\2\2\2\u06e1\u06e2\3\2\2\2")
        buf.write("\u06e2\u06e3\5\u00b6\\\2\u06e3c\3\2\2\2\u06e4\u06e5\7")
        buf.write("\u00d2\2\2\u06e5\u06e6\5n8\2\u06e6e\3\2\2\2\u06e7\u06e8")
        buf.write("\7\u0102\2\2\u06e8\u06eb\7\u0091\2\2\u06e9\u06ea\7\23")
        buf.write("\2\2\u06ea\u06ec\5\u00c0a\2\u06eb\u06e9\3\2\2\2\u06eb")
        buf.write("\u06ec\3\2\2\2\u06ec\u06ed\3\2\2\2\u06ed\u06ee\7\u00e7")
        buf.write("\2\2\u06ee\u06ef\5j\66\2\u06efg\3\2\2\2\u06f0\u06f1\7")
        buf.write("\u0102\2\2\u06f1\u06f2\7\u0098\2\2\u06f2\u06f5\7\u0091")
        buf.write("\2\2\u06f3\u06f4\7\23\2\2\u06f4\u06f6\5\u00c0a\2\u06f5")
        buf.write("\u06f3\3\2\2\2\u06f5\u06f6\3\2\2\2\u06f6\u06f7\3\2\2\2")
        buf.write("\u06f7\u06f8\7\u00e7\2\2\u06f8\u06f9\5l\67\2\u06f9i\3")
        buf.write("\2\2\2\u06fa\u0702\7D\2\2\u06fb\u06fc\7\u00fb\2\2\u06fc")
        buf.write("\u06fd\7\u00d2\2\2\u06fd\u0702\7\u0111\2\2\u06fe\u06ff")
        buf.write("\7\u00fb\2\2\u06ff\u0700\7\u00d2\2\2\u0700\u0702\5n8\2")
        buf.write("\u0701\u06fa\3\2\2\2\u0701\u06fb\3\2\2\2\u0701\u06fe\3")
        buf.write("\2\2\2\u0702k\3\2\2\2\u0703\u0704\7x\2\2\u0704\u0716\7")
        buf.write("\u0111\2\2\u0705\u0706\7x\2\2\u0706\u0707\7\4\2\2\u0707")
        buf.write("\u0708\5\u00acW\2\u0708\u0709\7\5\2\2\u0709\u070a\7\u00ff")
        buf.write("\2\2\u070a\u070b\7\4\2\2\u070b\u0710\5\u00be`\2\u070c")
        buf.write("\u070d\7\6\2\2\u070d\u070f\5\u00be`\2\u070e\u070c\3\2")
        buf.write("\2\2\u070f\u0712\3\2\2\2\u0710\u070e\3\2\2\2\u0710\u0711")
        buf.write("\3\2\2\2\u0711\u0713\3\2\2\2\u0712\u0710\3\2\2\2\u0713")
        buf.write("\u0714\7\5\2\2\u0714\u0716\3\2\2\2\u0715\u0703\3\2\2\2")
        buf.write("\u0715\u0705\3\2\2\2\u0716m\3\2\2\2\u0717\u071c\5p9\2")
        buf.write("\u0718\u0719\7\6\2\2\u0719\u071b\5p9\2\u071a\u0718\3\2")
        buf.write("\2\2\u071b\u071e\3\2\2\2\u071c\u071a\3\2\2\2\u071c\u071d")
        buf.write("\3\2\2\2\u071do\3\2\2\2\u071e\u071c\3\2\2\2\u071f\u0720")
        buf.write("\5\u00aeX\2\u0720\u0721\7\u0107\2\2\u0721\u0722\5\u00be")
        buf.write("`\2\u0722q\3\2\2\2\u0723\u0724\7\u0103\2\2\u0724\u0725")
        buf.write("\5\u00c0a\2\u0725s\3\2\2\2\u0726\u0727\7n\2\2\u0727\u0728")
        buf.write("\5\u00c0a\2\u0728u\3\2\2\2\u0729\u072a\7\b\2\2\u072a\u0731")
        buf.write("\5x=\2\u072b\u072d\7\6\2\2\u072c\u072b\3\2\2\2\u072c\u072d")
        buf.write("\3\2\2\2\u072d\u072e\3\2\2\2\u072e\u0730\5x=\2\u072f\u072c")
        buf.write("\3\2\2\2\u0730\u0733\3\2\2\2\u0731\u072f\3\2\2\2\u0731")
        buf.write("\u0732\3\2\2\2\u0732\u0734\3\2\2\2\u0733\u0731\3\2\2\2")
        buf.write("\u0734\u0735\7\t\2\2\u0735w\3\2\2\2\u0736\u0744\5\u0102")
        buf.write("\u0082\2\u0737\u0738\5\u0102\u0082\2\u0738\u0739\7\4\2")
        buf.write("\2\u0739\u073e\5\u00c6d\2\u073a\u073b\7\6\2\2\u073b\u073d")
        buf.write("\5\u00c6d\2\u073c\u073a\3\2\2\2\u073d\u0740\3\2\2\2\u073e")
        buf.write("\u073c\3\2\2\2\u073e\u073f\3\2\2\2\u073f\u0741\3\2\2\2")
        buf.write("\u0740\u073e\3\2\2\2\u0741\u0742\7\5\2\2\u0742\u0744\3")
        buf.write("\2\2\2\u0743\u0736\3\2\2\2\u0743\u0737\3\2\2\2\u0744y")
        buf.write("\3\2\2\2\u0745\u0746\7f\2\2\u0746\u074b\5\u008aF\2\u0747")
        buf.write("\u0748\7\6\2\2\u0748\u074a\5\u008aF\2\u0749\u0747\3\2")
        buf.write("\2\2\u074a\u074d\3\2\2\2\u074b\u0749\3\2\2\2\u074b\u074c")
        buf.write("\3\2\2\2\u074c\u0751\3\2\2\2\u074d\u074b\3\2\2\2\u074e")
        buf.write("\u0750\5\u0086D\2\u074f\u074e\3\2\2\2\u0750\u0753\3\2")
        buf.write("\2\2\u0751\u074f\3\2\2\2\u0751\u0752\3\2\2\2\u0752\u0755")
        buf.write("\3\2\2\2\u0753\u0751\3\2\2\2\u0754\u0756\5\u0080A\2\u0755")
        buf.write("\u0754\3\2\2\2\u0755\u0756\3\2\2\2\u0756{\3\2\2\2\u0757")
        buf.write("\u0758\7l\2\2\u0758\u0759\7 \2\2\u0759\u075e\5\u00be`")
        buf.write("\2\u075a\u075b\7\6\2\2\u075b\u075d\5\u00be`\2\u075c\u075a")
        buf.write("\3\2\2\2\u075d\u0760\3\2\2\2\u075e\u075c\3\2\2\2\u075e")
        buf.write("\u075f\3\2\2\2\u075f\u0772\3\2\2\2\u0760\u075e\3\2\2\2")
        buf.write("\u0761\u0762\7\u0105\2\2\u0762\u0773\7\u00c8\2\2\u0763")
        buf.write("\u0764\7\u0105\2\2\u0764\u0773\79\2\2\u0765\u0766\7m\2")
        buf.write("\2\u0766\u0767\7\u00d4\2\2\u0767\u0768\7\4\2\2\u0768\u076d")
        buf.write("\5~@\2\u0769\u076a\7\6\2\2\u076a\u076c\5~@\2\u076b\u0769")
        buf.write("\3\2\2\2\u076c\u076f\3\2\2\2\u076d\u076b\3\2\2\2\u076d")
        buf.write("\u076e\3\2\2\2\u076e\u0770\3\2\2\2\u076f\u076d\3\2\2\2")
        buf.write("\u0770\u0771\7\5\2\2\u0771\u0773\3\2\2\2\u0772\u0761\3")
        buf.write("\2\2\2\u0772\u0763\3\2\2\2\u0772\u0765\3\2\2\2\u0772\u0773")
        buf.write("\3\2\2\2\u0773\u0784\3\2\2\2\u0774\u0775\7l\2\2\u0775")
        buf.write("\u0776\7 \2\2\u0776\u0777\7m\2\2\u0777\u0778\7\u00d4\2")
        buf.write("\2\u0778\u0779\7\4\2\2\u0779\u077e\5~@\2\u077a\u077b\7")
        buf.write("\6\2\2\u077b\u077d\5~@\2\u077c\u077a\3\2\2\2\u077d\u0780")
        buf.write("\3\2\2\2\u077e\u077c\3\2\2\2\u077e\u077f\3\2\2\2\u077f")
        buf.write("\u0781\3\2\2\2\u0780\u077e\3\2\2\2\u0781\u0782\7\5\2\2")
        buf.write("\u0782\u0784\3\2\2\2\u0783\u0757\3\2\2\2\u0783\u0774\3")
        buf.write("\2\2\2\u0784}\3\2\2\2\u0785\u078e\7\4\2\2\u0786\u078b")
        buf.write("\5\u00be`\2\u0787\u0788\7\6\2\2\u0788\u078a\5\u00be`\2")
        buf.write("\u0789\u0787\3\2\2\2\u078a\u078d\3\2\2\2\u078b\u0789\3")
        buf.write("\2\2\2\u078b\u078c\3\2\2\2\u078c\u078f\3\2\2\2\u078d\u078b")
        buf.write("\3\2\2\2\u078e\u0786\3\2\2\2\u078e\u078f\3\2\2\2\u078f")
        buf.write("\u0790\3\2\2\2\u0790\u0793\7\5\2\2\u0791\u0793\5\u00be")
        buf.write("`\2\u0792\u0785\3\2\2\2\u0792\u0791\3\2\2\2\u0793\177")
        buf.write("\3\2\2\2\u0794\u0795\7\u00ad\2\2\u0795\u0796\7\4\2\2\u0796")
        buf.write("\u0797\5\u00b6\\\2\u0797\u0798\7b\2\2\u0798\u0799\5\u0082")
        buf.write("B\2\u0799\u079a\7r\2\2\u079a\u079b\7\4\2\2\u079b\u07a0")
        buf.write("\5\u0084C\2\u079c\u079d\7\6\2\2\u079d\u079f\5\u0084C\2")
        buf.write("\u079e\u079c\3\2\2\2\u079f\u07a2\3\2\2\2\u07a0\u079e\3")
        buf.write("\2\2\2\u07a0\u07a1\3\2\2\2\u07a1\u07a3\3\2\2\2\u07a2\u07a0")
        buf.write("\3\2\2\2\u07a3\u07a4\7\5\2\2\u07a4\u07a5\7\5\2\2\u07a5")
        buf.write("\u0081\3\2\2\2\u07a6\u07b3\5\u0102\u0082\2\u07a7\u07a8")
        buf.write("\7\4\2\2\u07a8\u07ad\5\u0102\u0082\2\u07a9\u07aa\7\6\2")
        buf.write("\2\u07aa\u07ac\5\u0102\u0082\2\u07ab\u07a9\3\2\2\2\u07ac")
        buf.write("\u07af\3\2\2\2\u07ad\u07ab\3\2\2\2\u07ad\u07ae\3\2\2\2")
        buf.write("\u07ae\u07b0\3\2\2\2\u07af\u07ad\3\2\2\2\u07b0\u07b1\7")
        buf.write("\5\2\2\u07b1\u07b3\3\2\2\2\u07b2\u07a6\3\2\2\2\u07b2\u07a7")
        buf.write("\3\2\2\2\u07b3\u0083\3\2\2\2\u07b4\u07b9\5\u00be`\2\u07b5")
        buf.write("\u07b7\7\30\2\2\u07b6\u07b5\3\2\2\2\u07b6\u07b7\3\2\2")
        buf.write("\2\u07b7\u07b8\3\2\2\2\u07b8\u07ba\5\u0102\u0082\2\u07b9")
        buf.write("\u07b6\3\2\2\2\u07b9\u07ba\3\2\2\2\u07ba\u0085\3\2\2\2")
        buf.write("\u07bb\u07bc\7\u0081\2\2\u07bc\u07be\7\u0100\2\2\u07bd")
        buf.write("\u07bf\7\u00a3\2\2\u07be\u07bd\3\2\2\2\u07be\u07bf\3\2")
        buf.write("\2\2\u07bf\u07c0\3\2\2\2\u07c0\u07c1\5\u00fe\u0080\2\u07c1")
        buf.write("\u07ca\7\4\2\2\u07c2\u07c7\5\u00be`\2\u07c3\u07c4\7\6")
        buf.write("\2\2\u07c4\u07c6\5\u00be`\2\u07c5\u07c3\3\2\2\2\u07c6")
        buf.write("\u07c9\3\2\2\2\u07c7\u07c5\3\2\2\2\u07c7\u07c8\3\2\2\2")
        buf.write("\u07c8\u07cb\3\2\2\2\u07c9\u07c7\3\2\2\2\u07ca\u07c2\3")
        buf.write("\2\2\2\u07ca\u07cb\3\2\2\2\u07cb\u07cc\3\2\2\2\u07cc\u07cd")
        buf.write("\7\5\2\2\u07cd\u07d9\5\u0102\u0082\2\u07ce\u07d0\7\30")
        buf.write("\2\2\u07cf\u07ce\3\2\2\2\u07cf\u07d0\3\2\2\2\u07d0\u07d1")
        buf.write("\3\2\2\2\u07d1\u07d6\5\u0102\u0082\2\u07d2\u07d3\7\6\2")
        buf.write("\2\u07d3\u07d5\5\u0102\u0082\2\u07d4\u07d2\3\2\2\2\u07d5")
        buf.write("\u07d8\3\2\2\2\u07d6\u07d4\3\2\2\2\u07d6\u07d7\3\2\2\2")
        buf.write("\u07d7\u07da\3\2\2\2\u07d8\u07d6\3\2\2\2\u07d9\u07cf\3")
        buf.write("\2\2\2\u07d9\u07da\3\2\2\2\u07da\u0087\3\2\2\2\u07db\u07dc")
        buf.write("\t\23\2\2\u07dc\u0089\3\2\2\2\u07dd\u07e1\5\u00a2R\2\u07de")
        buf.write("\u07e0\5\u008cG\2\u07df\u07de\3\2\2\2\u07e0\u07e3\3\2")
        buf.write("\2\2\u07e1\u07df\3\2\2\2\u07e1\u07e2\3\2\2\2\u07e2\u008b")
        buf.write("\3\2\2\2\u07e3\u07e1\3\2\2\2\u07e4\u07e5\5\u008eH\2\u07e5")
        buf.write("\u07e6\7~\2\2\u07e6\u07e8\5\u00a2R\2\u07e7\u07e9\5\u0090")
        buf.write("I\2\u07e8\u07e7\3\2\2\2\u07e8\u07e9\3\2\2\2\u07e9\u07f0")
        buf.write("\3\2\2\2\u07ea\u07eb\7\u0096\2\2\u07eb\u07ec\5\u008eH")
        buf.write("\2\u07ec\u07ed\7~\2\2\u07ed\u07ee\5\u00a2R\2\u07ee\u07f0")
        buf.write("\3\2\2\2\u07ef\u07e4\3\2\2\2\u07ef\u07ea\3\2\2\2\u07f0")
        buf.write("\u008d\3\2\2\2\u07f1\u07f3\7u\2\2\u07f2\u07f1\3\2\2\2")
        buf.write("\u07f2\u07f3\3\2\2\2\u07f3\u080a\3\2\2\2\u07f4\u080a\7")
        buf.write("8\2\2\u07f5\u07f7\7\u0084\2\2\u07f6\u07f8\7\u00a3\2\2")
        buf.write("\u07f7\u07f6\3\2\2\2\u07f7\u07f8\3\2\2\2\u07f8\u080a\3")
        buf.write("\2\2\2\u07f9\u07fb\7\u0084\2\2\u07fa\u07f9\3\2\2\2\u07fa")
        buf.write("\u07fb\3\2\2\2\u07fb\u07fc\3\2\2\2\u07fc\u080a\7\u00cd")
        buf.write("\2\2\u07fd\u07ff\7\u00c3\2\2\u07fe\u0800\7\u00a3\2\2\u07ff")
        buf.write("\u07fe\3\2\2\2\u07ff\u0800\3\2\2\2\u0800\u080a\3\2\2\2")
        buf.write("\u0801\u0803\7g\2\2\u0802\u0804\7\u00a3\2\2\u0803\u0802")
        buf.write("\3\2\2\2\u0803\u0804\3\2\2\2\u0804\u080a\3\2\2\2\u0805")
        buf.write("\u0807\7\u0084\2\2\u0806\u0805\3\2\2\2\u0806\u0807\3\2")
        buf.write("\2\2\u0807\u0808\3\2\2\2\u0808\u080a\7\24\2\2\u0809\u07f2")
        buf.write("\3\2\2\2\u0809\u07f4\3\2\2\2\u0809\u07f5\3\2\2\2\u0809")
        buf.write("\u07fa\3\2\2\2\u0809\u07fd\3\2\2\2\u0809\u0801\3\2\2\2")
        buf.write("\u0809\u0806\3\2\2\2\u080a\u008f\3\2\2\2\u080b\u080c\7")
        buf.write("\u009c\2\2\u080c\u0810\5\u00c0a\2\u080d\u080e\7\u00fe")
        buf.write("\2\2\u080e\u0810\5\u0096L\2\u080f\u080b\3\2\2\2\u080f")
        buf.write("\u080d\3\2\2\2\u0810\u0091\3\2\2\2\u0811\u0812\7\u00e3")
        buf.write("\2\2\u0812\u0814\7\4\2\2\u0813\u0815\5\u0094K\2\u0814")
        buf.write("\u0813\3\2\2\2\u0814\u0815\3\2\2\2\u0815\u0816\3\2\2\2")
        buf.write("\u0816\u0817\7\5\2\2\u0817\u0093\3\2\2\2\u0818\u081a\7")
        buf.write("\u0110\2\2\u0819\u0818\3\2\2\2\u0819\u081a\3\2\2\2\u081a")
        buf.write("\u081b\3\2\2\2\u081b\u081c\t\24\2\2\u081c\u0831\7\u00ac")
        buf.write("\2\2\u081d\u081e\5\u00be`\2\u081e\u081f\7\u00ca\2\2\u081f")
        buf.write("\u0831\3\2\2\2\u0820\u0821\7\36\2\2\u0821\u0822\7\u011d")
        buf.write("\2\2\u0822\u0823\7\u00a2\2\2\u0823\u0824\7\u009b\2\2\u0824")
        buf.write("\u082d\7\u011d\2\2\u0825\u082b\7\u009c\2\2\u0826\u082c")
        buf.write("\5\u0102\u0082\2\u0827\u0828\5\u00fe\u0080\2\u0828\u0829")
        buf.write("\7\4\2\2\u0829\u082a\7\5\2\2\u082a\u082c\3\2\2\2\u082b")
        buf.write("\u0826\3\2\2\2\u082b\u0827\3\2\2\2\u082c\u082e\3\2\2\2")
        buf.write("\u082d\u0825\3\2\2\2\u082d\u082e\3\2\2\2\u082e\u0831\3")
        buf.write("\2\2\2\u082f\u0831\5\u00be`\2\u0830\u0819\3\2\2\2\u0830")
        buf.write("\u081d\3\2\2\2\u0830\u0820\3\2\2\2\u0830\u082f\3\2\2\2")
        buf.write("\u0831\u0095\3\2\2\2\u0832\u0833\7\4\2\2\u0833\u0834\5")
        buf.write("\u0098M\2\u0834\u0835\7\5\2\2\u0835\u0097\3\2\2\2\u0836")
        buf.write("\u083b\5\u0100\u0081\2\u0837\u0838\7\6\2\2\u0838\u083a")
        buf.write("\5\u0100\u0081\2\u0839\u0837\3\2\2\2\u083a\u083d\3\2\2")
        buf.write("\2\u083b\u0839\3\2\2\2\u083b\u083c\3\2\2\2\u083c\u0099")
        buf.write("\3\2\2\2\u083d\u083b\3\2\2\2\u083e\u083f\7\4\2\2\u083f")
        buf.write("\u0844\5\u009cO\2\u0840\u0841\7\6\2\2\u0841\u0843\5\u009c")
        buf.write("O\2\u0842\u0840\3\2\2\2\u0843\u0846\3\2\2\2\u0844\u0842")
        buf.write("\3\2\2\2\u0844\u0845\3\2\2\2\u0845\u0847\3\2\2\2\u0846")
        buf.write("\u0844\3\2\2\2\u0847\u0848\7\5\2\2\u0848\u009b\3\2\2\2")
        buf.write("\u0849\u084b\5\u0100\u0081\2\u084a\u084c\t\21\2\2\u084b")
        buf.write("\u084a\3\2\2\2\u084b\u084c\3\2\2\2\u084c\u009d\3\2\2\2")
        buf.write("\u084d\u084e\7\4\2\2\u084e\u0853\5\u00a0Q\2\u084f\u0850")
        buf.write("\7\6\2\2\u0850\u0852\5\u00a0Q\2\u0851\u084f\3\2\2\2\u0852")
        buf.write("\u0855\3\2\2\2\u0853\u0851\3\2\2\2\u0853\u0854\3\2\2\2")
        buf.write("\u0854\u0856\3\2\2\2\u0855\u0853\3\2\2\2\u0856\u0857\7")
        buf.write("\5\2\2\u0857\u009f\3\2\2\2\u0858\u085a\5\u0102\u0082\2")
        buf.write("\u0859\u085b\5 \21\2\u085a\u0859\3\2\2\2\u085a\u085b\3")
        buf.write("\2\2\2\u085b\u00a1\3\2\2\2\u085c\u085e\5\u00aeX\2\u085d")
        buf.write("\u085f\5\u0092J\2\u085e\u085d\3\2\2\2\u085e\u085f\3\2")
        buf.write("\2\2\u085f\u0860\3\2\2\2\u0860\u0861\5\u00a8U\2\u0861")
        buf.write("\u0875\3\2\2\2\u0862\u0863\7\4\2\2\u0863\u0864\5\"\22")
        buf.write("\2\u0864\u0866\7\5\2\2\u0865\u0867\5\u0092J\2\u0866\u0865")
        buf.write("\3\2\2\2\u0866\u0867\3\2\2\2\u0867\u0868\3\2\2\2\u0868")
        buf.write("\u0869\5\u00a8U\2\u0869\u0875\3\2\2\2\u086a\u086b\7\4")
        buf.write("\2\2\u086b\u086c\5\u008aF\2\u086c\u086e\7\5\2\2\u086d")
        buf.write("\u086f\5\u0092J\2\u086e\u086d\3\2\2\2\u086e\u086f\3\2")
        buf.write("\2\2\u086f\u0870\3\2\2\2\u0870\u0871\5\u00a8U\2\u0871")
        buf.write("\u0875\3\2\2\2\u0872\u0875\5\u00a4S\2\u0873\u0875\5\u00a6")
        buf.write("T\2\u0874\u085c\3\2\2\2\u0874\u0862\3\2\2\2\u0874\u086a")
        buf.write("\3\2\2\2\u0874\u0872\3\2\2\2\u0874\u0873\3\2\2\2\u0875")
        buf.write("\u00a3\3\2\2\2\u0876\u0877\7\u00ff\2\2\u0877\u087c\5\u00be")
        buf.write("`\2\u0878\u0879\7\6\2\2\u0879\u087b\5\u00be`\2\u087a\u0878")
        buf.write("\3\2\2\2\u087b\u087e\3\2\2\2\u087c\u087a\3\2\2\2\u087c")
        buf.write("\u087d\3\2\2\2\u087d\u087f\3\2\2\2\u087e\u087c\3\2\2\2")
        buf.write("\u087f\u0880\5\u00a8U\2\u0880\u00a5\3\2\2\2\u0881\u0882")
        buf.write("\5\u0100\u0081\2\u0882\u088b\7\4\2\2\u0883\u0888\5\u00be")
        buf.write("`\2\u0884\u0885\7\6\2\2\u0885\u0887\5\u00be`\2\u0886\u0884")
        buf.write("\3\2\2\2\u0887\u088a\3\2\2\2\u0888\u0886\3\2\2\2\u0888")
        buf.write("\u0889\3\2\2\2\u0889\u088c\3\2\2\2\u088a\u0888\3\2\2\2")
        buf.write("\u088b\u0883\3\2\2\2\u088b\u088c\3\2\2\2\u088c\u088d\3")
        buf.write("\2\2\2\u088d\u088e\7\5\2\2\u088e\u088f\5\u00a8U\2\u088f")
        buf.write("\u00a7\3\2\2\2\u0890\u0892\7\30\2\2\u0891\u0890\3\2\2")
        buf.write("\2\u0891\u0892\3\2\2\2\u0892\u0893\3\2\2\2\u0893\u0895")
        buf.write("\5\u0104\u0083\2\u0894\u0896\5\u0096L\2\u0895\u0894\3")
        buf.write("\2\2\2\u0895\u0896\3\2\2\2\u0896\u0898\3\2\2\2\u0897\u0891")
        buf.write("\3\2\2\2\u0897\u0898\3\2\2\2\u0898\u00a9\3\2\2\2\u0899")
        buf.write("\u089a\7\u00c9\2\2\u089a\u089b\7d\2\2\u089b\u089c\7\u00cf")
        buf.write("\2\2\u089c\u08a0\7\u0119\2\2\u089d\u089e\7\u0105\2\2\u089e")
        buf.write("\u089f\7\u00d0\2\2\u089f\u08a1\5:\36\2\u08a0\u089d\3\2")
        buf.write("\2\2\u08a0\u08a1\3\2\2\2\u08a1\u08cb\3\2\2\2\u08a2\u08a3")
        buf.write("\7\u00c9\2\2\u08a3\u08a4\7d\2\2\u08a4\u08ae\7E\2\2\u08a5")
        buf.write("\u08a6\7]\2\2\u08a6\u08a7\7\u00e6\2\2\u08a7\u08a8\7 \2")
        buf.write("\2\u08a8\u08ac\7\u0119\2\2\u08a9\u08aa\7R\2\2\u08aa\u08ab")
        buf.write("\7 \2\2\u08ab\u08ad\7\u0119\2\2\u08ac\u08a9\3\2\2\2\u08ac")
        buf.write("\u08ad\3\2\2\2\u08ad\u08af\3\2\2\2\u08ae\u08a5\3\2\2\2")
        buf.write("\u08ae\u08af\3\2\2\2\u08af\u08b5\3\2\2\2\u08b0\u08b1\7")
        buf.write(",\2\2\u08b1\u08b2\7}\2\2\u08b2\u08b3\7\u00e6\2\2\u08b3")
        buf.write("\u08b4\7 \2\2\u08b4\u08b6\7\u0119\2\2\u08b5\u08b0\3\2")
        buf.write("\2\2\u08b5\u08b6\3\2\2\2\u08b6\u08bc\3\2\2\2\u08b7\u08b8")
        buf.write("\7\u0090\2\2\u08b8\u08b9\7\177\2\2\u08b9\u08ba\7\u00e6")
        buf.write("\2\2\u08ba\u08bb\7 \2\2\u08bb\u08bd\7\u0119\2\2\u08bc")
        buf.write("\u08b7\3\2\2\2\u08bc\u08bd\3\2\2\2\u08bd\u08c2\3\2\2\2")
        buf.write("\u08be\u08bf\7\u0087\2\2\u08bf\u08c0\7\u00e6\2\2\u08c0")
        buf.write("\u08c1\7 \2\2\u08c1\u08c3\7\u0119\2\2\u08c2\u08be\3\2")
        buf.write("\2\2\u08c2\u08c3\3\2\2\2\u08c3\u08c8\3\2\2\2\u08c4\u08c5")
        buf.write("\7\u0099\2\2\u08c5\u08c6\7C\2\2\u08c6\u08c7\7\30\2\2\u08c7")
        buf.write("\u08c9\7\u0119\2\2\u08c8\u08c4\3\2\2\2\u08c8\u08c9\3\2")
        buf.write("\2\2\u08c9\u08cb\3\2\2\2\u08ca\u0899\3\2\2\2\u08ca\u08a2")
        buf.write("\3\2\2\2\u08cb\u00ab\3\2\2\2\u08cc\u08d1\5\u00aeX\2\u08cd")
        buf.write("\u08ce\7\6\2\2\u08ce\u08d0\5\u00aeX\2\u08cf\u08cd\3\2")
        buf.write("\2\2\u08d0\u08d3\3\2\2\2\u08d1\u08cf\3\2\2\2\u08d1\u08d2")
        buf.write("\3\2\2\2\u08d2\u00ad\3\2\2\2\u08d3\u08d1\3\2\2\2\u08d4")
        buf.write("\u08d9\5\u0100\u0081\2\u08d5\u08d6\7\7\2\2\u08d6\u08d8")
        buf.write("\5\u0100\u0081\2\u08d7\u08d5\3\2\2\2\u08d8\u08db\3\2\2")
        buf.write("\2\u08d9\u08d7\3\2\2\2\u08d9\u08da\3\2\2\2\u08da\u00af")
        buf.write("\3\2\2\2\u08db\u08d9\3\2\2\2\u08dc\u08dd\5\u0100\u0081")
        buf.write("\2\u08dd\u08de\7\7\2\2\u08de\u08e0\3\2\2\2\u08df\u08dc")
        buf.write("\3\2\2\2\u08df\u08e0\3\2\2\2\u08e0\u08e1\3\2\2\2\u08e1")
        buf.write("\u08e2\5\u0100\u0081\2\u08e2\u00b1\3\2\2\2\u08e3\u08e4")
        buf.write("\5\u0100\u0081\2\u08e4\u08e5\7\7\2\2\u08e5\u08e7\3\2\2")
        buf.write("\2\u08e6\u08e3\3\2\2\2\u08e6\u08e7\3\2\2\2\u08e7\u08e8")
        buf.write("\3\2\2\2\u08e8\u08e9\5\u0100\u0081\2\u08e9\u00b3\3\2\2")
        buf.write("\2\u08ea\u08f2\5\u00be`\2\u08eb\u08ed\7\30\2\2\u08ec\u08eb")
        buf.write("\3\2\2\2\u08ec\u08ed\3\2\2\2\u08ed\u08f0\3\2\2\2\u08ee")
        buf.write("\u08f1\5\u0100\u0081\2\u08ef\u08f1\5\u0096L\2\u08f0\u08ee")
        buf.write("\3\2\2\2\u08f0\u08ef\3\2\2\2\u08f1\u08f3\3\2\2\2\u08f2")
        buf.write("\u08ec\3\2\2\2\u08f2\u08f3\3\2\2\2\u08f3\u00b5\3\2\2\2")
        buf.write("\u08f4\u08f9\5\u00b4[\2\u08f5\u08f6\7\6\2\2\u08f6\u08f8")
        buf.write("\5\u00b4[\2\u08f7\u08f5\3\2\2\2\u08f8\u08fb\3\2\2\2\u08f9")
        buf.write("\u08f7\3\2\2\2\u08f9\u08fa\3\2\2\2\u08fa\u00b7\3\2\2\2")
        buf.write("\u08fb\u08f9\3\2\2\2\u08fc\u08fd\7\4\2\2\u08fd\u0902\5")
        buf.write("\u00ba^\2\u08fe\u08ff\7\6\2\2\u08ff\u0901\5\u00ba^\2\u0900")
        buf.write("\u08fe\3\2\2\2\u0901\u0904\3\2\2\2\u0902\u0900\3\2\2\2")
        buf.write("\u0902\u0903\3\2\2\2\u0903\u0905\3\2\2\2\u0904\u0902\3")
        buf.write("\2\2\2\u0905\u0906\7\5\2\2\u0906\u00b9\3\2\2\2\u0907\u0915")
        buf.write("\5\u00fe\u0080\2\u0908\u0909\5\u0102\u0082\2\u0909\u090a")
        buf.write("\7\4\2\2\u090a\u090f\5\u00bc_\2\u090b\u090c\7\6\2\2\u090c")
        buf.write("\u090e\5\u00bc_\2\u090d\u090b\3\2\2\2\u090e\u0911\3\2")
        buf.write("\2\2\u090f\u090d\3\2\2\2\u090f\u0910\3\2\2\2\u0910\u0912")
        buf.write("\3\2\2\2\u0911\u090f\3\2\2\2\u0912\u0913\7\5\2\2\u0913")
        buf.write("\u0915\3\2\2\2\u0914\u0907\3\2\2\2\u0914\u0908\3\2\2\2")
        buf.write("\u0915\u00bb\3\2\2\2\u0916\u0919\5\u00fe\u0080\2\u0917")
        buf.write("\u0919\5\u00c8e\2\u0918\u0916\3\2\2\2\u0918\u0917\3\2")
        buf.write("\2\2\u0919\u00bd\3\2\2\2\u091a\u091b\5\u00c0a\2\u091b")
        buf.write("\u00bf\3\2\2\2\u091c\u091d\ba\1\2\u091d\u091e\7\u0098")
        buf.write("\2\2\u091e\u0929\5\u00c0a\7\u091f\u0920\7U\2\2\u0920\u0921")
        buf.write("\7\4\2\2\u0921\u0922\5\"\22\2\u0922\u0923\7\5\2\2\u0923")
        buf.write("\u0929\3\2\2\2\u0924\u0926\5\u00c4c\2\u0925\u0927\5\u00c2")
        buf.write("b\2\u0926\u0925\3\2\2\2\u0926\u0927\3\2\2\2\u0927\u0929")
        buf.write("\3\2\2\2\u0928\u091c\3\2\2\2\u0928\u091f\3\2\2\2\u0928")
        buf.write("\u0924\3\2\2\2\u0929\u0932\3\2\2\2\u092a\u092b\f\4\2\2")
        buf.write("\u092b\u092c\7\23\2\2\u092c\u0931\5\u00c0a\5\u092d\u092e")
        buf.write("\f\3\2\2\u092e\u092f\7\u00a0\2\2\u092f\u0931\5\u00c0a")
        buf.write("\4\u0930\u092a\3\2\2\2\u0930\u092d\3\2\2\2\u0931\u0934")
        buf.write("\3\2\2\2\u0932\u0930\3\2\2\2\u0932\u0933\3\2\2\2\u0933")
        buf.write("\u00c1\3\2\2\2\u0934\u0932\3\2\2\2\u0935\u0937\7\u0098")
        buf.write("\2\2\u0936\u0935\3\2\2\2\u0936\u0937\3\2\2\2\u0937\u0938")
        buf.write("\3\2\2\2\u0938\u0939\7\34\2\2\u0939\u093a\5\u00c4c\2\u093a")
        buf.write("\u093b\7\23\2\2\u093b\u093c\5\u00c4c\2\u093c\u0988\3\2")
        buf.write("\2\2\u093d\u093f\7\u0098\2\2\u093e\u093d\3\2\2\2\u093e")
        buf.write("\u093f\3\2\2\2\u093f\u0940\3\2\2\2\u0940\u0941\7r\2\2")
        buf.write("\u0941\u0942\7\4\2\2\u0942\u0947\5\u00be`\2\u0943\u0944")
        buf.write("\7\6\2\2\u0944\u0946\5\u00be`\2\u0945\u0943\3\2\2\2\u0946")
        buf.write("\u0949\3\2\2\2\u0947\u0945\3\2\2\2\u0947\u0948\3\2\2\2")
        buf.write("\u0948\u094a\3\2\2\2\u0949\u0947\3\2\2\2\u094a\u094b\7")
        buf.write("\5\2\2\u094b\u0988\3\2\2\2\u094c\u094e\7\u0098\2\2\u094d")
        buf.write("\u094c\3\2\2\2\u094d\u094e\3\2\2\2\u094e\u094f\3\2\2\2")
        buf.write("\u094f\u0950\7r\2\2\u0950\u0951\7\4\2\2\u0951\u0952\5")
        buf.write("\"\22\2\u0952\u0953\7\5\2\2\u0953\u0988\3\2\2\2\u0954")
        buf.write("\u0956\7\u0098\2\2\u0955\u0954\3\2\2\2\u0955\u0956\3\2")
        buf.write("\2\2\u0956\u0957\3\2\2\2\u0957\u0958\7\u00c4\2\2\u0958")
        buf.write("\u0988\5\u00c4c\2\u0959\u095b\7\u0098\2\2\u095a\u0959")
        buf.write("\3\2\2\2\u095a\u095b\3\2\2\2\u095b\u095c\3\2\2\2\u095c")
        buf.write("\u095d\7\u0085\2\2\u095d\u096b\t\25\2\2\u095e\u095f\7")
        buf.write("\4\2\2\u095f\u096c\7\5\2\2\u0960\u0961\7\4\2\2\u0961\u0966")
        buf.write("\5\u00be`\2\u0962\u0963\7\6\2\2\u0963\u0965\5\u00be`\2")
        buf.write("\u0964\u0962\3\2\2\2\u0965\u0968\3\2\2\2\u0966\u0964\3")
        buf.write("\2\2\2\u0966\u0967\3\2\2\2\u0967\u0969\3\2\2\2\u0968\u0966")
        buf.write("\3\2\2\2\u0969\u096a\7\5\2\2\u096a\u096c\3\2\2\2\u096b")
        buf.write("\u095e\3\2\2\2\u096b\u0960\3\2\2\2\u096c\u0988\3\2\2\2")
        buf.write("\u096d\u096f\7\u0098\2\2\u096e\u096d\3\2\2\2\u096e\u096f")
        buf.write("\3\2\2\2\u096f\u0970\3\2\2\2\u0970\u0971\7\u0085\2\2\u0971")
        buf.write("\u0974\5\u00c4c\2\u0972\u0973\7Q\2\2\u0973\u0975\7\u0119")
        buf.write("\2\2\u0974\u0972\3\2\2\2\u0974\u0975\3\2\2\2\u0975\u0988")
        buf.write("\3\2\2\2\u0976\u0978\7|\2\2\u0977\u0979\7\u0098\2\2\u0978")
        buf.write("\u0977\3\2\2\2\u0978\u0979\3\2\2\2\u0979\u097a\3\2\2\2")
        buf.write("\u097a\u0988\7\u0099\2\2\u097b\u097d\7|\2\2\u097c\u097e")
        buf.write("\7\u0098\2\2\u097d\u097c\3\2\2\2\u097d\u097e\3\2\2\2\u097e")
        buf.write("\u097f\3\2\2\2\u097f\u0988\t\26\2\2\u0980\u0982\7|\2\2")
        buf.write("\u0981\u0983\7\u0098\2\2\u0982\u0981\3\2\2\2\u0982\u0983")
        buf.write("\3\2\2\2\u0983\u0984\3\2\2\2\u0984\u0985\7K\2\2\u0985")
        buf.write("\u0986\7f\2\2\u0986\u0988\5\u00c4c\2\u0987\u0936\3\2\2")
        buf.write("\2\u0987\u093e\3\2\2\2\u0987\u094d\3\2\2\2\u0987\u0955")
        buf.write("\3\2\2\2\u0987\u095a\3\2\2\2\u0987\u096e\3\2\2\2\u0987")
        buf.write("\u0976\3\2\2\2\u0987\u097b\3\2\2\2\u0987\u0980\3\2\2\2")
        buf.write("\u0988\u00c3\3\2\2\2\u0989\u098a\bc\1\2\u098a\u098e\5")
        buf.write("\u00c6d\2\u098b\u098c\t\27\2\2\u098c\u098e\5\u00c4c\t")
        buf.write("\u098d\u0989\3\2\2\2\u098d\u098b\3\2\2\2\u098e\u09a4\3")
        buf.write("\2\2\2\u098f\u0990\f\b\2\2\u0990\u0991\t\30\2\2\u0991")
        buf.write("\u09a3\5\u00c4c\t\u0992\u0993\f\7\2\2\u0993\u0994\t\31")
        buf.write("\2\2\u0994\u09a3\5\u00c4c\b\u0995\u0996\f\6\2\2\u0996")
        buf.write("\u0997\7\u0115\2\2\u0997\u09a3\5\u00c4c\7\u0998\u0999")
        buf.write("\f\5\2\2\u0999\u099a\7\u0118\2\2\u099a\u09a3\5\u00c4c")
        buf.write("\6\u099b\u099c\f\4\2\2\u099c\u099d\7\u0116\2\2\u099d\u09a3")
        buf.write("\5\u00c4c\5\u099e\u099f\f\3\2\2\u099f\u09a0\5\u00caf\2")
        buf.write("\u09a0\u09a1\5\u00c4c\4\u09a1\u09a3\3\2\2\2\u09a2\u098f")
        buf.write("\3\2\2\2\u09a2\u0992\3\2\2\2\u09a2\u0995\3\2\2\2\u09a2")
        buf.write("\u0998\3\2\2\2\u09a2\u099b\3\2\2\2\u09a2\u099e\3\2\2\2")
        buf.write("\u09a3\u09a6\3\2\2\2\u09a4\u09a2\3\2\2\2\u09a4\u09a5\3")
        buf.write("\2\2\2\u09a5\u00c5\3\2\2\2\u09a6\u09a4\3\2\2\2\u09a7\u09a8")
        buf.write("\bd\1\2\u09a8\u0a60\t\32\2\2\u09a9\u09ab\7#\2\2\u09aa")
        buf.write("\u09ac\5\u00eex\2\u09ab\u09aa\3\2\2\2\u09ac\u09ad\3\2")
        buf.write("\2\2\u09ad\u09ab\3\2\2\2\u09ad\u09ae\3\2\2\2\u09ae\u09b1")
        buf.write("\3\2\2\2\u09af\u09b0\7O\2\2\u09b0\u09b2\5\u00be`\2\u09b1")
        buf.write("\u09af\3\2\2\2\u09b1\u09b2\3\2\2\2\u09b2\u09b3\3\2\2\2")
        buf.write("\u09b3\u09b4\7P\2\2\u09b4\u0a60\3\2\2\2\u09b5\u09b6\7")
        buf.write("#\2\2\u09b6\u09b8\5\u00be`\2\u09b7\u09b9\5\u00eex\2\u09b8")
        buf.write("\u09b7\3\2\2\2\u09b9\u09ba\3\2\2\2\u09ba\u09b8\3\2\2\2")
        buf.write("\u09ba\u09bb\3\2\2\2\u09bb\u09be\3\2\2\2\u09bc\u09bd\7")
        buf.write("O\2\2\u09bd\u09bf\5\u00be`\2\u09be\u09bc\3\2\2\2\u09be")
        buf.write("\u09bf\3\2\2\2\u09bf\u09c0\3\2\2\2\u09c0\u09c1\7P\2\2")
        buf.write("\u09c1\u0a60\3\2\2\2\u09c2\u09c3\7$\2\2\u09c3\u09c4\7")
        buf.write("\4\2\2\u09c4\u09c5\5\u00be`\2\u09c5\u09c6\7\30\2\2\u09c6")
        buf.write("\u09c7\5\u00e0q\2\u09c7\u09c8\7\5\2\2\u09c8\u0a60\3\2")
        buf.write("\2\2\u09c9\u09ca\7\u00de\2\2\u09ca\u09d3\7\4\2\2\u09cb")
        buf.write("\u09d0\5\u00b4[\2\u09cc\u09cd\7\6\2\2\u09cd\u09cf\5\u00b4")
        buf.write("[\2\u09ce\u09cc\3\2\2\2\u09cf\u09d2\3\2\2\2\u09d0\u09ce")
        buf.write("\3\2\2\2\u09d0\u09d1\3\2\2\2\u09d1\u09d4\3\2\2\2\u09d2")
        buf.write("\u09d0\3\2\2\2\u09d3\u09cb\3\2\2\2\u09d3\u09d4\3\2\2\2")
        buf.write("\u09d4\u09d5\3\2\2\2\u09d5\u0a60\7\5\2\2\u09d6\u09d7\7")
        buf.write("`\2\2\u09d7\u09d8\7\4\2\2\u09d8\u09db\5\u00be`\2\u09d9")
        buf.write("\u09da\7p\2\2\u09da\u09dc\7\u009a\2\2\u09db\u09d9\3\2")
        buf.write("\2\2\u09db\u09dc\3\2\2\2\u09dc\u09dd\3\2\2\2\u09dd\u09de")
        buf.write("\7\5\2\2\u09de\u0a60\3\2\2\2\u09df\u09e0\7\u0080\2\2\u09e0")
        buf.write("\u09e1\7\4\2\2\u09e1\u09e4\5\u00be`\2\u09e2\u09e3\7p\2")
        buf.write("\2\u09e3\u09e5\7\u009a\2\2\u09e4\u09e2\3\2\2\2\u09e4\u09e5")
        buf.write("\3\2\2\2\u09e5\u09e6\3\2\2\2\u09e6\u09e7\7\5\2\2\u09e7")
        buf.write("\u0a60\3\2\2\2\u09e8\u09e9\7\u00af\2\2\u09e9\u09ea\7\4")
        buf.write("\2\2\u09ea\u09eb\5\u00c4c\2\u09eb\u09ec\7r\2\2\u09ec\u09ed")
        buf.write("\5\u00c4c\2\u09ed\u09ee\7\5\2\2\u09ee\u0a60\3\2\2\2\u09ef")
        buf.write("\u0a60\5\u00c8e\2\u09f0\u0a60\7\u0111\2\2\u09f1\u09f2")
        buf.write("\5\u00fe\u0080\2\u09f2\u09f3\7\7\2\2\u09f3\u09f4\7\u0111")
        buf.write("\2\2\u09f4\u0a60\3\2\2\2\u09f5\u09f6\7\4\2\2\u09f6\u09f9")
        buf.write("\5\u00b4[\2\u09f7\u09f8\7\6\2\2\u09f8\u09fa\5\u00b4[\2")
        buf.write("\u09f9\u09f7\3\2\2\2\u09fa\u09fb\3\2\2\2\u09fb\u09f9\3")
        buf.write("\2\2\2\u09fb\u09fc\3\2\2\2\u09fc\u09fd\3\2\2\2\u09fd\u09fe")
        buf.write("\7\5\2\2\u09fe\u0a60\3\2\2\2\u09ff\u0a00\7\4\2\2\u0a00")
        buf.write("\u0a01\5\"\22\2\u0a01\u0a02\7\5\2\2\u0a02\u0a60\3\2\2")
        buf.write("\2\u0a03\u0a04\5\u00fc\177\2\u0a04\u0a10\7\4\2\2\u0a05")
        buf.write("\u0a07\5\u0088E\2\u0a06\u0a05\3\2\2\2\u0a06\u0a07\3\2")
        buf.write("\2\2\u0a07\u0a08\3\2\2\2\u0a08\u0a0d\5\u00be`\2\u0a09")
        buf.write("\u0a0a\7\6\2\2\u0a0a\u0a0c\5\u00be`\2\u0a0b\u0a09\3\2")
        buf.write("\2\2\u0a0c\u0a0f\3\2\2\2\u0a0d\u0a0b\3\2\2\2\u0a0d\u0a0e")
        buf.write("\3\2\2\2\u0a0e\u0a11\3\2\2\2\u0a0f\u0a0d\3\2\2\2\u0a10")
        buf.write("\u0a06\3\2\2\2\u0a10\u0a11\3\2\2\2\u0a11\u0a12\3\2\2\2")
        buf.write("\u0a12\u0a19\7\5\2\2\u0a13\u0a14\7^\2\2\u0a14\u0a15\7")
        buf.write("\4\2\2\u0a15\u0a16\7\u0103\2\2\u0a16\u0a17\5\u00c0a\2")
        buf.write("\u0a17\u0a18\7\5\2\2\u0a18\u0a1a\3\2\2\2\u0a19\u0a13\3")
        buf.write("\2\2\2\u0a19\u0a1a\3\2\2\2\u0a1a\u0a1d\3\2\2\2\u0a1b\u0a1c")
        buf.write("\7\u00a5\2\2\u0a1c\u0a1e\5\u00f4{\2\u0a1d\u0a1b\3\2\2")
        buf.write("\2\u0a1d\u0a1e\3\2\2\2\u0a1e\u0a60\3\2\2\2\u0a1f\u0a20")
        buf.write("\5\u0102\u0082\2\u0a20\u0a21\7\n\2\2\u0a21\u0a22\5\u00be")
        buf.write("`\2\u0a22\u0a60\3\2\2\2\u0a23\u0a24\7\4\2\2\u0a24\u0a27")
        buf.write("\5\u0102\u0082\2\u0a25\u0a26\7\6\2\2\u0a26\u0a28\5\u0102")
        buf.write("\u0082\2\u0a27\u0a25\3\2\2\2\u0a28\u0a29\3\2\2\2\u0a29")
        buf.write("\u0a27\3\2\2\2\u0a29\u0a2a\3\2\2\2\u0a2a\u0a2b\3\2\2\2")
        buf.write("\u0a2b\u0a2c\7\5\2\2\u0a2c\u0a2d\7\n\2\2\u0a2d\u0a2e\5")
        buf.write("\u00be`\2\u0a2e\u0a60\3\2\2\2\u0a2f\u0a60\5\u0102\u0082")
        buf.write("\2\u0a30\u0a31\7\4\2\2\u0a31\u0a32\5\u00be`\2\u0a32\u0a33")
        buf.write("\7\5\2\2\u0a33\u0a60\3\2\2\2\u0a34\u0a35\7Z\2\2\u0a35")
        buf.write("\u0a36\7\4\2\2\u0a36\u0a37\5\u0102\u0082\2\u0a37\u0a38")
        buf.write("\7f\2\2\u0a38\u0a39\5\u00c4c\2\u0a39\u0a3a\7\5\2\2\u0a3a")
        buf.write("\u0a60\3\2\2\2\u0a3b\u0a3c\t\33\2\2\u0a3c\u0a3d\7\4\2")
        buf.write("\2\u0a3d\u0a3e\5\u00c4c\2\u0a3e\u0a3f\t\34\2\2\u0a3f\u0a42")
        buf.write("\5\u00c4c\2\u0a40\u0a41\t\35\2\2\u0a41\u0a43\5\u00c4c")
        buf.write("\2\u0a42\u0a40\3\2\2\2\u0a42\u0a43\3\2\2\2\u0a43\u0a44")
        buf.write("\3\2\2\2\u0a44\u0a45\7\5\2\2\u0a45\u0a60\3\2\2\2\u0a46")
        buf.write("\u0a47\7\u00ef\2\2\u0a47\u0a49\7\4\2\2\u0a48\u0a4a\t\36")
        buf.write("\2\2\u0a49\u0a48\3\2\2\2\u0a49\u0a4a\3\2\2\2\u0a4a\u0a4c")
        buf.write("\3\2\2\2\u0a4b\u0a4d\5\u00c4c\2\u0a4c\u0a4b\3\2\2\2\u0a4c")
        buf.write("\u0a4d\3\2\2\2\u0a4d\u0a4e\3\2\2\2\u0a4e\u0a4f\7f\2\2")
        buf.write("\u0a4f\u0a50\5\u00c4c\2\u0a50\u0a51\7\5\2\2\u0a51\u0a60")
        buf.write("\3\2\2\2\u0a52\u0a53\7\u00a7\2\2\u0a53\u0a54\7\4\2\2\u0a54")
        buf.write("\u0a55\5\u00c4c\2\u0a55\u0a56\7\u00ae\2\2\u0a56\u0a57")
        buf.write("\5\u00c4c\2\u0a57\u0a58\7f\2\2\u0a58\u0a5b\5\u00c4c\2")
        buf.write("\u0a59\u0a5a\7b\2\2\u0a5a\u0a5c\5\u00c4c\2\u0a5b\u0a59")
        buf.write("\3\2\2\2\u0a5b\u0a5c\3\2\2\2\u0a5c\u0a5d\3\2\2\2\u0a5d")
        buf.write("\u0a5e\7\5\2\2\u0a5e\u0a60\3\2\2\2\u0a5f\u09a7\3\2\2\2")
        buf.write("\u0a5f\u09a9\3\2\2\2\u0a5f\u09b5\3\2\2\2\u0a5f\u09c2\3")
        buf.write("\2\2\2\u0a5f\u09c9\3\2\2\2\u0a5f\u09d6\3\2\2\2\u0a5f\u09df")
        buf.write("\3\2\2\2\u0a5f\u09e8\3\2\2\2\u0a5f\u09ef\3\2\2\2\u0a5f")
        buf.write("\u09f0\3\2\2\2\u0a5f\u09f1\3\2\2\2\u0a5f\u09f5\3\2\2\2")
        buf.write("\u0a5f\u09ff\3\2\2\2\u0a5f\u0a03\3\2\2\2\u0a5f\u0a1f\3")
        buf.write("\2\2\2\u0a5f\u0a23\3\2\2\2\u0a5f\u0a2f\3\2\2\2\u0a5f\u0a30")
        buf.write("\3\2\2\2\u0a5f\u0a34\3\2\2\2\u0a5f\u0a3b\3\2\2\2\u0a5f")
        buf.write("\u0a46\3\2\2\2\u0a5f\u0a52\3\2\2\2\u0a60\u0a6b\3\2\2\2")
        buf.write("\u0a61\u0a62\f\n\2\2\u0a62\u0a63\7\13\2\2\u0a63\u0a64")
        buf.write("\5\u00c4c\2\u0a64\u0a65\7\f\2\2\u0a65\u0a6a\3\2\2\2\u0a66")
        buf.write("\u0a67\f\b\2\2\u0a67\u0a68\7\7\2\2\u0a68\u0a6a\5\u0102")
        buf.write("\u0082\2\u0a69\u0a61\3\2\2\2\u0a69\u0a66\3\2\2\2\u0a6a")
        buf.write("\u0a6d\3\2\2\2\u0a6b\u0a69\3\2\2\2\u0a6b\u0a6c\3\2\2\2")
        buf.write("\u0a6c\u00c7\3\2\2\2\u0a6d\u0a6b\3\2\2\2\u0a6e\u0a7b\7")
        buf.write("\u0099\2\2\u0a6f\u0a7b\5\u00d2j\2\u0a70\u0a71\5\u0102")
        buf.write("\u0082\2\u0a71\u0a72\7\u0119\2\2\u0a72\u0a7b\3\2\2\2\u0a73")
        buf.write("\u0a7b\5\u0108\u0085\2\u0a74\u0a7b\5\u00d0i\2\u0a75\u0a77")
        buf.write("\7\u0119\2\2\u0a76\u0a75\3\2\2\2\u0a77\u0a78\3\2\2\2\u0a78")
        buf.write("\u0a76\3\2\2\2\u0a78\u0a79\3\2\2\2\u0a79\u0a7b\3\2\2\2")
        buf.write("\u0a7a\u0a6e\3\2\2\2\u0a7a\u0a6f\3\2\2\2\u0a7a\u0a70\3")
        buf.write("\2\2\2\u0a7a\u0a73\3\2\2\2\u0a7a\u0a74\3\2\2\2\u0a7a\u0a76")
        buf.write("\3\2\2\2\u0a7b\u00c9\3\2\2\2\u0a7c\u0a7d\t\37\2\2\u0a7d")
        buf.write("\u00cb\3\2\2\2\u0a7e\u0a7f\t \2\2\u0a7f\u00cd\3\2\2\2")
        buf.write("\u0a80\u0a81\t!\2\2\u0a81\u00cf\3\2\2\2\u0a82\u0a83\t")
        buf.write("\"\2\2\u0a83\u00d1\3\2\2\2\u0a84\u0a87\7z\2\2\u0a85\u0a88")
        buf.write("\5\u00d4k\2\u0a86\u0a88\5\u00d8m\2\u0a87\u0a85\3\2\2\2")
        buf.write("\u0a87\u0a86\3\2\2\2\u0a87\u0a88\3\2\2\2\u0a88\u00d3\3")
        buf.write("\2\2\2\u0a89\u0a8b\5\u00d6l\2\u0a8a\u0a8c\5\u00dan\2\u0a8b")
        buf.write("\u0a8a\3\2\2\2\u0a8b\u0a8c\3\2\2\2\u0a8c\u00d5\3\2\2\2")
        buf.write("\u0a8d\u0a8e\5\u00dco\2\u0a8e\u0a8f\5\u0102\u0082\2\u0a8f")
        buf.write("\u0a91\3\2\2\2\u0a90\u0a8d\3\2\2\2\u0a91\u0a92\3\2\2\2")
        buf.write("\u0a92\u0a90\3\2\2\2\u0a92\u0a93\3\2\2\2\u0a93\u00d7\3")
        buf.write("\2\2\2\u0a94\u0a97\5\u00dan\2\u0a95\u0a98\5\u00d6l\2\u0a96")
        buf.write("\u0a98\5\u00dan\2\u0a97\u0a95\3\2\2\2\u0a97\u0a96\3\2")
        buf.write("\2\2\u0a97\u0a98\3\2\2\2\u0a98\u00d9\3\2\2\2\u0a99\u0a9a")
        buf.write("\5\u00dco\2\u0a9a\u0a9b\5\u0102\u0082\2\u0a9b\u0a9c\7")
        buf.write("\u00e9\2\2\u0a9c\u0a9d\5\u0102\u0082\2\u0a9d\u00db\3\2")
        buf.write("\2\2\u0a9e\u0aa0\t#\2\2\u0a9f\u0a9e\3\2\2\2\u0a9f\u0aa0")
        buf.write("\3\2\2\2\u0aa0\u0aa1\3\2\2\2\u0aa1\u0aa4\t\24\2\2\u0aa2")
        buf.write("\u0aa4\7\u0119\2\2\u0aa3\u0a9f\3\2\2\2\u0aa3\u0aa2\3\2")
        buf.write("\2\2\u0aa4\u00dd\3\2\2\2\u0aa5\u0aa9\7`\2\2\u0aa6\u0aa7")
        buf.write("\7\17\2\2\u0aa7\u0aa9\5\u0100\u0081\2\u0aa8\u0aa5\3\2")
        buf.write("\2\2\u0aa8\u0aa6\3\2\2\2\u0aa9\u00df\3\2\2\2\u0aaa\u0aab")
        buf.write("\7\27\2\2\u0aab\u0aac\7\u010b\2\2\u0aac\u0aad\5\u00e0")
        buf.write("q\2\u0aad\u0aae\7\u010d\2\2\u0aae\u0acd\3\2\2\2\u0aaf")
        buf.write("\u0ab0\7\u0090\2\2\u0ab0\u0ab1\7\u010b\2\2\u0ab1\u0ab2")
        buf.write("\5\u00e0q\2\u0ab2\u0ab3\7\6\2\2\u0ab3\u0ab4\5\u00e0q\2")
        buf.write("\u0ab4\u0ab5\7\u010d\2\2\u0ab5\u0acd\3\2\2\2\u0ab6\u0abd")
        buf.write("\7\u00de\2\2\u0ab7\u0ab9\7\u010b\2\2\u0ab8\u0aba\5\u00ea")
        buf.write("v\2\u0ab9\u0ab8\3\2\2\2\u0ab9\u0aba\3\2\2\2\u0aba\u0abb")
        buf.write("\3\2\2\2\u0abb\u0abe\7\u010d\2\2\u0abc\u0abe\7\u0109\2")
        buf.write("\2\u0abd\u0ab7\3\2\2\2\u0abd\u0abc\3\2\2\2\u0abe\u0acd")
        buf.write("\3\2\2\2\u0abf\u0aca\5\u0102\u0082\2\u0ac0\u0ac1\7\4\2")
        buf.write("\2\u0ac1\u0ac6\7\u011d\2\2\u0ac2\u0ac3\7\6\2\2\u0ac3\u0ac5")
        buf.write("\7\u011d\2\2\u0ac4\u0ac2\3\2\2\2\u0ac5\u0ac8\3\2\2\2\u0ac6")
        buf.write("\u0ac4\3\2\2\2\u0ac6\u0ac7\3\2\2\2\u0ac7\u0ac9\3\2\2\2")
        buf.write("\u0ac8\u0ac6\3\2\2\2\u0ac9\u0acb\7\5\2\2\u0aca\u0ac0\3")
        buf.write("\2\2\2\u0aca\u0acb\3\2\2\2\u0acb\u0acd\3\2\2\2\u0acc\u0aaa")
        buf.write("\3\2\2\2\u0acc\u0aaf\3\2\2\2\u0acc\u0ab6\3\2\2\2\u0acc")
        buf.write("\u0abf\3\2\2\2\u0acd\u00e1\3\2\2\2\u0ace\u0ad3\5\u00e4")
        buf.write("s\2\u0acf\u0ad0\7\6\2\2\u0ad0\u0ad2\5\u00e4s\2\u0ad1\u0acf")
        buf.write("\3\2\2\2\u0ad2\u0ad5\3\2\2\2\u0ad3\u0ad1\3\2\2\2\u0ad3")
        buf.write("\u0ad4\3\2\2\2\u0ad4\u00e3\3\2\2\2\u0ad5\u0ad3\3\2\2\2")
        buf.write("\u0ad6\u0ad7\5\u00aeX\2\u0ad7\u0ada\5\u00e0q\2\u0ad8\u0ad9")
        buf.write("\7\u0098\2\2\u0ad9\u0adb\7\u0099\2\2\u0ada\u0ad8\3\2\2")
        buf.write("\2\u0ada\u0adb\3\2\2\2\u0adb\u0add\3\2\2\2\u0adc\u0ade")
        buf.write("\5 \21\2\u0add\u0adc\3\2\2\2\u0add\u0ade\3\2\2\2\u0ade")
        buf.write("\u0ae0\3\2\2\2\u0adf\u0ae1\5\u00dep\2\u0ae0\u0adf\3\2")
        buf.write("\2\2\u0ae0\u0ae1\3\2\2\2\u0ae1\u00e5\3\2\2\2\u0ae2\u0ae7")
        buf.write("\5\u00e8u\2\u0ae3\u0ae4\7\6\2\2\u0ae4\u0ae6\5\u00e8u\2")
        buf.write("\u0ae5\u0ae3\3\2\2\2\u0ae6\u0ae9\3\2\2\2\u0ae7\u0ae5\3")
        buf.write("\2\2\2\u0ae7\u0ae8\3\2\2\2\u0ae8\u00e7\3\2\2\2\u0ae9\u0ae7")
        buf.write("\3\2\2\2\u0aea\u0aeb\5\u0100\u0081\2\u0aeb\u0aee\5\u00e0")
        buf.write("q\2\u0aec\u0aed\7\u0098\2\2\u0aed\u0aef\7\u0099\2\2\u0aee")
        buf.write("\u0aec\3\2\2\2\u0aee\u0aef\3\2\2\2\u0aef\u0af1\3\2\2\2")
        buf.write("\u0af0\u0af2\5 \21\2\u0af1\u0af0\3\2\2\2\u0af1\u0af2\3")
        buf.write("\2\2\2\u0af2\u00e9\3\2\2\2\u0af3\u0af8\5\u00ecw\2\u0af4")
        buf.write("\u0af5\7\6\2\2\u0af5\u0af7\5\u00ecw\2\u0af6\u0af4\3\2")
        buf.write("\2\2\u0af7\u0afa\3\2\2\2\u0af8\u0af6\3\2\2\2\u0af8\u0af9")
        buf.write("\3\2\2\2\u0af9\u00eb\3\2\2\2\u0afa\u0af8\3\2\2\2\u0afb")
        buf.write("\u0afc\5\u0102\u0082\2\u0afc\u0afd\7\r\2\2\u0afd\u0b00")
        buf.write("\5\u00e0q\2\u0afe\u0aff\7\u0098\2\2\u0aff\u0b01\7\u0099")
        buf.write("\2\2\u0b00\u0afe\3\2\2\2\u0b00\u0b01\3\2\2\2\u0b01\u0b03")
        buf.write("\3\2\2\2\u0b02\u0b04\5 \21\2\u0b03\u0b02\3\2\2\2\u0b03")
        buf.write("\u0b04\3\2\2\2\u0b04\u00ed\3\2\2\2\u0b05\u0b06\7\u0102")
        buf.write("\2\2\u0b06\u0b07\5\u00be`\2\u0b07\u0b08\7\u00e7\2\2\u0b08")
        buf.write("\u0b09\5\u00be`\2\u0b09\u00ef\3\2\2\2\u0b0a\u0b0b\7\u0104")
        buf.write("\2\2\u0b0b\u0b10\5\u00f2z\2\u0b0c\u0b0d\7\6\2\2\u0b0d")
        buf.write("\u0b0f\5\u00f2z\2\u0b0e\u0b0c\3\2\2\2\u0b0f\u0b12\3\2")
        buf.write("\2\2\u0b10\u0b0e\3\2\2\2\u0b10\u0b11\3\2\2\2\u0b11\u00f1")
        buf.write("\3\2\2\2\u0b12\u0b10\3\2\2\2\u0b13\u0b14\5\u0100\u0081")
        buf.write("\2\u0b14\u0b15\7\30\2\2\u0b15\u0b16\5\u00f4{\2\u0b16\u00f3")
        buf.write("\3\2\2\2\u0b17\u0b46\5\u0100\u0081\2\u0b18\u0b19\7\4\2")
        buf.write("\2\u0b19\u0b1a\5\u0100\u0081\2\u0b1a\u0b1b\7\5\2\2\u0b1b")
        buf.write("\u0b46\3\2\2\2\u0b1c\u0b3f\7\4\2\2\u0b1d\u0b1e\7(\2\2")
        buf.write("\u0b1e\u0b1f\7 \2\2\u0b1f\u0b24\5\u00be`\2\u0b20\u0b21")
        buf.write("\7\6\2\2\u0b21\u0b23\5\u00be`\2\u0b22\u0b20\3\2\2\2\u0b23")
        buf.write("\u0b26\3\2\2\2\u0b24\u0b22\3\2\2\2\u0b24\u0b25\3\2\2\2")
        buf.write("\u0b25\u0b40\3\2\2\2\u0b26\u0b24\3\2\2\2\u0b27\u0b28\t")
        buf.write("$\2\2\u0b28\u0b29\7 \2\2\u0b29\u0b2e\5\u00be`\2\u0b2a")
        buf.write("\u0b2b\7\6\2\2\u0b2b\u0b2d\5\u00be`\2\u0b2c\u0b2a\3\2")
        buf.write("\2\2\u0b2d\u0b30\3\2\2\2\u0b2e\u0b2c\3\2\2\2\u0b2e\u0b2f")
        buf.write("\3\2\2\2\u0b2f\u0b32\3\2\2\2\u0b30\u0b2e\3\2\2\2\u0b31")
        buf.write("\u0b27\3\2\2\2\u0b31\u0b32\3\2\2\2\u0b32\u0b3d\3\2\2\2")
        buf.write("\u0b33\u0b34\t%\2\2\u0b34\u0b35\7 \2\2\u0b35\u0b3a\5X")
        buf.write("-\2\u0b36\u0b37\7\6\2\2\u0b37\u0b39\5X-\2\u0b38\u0b36")
        buf.write("\3\2\2\2\u0b39\u0b3c\3\2\2\2\u0b3a\u0b38\3\2\2\2\u0b3a")
        buf.write("\u0b3b\3\2\2\2\u0b3b\u0b3e\3\2\2\2\u0b3c\u0b3a\3\2\2\2")
        buf.write("\u0b3d\u0b33\3\2\2\2\u0b3d\u0b3e\3\2\2\2\u0b3e\u0b40\3")
        buf.write("\2\2\2\u0b3f\u0b1d\3\2\2\2\u0b3f\u0b31\3\2\2\2\u0b40\u0b42")
        buf.write("\3\2\2\2\u0b41\u0b43\5\u00f6|\2\u0b42\u0b41\3\2\2\2\u0b42")
        buf.write("\u0b43\3\2\2\2\u0b43\u0b44\3\2\2\2\u0b44\u0b46\7\5\2\2")
        buf.write("\u0b45\u0b17\3\2\2\2\u0b45\u0b18\3\2\2\2\u0b45\u0b1c\3")
        buf.write("\2\2\2\u0b46\u00f5\3\2\2\2\u0b47\u0b48\7\u00b6\2\2\u0b48")
        buf.write("\u0b58\5\u00f8}\2\u0b49\u0b4a\7\u00ca\2\2\u0b4a\u0b58")
        buf.write("\5\u00f8}\2\u0b4b\u0b4c\7\u00b6\2\2\u0b4c\u0b4d\7\34\2")
        buf.write("\2\u0b4d\u0b4e\5\u00f8}\2\u0b4e\u0b4f\7\23\2\2\u0b4f\u0b50")
        buf.write("\5\u00f8}\2\u0b50\u0b58\3\2\2\2\u0b51\u0b52\7\u00ca\2")
        buf.write("\2\u0b52\u0b53\7\34\2\2\u0b53\u0b54\5\u00f8}\2\u0b54\u0b55")
        buf.write("\7\23\2\2\u0b55\u0b56\5\u00f8}\2\u0b56\u0b58\3\2\2\2\u0b57")
        buf.write("\u0b47\3\2\2\2\u0b57\u0b49\3\2\2\2\u0b57\u0b4b\3\2\2\2")
        buf.write("\u0b57\u0b51\3\2\2\2\u0b58\u00f7\3\2\2\2\u0b59\u0b5a\7")
        buf.write("\u00f4\2\2\u0b5a\u0b61\t&\2\2\u0b5b\u0b5c\7:\2\2\u0b5c")
        buf.write("\u0b61\7\u00c9\2\2\u0b5d\u0b5e\5\u00be`\2\u0b5e\u0b5f")
        buf.write("\t&\2\2\u0b5f\u0b61\3\2\2\2\u0b60\u0b59\3\2\2\2\u0b60")
        buf.write("\u0b5b\3\2\2\2\u0b60\u0b5d\3\2\2\2\u0b61\u00f9\3\2\2\2")
        buf.write("\u0b62\u0b67\5\u00fe\u0080\2\u0b63\u0b64\7\6\2\2\u0b64")
        buf.write("\u0b66\5\u00fe\u0080\2\u0b65\u0b63\3\2\2\2\u0b66\u0b69")
        buf.write("\3\2\2\2\u0b67\u0b65\3\2\2\2\u0b67\u0b68\3\2\2\2\u0b68")
        buf.write("\u00fb\3\2\2\2\u0b69\u0b67\3\2\2\2\u0b6a\u0b6f\5\u00fe")
        buf.write("\u0080\2\u0b6b\u0b6f\7^\2\2\u0b6c\u0b6f\7\u0084\2\2\u0b6d")
        buf.write("\u0b6f\7\u00c3\2\2\u0b6e\u0b6a\3\2\2\2\u0b6e\u0b6b\3\2")
        buf.write("\2\2\u0b6e\u0b6c\3\2\2\2\u0b6e\u0b6d\3\2\2\2\u0b6f\u00fd")
        buf.write("\3\2\2\2\u0b70\u0b75\5\u0102\u0082\2\u0b71\u0b72\7\7\2")
        buf.write("\2\u0b72\u0b74\5\u0102\u0082\2\u0b73\u0b71\3\2\2\2\u0b74")
        buf.write("\u0b77\3\2\2\2\u0b75\u0b73\3\2\2\2\u0b75\u0b76\3\2\2\2")
        buf.write("\u0b76\u00ff\3\2\2\2\u0b77\u0b75\3\2\2\2\u0b78\u0b79\5")
        buf.write("\u0102\u0082\2\u0b79\u0101\3\2\2\2\u0b7a\u0b7d\5\u0104")
        buf.write("\u0083\2\u0b7b\u0b7d\5\u010c\u0087\2\u0b7c\u0b7a\3\2\2")
        buf.write("\2\u0b7c\u0b7b\3\2\2\2\u0b7d\u0103\3\2\2\2\u0b7e\u0b82")
        buf.write("\7\u0123\2\2\u0b7f\u0b82\5\u0106\u0084\2\u0b80\u0b82\5")
        buf.write("\u010e\u0088\2\u0b81\u0b7e\3\2\2\2\u0b81\u0b7f\3\2\2\2")
        buf.write("\u0b81\u0b80\3\2\2\2\u0b82\u0105\3\2\2\2\u0b83\u0b84\7")
        buf.write("\u0124\2\2\u0b84\u0107\3\2\2\2\u0b85\u0b87\7\u0110\2\2")
        buf.write("\u0b86\u0b85\3\2\2\2\u0b86\u0b87\3\2\2\2\u0b87\u0b88\3")
        buf.write("\2\2\2\u0b88\u0baa\7\u011e\2\2\u0b89\u0b8b\7\u0110\2\2")
        buf.write("\u0b8a\u0b89\3\2\2\2\u0b8a\u0b8b\3\2\2\2\u0b8b\u0b8c\3")
        buf.write("\2\2\2\u0b8c\u0baa\7\u011f\2\2\u0b8d\u0b8f\7\u0110\2\2")
        buf.write("\u0b8e\u0b8d\3\2\2\2\u0b8e\u0b8f\3\2\2\2\u0b8f\u0b90\3")
        buf.write("\2\2\2\u0b90\u0baa\7\u011d\2\2\u0b91\u0b93\7\u0110\2\2")
        buf.write("\u0b92\u0b91\3\2\2\2\u0b92\u0b93\3\2\2\2\u0b93\u0b94\3")
        buf.write("\2\2\2\u0b94\u0baa\7\u011a\2\2\u0b95\u0b97\7\u0110\2\2")
        buf.write("\u0b96\u0b95\3\2\2\2\u0b96\u0b97\3\2\2\2\u0b97\u0b98\3")
        buf.write("\2\2\2\u0b98\u0baa\7\u011b\2\2\u0b99\u0b9b\7\u0110\2\2")
        buf.write("\u0b9a\u0b99\3\2\2\2\u0b9a\u0b9b\3\2\2\2\u0b9b\u0b9c\3")
        buf.write("\2\2\2\u0b9c\u0baa\7\u011c\2\2\u0b9d\u0b9f\7\u0110\2\2")
        buf.write("\u0b9e\u0b9d\3\2\2\2\u0b9e\u0b9f\3\2\2\2\u0b9f\u0ba0\3")
        buf.write("\2\2\2\u0ba0\u0baa\7\u0121\2\2\u0ba1\u0ba3\7\u0110\2\2")
        buf.write("\u0ba2\u0ba1\3\2\2\2\u0ba2\u0ba3\3\2\2\2\u0ba3\u0ba4\3")
        buf.write("\2\2\2\u0ba4\u0baa\7\u0120\2\2\u0ba5\u0ba7\7\u0110\2\2")
        buf.write("\u0ba6\u0ba5\3\2\2\2\u0ba6\u0ba7\3\2\2\2\u0ba7\u0ba8\3")
        buf.write("\2\2\2\u0ba8\u0baa\7\u0122\2\2\u0ba9\u0b86\3\2\2\2\u0ba9")
        buf.write("\u0b8a\3\2\2\2\u0ba9\u0b8e\3\2\2\2\u0ba9\u0b92\3\2\2\2")
        buf.write("\u0ba9\u0b96\3\2\2\2\u0ba9\u0b9a\3\2\2\2\u0ba9\u0b9e\3")
        buf.write("\2\2\2\u0ba9\u0ba2\3\2\2\2\u0ba9\u0ba6\3\2\2\2\u0baa\u0109")
        buf.write("\3\2\2\2\u0bab\u0bac\7\u00f2\2\2\u0bac\u0bb3\5\u00e0q")
        buf.write("\2\u0bad\u0bb3\5 \21\2\u0bae\u0bb3\5\u00dep\2\u0baf\u0bb0")
        buf.write("\t\'\2\2\u0bb0\u0bb1\7\u0098\2\2\u0bb1\u0bb3\7\u0099\2")
        buf.write("\2\u0bb2\u0bab\3\2\2\2\u0bb2\u0bad\3\2\2\2\u0bb2\u0bae")
        buf.write("\3\2\2\2\u0bb2\u0baf\3\2\2\2\u0bb3\u010b\3\2\2\2\u0bb4")
        buf.write("\u0bb5\t(\2\2\u0bb5\u010d\3\2\2\2\u0bb6\u0bb7\t)\2\2\u0bb7")
        buf.write("\u010f\3\2\2\2\u0188\u0114\u012d\u0132\u013a\u0142\u0144")
        buf.write("\u0158\u015c\u0162\u0165\u0168\u016f\u0174\u0177\u017e")
        buf.write("\u018a\u0193\u0195\u0199\u019c\u01a3\u01ae\u01b0\u01b8")
        buf.write("\u01bd\u01c0\u01c6\u01d1\u0211\u021a\u021e\u0224\u0228")
        buf.write("\u022d\u0233\u023f\u0247\u024d\u025a\u025f\u026f\u0276")
        buf.write("\u027a\u0280\u028f\u0293\u0299\u029f\u02a2\u02a5\u02ab")
        buf.write("\u02af\u02b7\u02b9\u02c2\u02c5\u02ce\u02d3\u02d9\u02e0")
        buf.write("\u02e3\u02e9\u02f4\u02f7\u02fb\u0300\u0305\u030c\u030f")
        buf.write("\u0312\u0319\u031e\u0327\u032f\u0335\u0338\u033b\u0341")
        buf.write("\u0345\u0349\u034d\u034f\u0357\u035f\u0365\u036b\u036e")
        buf.write("\u0372\u0375\u0379\u0395\u0398\u039c\u03a2\u03a5\u03a8")
        buf.write("\u03ae\u03b6\u03bb\u03c1\u03c7\u03d3\u03d6\u03dd\u03ee")
        buf.write("\u03f7\u03fa\u0400\u0409\u0410\u0413\u041d\u0421\u0428")
        buf.write("\u049c\u04a4\u04ac\u04b5\u04bf\u04c3\u04c6\u04cc\u04d2")
        buf.write("\u04de\u04ea\u04ef\u04f8\u0500\u0507\u0509\u050e\u0512")
        buf.write("\u0517\u051c\u0521\u0524\u0529\u052d\u0532\u0534\u0538")
        buf.write("\u0541\u0549\u0552\u0559\u0562\u0567\u056a\u057d\u057f")
        buf.write("\u0588\u058f\u0592\u0599\u059d\u05a3\u05ab\u05b6\u05c1")
        buf.write("\u05c8\u05ce\u05db\u05e2\u05e9\u05f5\u05fd\u0603\u0606")
        buf.write("\u060f\u0612\u061b\u061e\u0627\u062a\u0633\u0636\u0639")
        buf.write("\u063e\u0640\u064b\u0651\u0654\u0656\u0662\u0666\u066a")
        buf.write("\u0670\u0674\u067c\u0680\u0683\u0686\u0689\u068d\u0691")
        buf.write("\u0694\u0698\u069d\u06a1\u06a4\u06a7\u06aa\u06ac\u06b8")
        buf.write("\u06bb\u06bf\u06c9\u06cd\u06cf\u06d2\u06d6\u06dc\u06e0")
        buf.write("\u06eb\u06f5\u0701\u0710\u0715\u071c\u072c\u0731\u073e")
        buf.write("\u0743\u074b\u0751\u0755\u075e\u076d\u0772\u077e\u0783")
        buf.write("\u078b\u078e\u0792\u07a0\u07ad\u07b2\u07b6\u07b9\u07be")
        buf.write("\u07c7\u07ca\u07cf\u07d6\u07d9\u07e1\u07e8\u07ef\u07f2")
        buf.write("\u07f7\u07fa\u07ff\u0803\u0806\u0809\u080f\u0814\u0819")
        buf.write("\u082b\u082d\u0830\u083b\u0844\u084b\u0853\u085a\u085e")
        buf.write("\u0866\u086e\u0874\u087c\u0888\u088b\u0891\u0895\u0897")
        buf.write("\u08a0\u08ac\u08ae\u08b5\u08bc\u08c2\u08c8\u08ca\u08d1")
        buf.write("\u08d9\u08df\u08e6\u08ec\u08f0\u08f2\u08f9\u0902\u090f")
        buf.write("\u0914\u0918\u0926\u0928\u0930\u0932\u0936\u093e\u0947")
        buf.write("\u094d\u0955\u095a\u0966\u096b\u096e\u0974\u0978\u097d")
        buf.write("\u0982\u0987\u098d\u09a2\u09a4\u09ad\u09b1\u09ba\u09be")
        buf.write("\u09d0\u09d3\u09db\u09e4\u09fb\u0a06\u0a0d\u0a10\u0a19")
        buf.write("\u0a1d\u0a29\u0a42\u0a49\u0a4c\u0a5b\u0a5f\u0a69\u0a6b")
        buf.write("\u0a78\u0a7a\u0a87\u0a8b\u0a92\u0a97\u0a9f\u0aa3\u0aa8")
        buf.write("\u0ab9\u0abd\u0ac6\u0aca\u0acc\u0ad3\u0ada\u0add\u0ae0")
        buf.write("\u0ae7\u0aee\u0af1\u0af8\u0b00\u0b03\u0b10\u0b24\u0b2e")
        buf.write("\u0b31\u0b3a\u0b3d\u0b3f\u0b42\u0b45\u0b57\u0b60\u0b67")
        buf.write("\u0b6e\u0b75\u0b7c\u0b81\u0b86\u0b8a\u0b8e\u0b92\u0b96")
        buf.write("\u0b9a\u0b9e\u0ba2\u0ba6\u0ba9\u0bb2")
        return buf.getvalue()


class SparkSQLParser ( Parser ):

    grammarFileName = "SparkSQL.g4"

    atn = ATNDeserializer().deserialize(serializedATN())

    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]

    sharedContextCache = PredictionContextCache()

    literalNames = [ "<INVALID>", "';'", "'('", "')'", "','", "'.'", "'/*+'", 
                     "'*/'", "'->'", "'['", "']'", "':'", "'ADD'", "'AFTER'", 
                     "'ALL'", "'ALTER'", "'ANALYZE'", "'AND'", "'ANTI'", 
                     "'ANY'", "'ARCHIVE'", "'ARRAY'", "'AS'", "'ASC'", "'AT'", 
                     "'AUTHORIZATION'", "'BETWEEN'", "'BOTH'", "'BUCKET'", 
                     "'BUCKETS'", "'BY'", "'CACHE'", "'CASCADE'", "'CASE'", 
                     "'CAST'", "'CHANGE'", "'CHECK'", "'CLEAR'", "'CLUSTER'", 
                     "'CLUSTERED'", "'CODEGEN'", "'COLLATE'", "'COLLECTION'", 
                     "'COLUMN'", "'COLUMNS'", "'COMMENT'", "'COMMIT'", "'COMPACT'", 
                     "'COMPACTIONS'", "'COMPUTE'", "'CONCATENATE'", "'CONSTRAINT'", 
                     "'COST'", "'CREATE'", "'CROSS'", "'CUBE'", "'CURRENT'", 
                     "'CURRENT_DATE'", "'CURRENT_TIME'", "'CURRENT_TIMESTAMP'", 
                     "'CURRENT_USER'", "'DATA'", "'DATABASE'", "<INVALID>", 
                     "'DBPROPERTIES'", "'DEFINED'", "'DELETE'", "'DELIMITED'", 
                     "'DESC'", "'DESCRIBE'", "'DFS'", "'DIRECTORIES'", "'DIRECTORY'", 
                     "'DISTINCT'", "'DISTRIBUTE'", "'DIV'", "'DROP'", "'ELSE'", 
                     "'END'", "'ESCAPE'", "'ESCAPED'", "'EXCEPT'", "'EXCHANGE'", 
                     "'EXISTS'", "'EXPLAIN'", "'EXPORT'", "'EXTENDED'", 
                     "'EXTERNAL'", "'EXTRACT'", "'FALSE'", "'FETCH'", "'FIELDS'", 
                     "'FILTER'", "'FILEFORMAT'", "'FIRST'", "'FOLLOWING'", 
                     "'FOR'", "'FOREIGN'", "'FORMAT'", "'FORMATTED'", "'FROM'", 
                     "'FULL'", "'FUNCTION'", "'FUNCTIONS'", "'GLOBAL'", 
                     "'GRANT'", "'GROUP'", "'GROUPING'", "'HAVING'", "'IF'", 
                     "'IGNORE'", "'IMPORT'", "'IN'", "'INDEX'", "'INDEXES'", 
                     "'INNER'", "'INPATH'", "'INPUTFORMAT'", "'INSERT'", 
                     "'INTERSECT'", "'INTERVAL'", "'INTO'", "'IS'", "'ITEMS'", 
                     "'JOIN'", "'KEYS'", "'LAST'", "'LATERAL'", "'LAZY'", 
                     "'LEADING'", "'LEFT'", "'LIKE'", "'LIMIT'", "'LINES'", 
                     "'LIST'", "'LOAD'", "'LOCAL'", "'LOCATION'", "'LOCK'", 
                     "'LOCKS'", "'LOGICAL'", "'MACRO'", "'MAP'", "'MATCHED'", 
                     "'MERGE'", "'MSCK'", "'NAMESPACE'", "'NAMESPACES'", 
                     "'NATURAL'", "'NO'", "<INVALID>", "'NULL'", "'NULLS'", 
                     "'OF'", "'ON'", "'ONLY'", "'OPTION'", "'OPTIONS'", 
                     "'OR'", "'ORDER'", "'OUT'", "'OUTER'", "'OUTPUTFORMAT'", 
                     "'OVER'", "'OVERLAPS'", "'OVERLAY'", "'OVERWRITE'", 
                     "'PARTITION'", "'PARTITIONED'", "'PARTITIONS'", "'PERCENT'", 
                     "'PIVOT'", "'PLACING'", "'POSITION'", "'PRECEDING'", 
                     "'PRIMARY'", "'PRINCIPALS'", "'PROPERTIES'", "'PURGE'", 
                     "'QUERY'", "'RANGE'", "'RECORDREADER'", "'RECORDWRITER'", 
                     "'RECOVER'", "'REDUCE'", "'REFERENCES'", "'REFRESH'", 
                     "'RENAME'", "'REPAIR'", "'REPLACE'", "'RESET'", "'RESTRICT'", 
                     "'REVOKE'", "'RIGHT'", "<INVALID>", "'ROLE'", "'ROLES'", 
                     "'ROLLBACK'", "'ROLLUP'", "'ROW'", "'ROWS'", "'SCHEMA'", 
                     "'SELECT'", "'SEMI'", "'SEPARATED'", "'SERDE'", "'SERDEPROPERTIES'", 
                     "'SESSION_USER'", "'SET'", "'MINUS'", "'SETS'", "'SHOW'", 
                     "'SKEWED'", "'SOME'", "'SORT'", "'SORTED'", "'START'", 
                     "'STATISTICS'", "'STORED'", "'STRATIFY'", "'STRUCT'", 
                     "'SUBSTR'", "'SUBSTRING'", "'TABLE'", "'TABLES'", "'TABLESAMPLE'", 
                     "'TBLPROPERTIES'", "<INVALID>", "'TERMINATED'", "'THEN'", 
                     "'TIME'", "'TO'", "'TOUCH'", "'TRAILING'", "'TRANSACTION'", 
                     "'TRANSACTIONS'", "'TRANSFORM'", "'TRIM'", "'TRUE'", 
                     "'TRUNCATE'", "'TYPE'", "'UNARCHIVE'", "'UNBOUNDED'", 
                     "'UNCACHE'", "'UNION'", "'UNIQUE'", "'UNKNOWN'", "'UNLOCK'", 
                     "'UNSET'", "'UPDATE'", "'USE'", "'USER'", "'USING'", 
                     "'VALUES'", "'VIEW'", "'VIEWS'", "'WHEN'", "'WHERE'", 
                     "'WINDOW'", "'WITH'", "'ZONE'", "<INVALID>", "'<=>'", 
                     "'<>'", "'!='", "'<'", "<INVALID>", "'>'", "<INVALID>", 
                     "'+'", "'-'", "'*'", "'/'", "'%'", "'~'", "'&'", "'|'", 
                     "'||'", "'^'" ]

    symbolicNames = [ "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                      "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                      "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                      "ADD", "AFTER", "ALL", "ALTER", "ANALYZE", "AND", 
                      "ANTI", "ANY", "ARCHIVE", "ARRAY", "AS", "ASC", "AT", 
                      "AUTHORIZATION", "BETWEEN", "BOTH", "BUCKET", "BUCKETS", 
                      "BY", "CACHE", "CASCADE", "CASE", "CAST", "CHANGE", 
                      "CHECK", "CLEAR", "CLUSTER", "CLUSTERED", "CODEGEN", 
                      "COLLATE", "COLLECTION", "COLUMN", "COLUMNS", "COMMENT", 
                      "COMMIT", "COMPACT", "COMPACTIONS", "COMPUTE", "CONCATENATE", 
                      "CONSTRAINT", "COST", "CREATE", "CROSS", "CUBE", "CURRENT", 
                      "CURRENT_DATE", "CURRENT_TIME", "CURRENT_TIMESTAMP", 
                      "CURRENT_USER", "DATA", "DATABASE", "DATABASES", "DBPROPERTIES", 
                      "DEFINED", "DELETE", "DELIMITED", "DESC", "DESCRIBE", 
                      "DFS", "DIRECTORIES", "DIRECTORY", "DISTINCT", "DISTRIBUTE", 
                      "DIV", "DROP", "ELSE", "END", "ESCAPE", "ESCAPED", 
                      "EXCEPT", "EXCHANGE", "EXISTS", "EXPLAIN", "EXPORT", 
                      "EXTENDED", "EXTERNAL", "EXTRACT", "FALSE", "FETCH", 
                      "FIELDS", "FILTER", "FILEFORMAT", "FIRST", "FOLLOWING", 
                      "FOR", "FOREIGN", "FORMAT", "FORMATTED", "FROM", "FULL", 
                      "FUNCTION", "FUNCTIONS", "GLOBAL", "GRANT", "GROUP", 
                      "GROUPING", "HAVING", "IF", "IGNORE", "IMPORT", "IN", 
                      "INDEX", "INDEXES", "INNER", "INPATH", "INPUTFORMAT", 
                      "INSERT", "INTERSECT", "INTERVAL", "INTO", "IS", "ITEMS", 
                      "JOIN", "KEYS", "LAST", "LATERAL", "LAZY", "LEADING", 
                      "LEFT", "LIKE", "LIMIT", "LINES", "LIST", "LOAD", 
                      "LOCAL", "LOCATION", "LOCK", "LOCKS", "LOGICAL", "MACRO", 
                      "MAP", "MATCHED", "MERGE", "MSCK", "NAMESPACE", "NAMESPACES", 
                      "NATURAL", "NO", "NOT", "NULL", "NULLS", "OF", "ON", 
                      "ONLY", "OPTION", "OPTIONS", "OR", "ORDER", "OUT", 
                      "OUTER", "OUTPUTFORMAT", "OVER", "OVERLAPS", "OVERLAY", 
                      "OVERWRITE", "PARTITION", "PARTITIONED", "PARTITIONS", 
                      "PERCENTLIT", "PIVOT", "PLACING", "POSITION", "PRECEDING", 
                      "PRIMARY", "PRINCIPALS", "PROPERTIES", "PURGE", "QUERY", 
                      "RANGE", "RECORDREADER", "RECORDWRITER", "RECOVER", 
                      "REDUCE", "REFERENCES", "REFRESH", "RENAME", "REPAIR", 
                      "REPLACE", "RESET", "RESTRICT", "REVOKE", "RIGHT", 
                      "RLIKE", "ROLE", "ROLES", "ROLLBACK", "ROLLUP", "ROW", 
                      "ROWS", "SCHEMA", "SELECT", "SEMI", "SEPARATED", "SERDE", 
                      "SERDEPROPERTIES", "SESSION_USER", "SET", "SETMINUS", 
                      "SETS", "SHOW", "SKEWED", "SOME", "SORT", "SORTED", 
                      "START", "STATISTICS", "STORED", "STRATIFY", "STRUCT", 
                      "SUBSTR", "SUBSTRING", "TABLE", "TABLES", "TABLESAMPLE", 
                      "TBLPROPERTIES", "TEMPORARY", "TERMINATED", "THEN", 
                      "TIME", "TO", "TOUCH", "TRAILING", "TRANSACTION", 
                      "TRANSACTIONS", "TRANSFORM", "TRIM", "TRUE", "TRUNCATE", 
                      "TYPE", "UNARCHIVE", "UNBOUNDED", "UNCACHE", "UNION", 
                      "UNIQUE", "UNKNOWN", "UNLOCK", "UNSET", "UPDATE", 
                      "USE", "USER", "USING", "VALUES", "VIEW", "VIEWS", 
                      "WHEN", "WHERE", "WINDOW", "WITH", "ZONE", "EQ", "NSEQ", 
                      "NEQ", "NEQJ", "LT", "LTE", "GT", "GTE", "PLUS", "MINUS", 
                      "ASTERISK", "SLASH", "PERCENT", "TILDE", "AMPERSAND", 
                      "PIPE", "CONCAT_PIPE", "HAT", "STRING", "BIGINT_LITERAL", 
                      "SMALLINT_LITERAL", "TINYINT_LITERAL", "INTEGER_VALUE", 
                      "EXPONENT_VALUE", "DECIMAL_VALUE", "FLOAT_LITERAL", 
                      "DOUBLE_LITERAL", "BIGDECIMAL_LITERAL", "IDENTIFIER", 
                      "BACKQUOTED_IDENTIFIER", "SIMPLE_COMMENT", "BRACKETED_COMMENT", 
                      "WS", "UNRECOGNIZED" ]

    RULE_singleStatement = 0
    RULE_singleExpression = 1
    RULE_singleTableIdentifier = 2
    RULE_singleMultipartIdentifier = 3
    RULE_singleFunctionIdentifier = 4
    RULE_singleDataType = 5
    RULE_singleTableSchema = 6
    RULE_statement = 7
    RULE_configKey = 8
    RULE_unsupportedHiveNativeCommands = 9
    RULE_createTableHeader = 10
    RULE_replaceTableHeader = 11
    RULE_bucketSpec = 12
    RULE_skewSpec = 13
    RULE_locationSpec = 14
    RULE_commentSpec = 15
    RULE_query = 16
    RULE_insertInto = 17
    RULE_partitionSpecLocation = 18
    RULE_partitionSpec = 19
    RULE_partitionVal = 20
    RULE_namespace = 21
    RULE_describeFuncName = 22
    RULE_describeColName = 23
    RULE_ctes = 24
    RULE_namedQuery = 25
    RULE_tableProvider = 26
    RULE_createTableClauses = 27
    RULE_tablePropertyList = 28
    RULE_tableProperty = 29
    RULE_tablePropertyKey = 30
    RULE_tablePropertyValue = 31
    RULE_constantList = 32
    RULE_nestedConstantList = 33
    RULE_createFileFormat = 34
    RULE_fileFormat = 35
    RULE_storageHandler = 36
    RULE_resource = 37
    RULE_dmlStatementNoWith = 38
    RULE_queryOrganization = 39
    RULE_multiInsertQueryBody = 40
    RULE_queryTerm = 41
    RULE_queryPrimary = 42
    RULE_sortItem = 43
    RULE_fromStatement = 44
    RULE_fromStatementBody = 45
    RULE_querySpecification = 46
    RULE_transformClause = 47
    RULE_selectClause = 48
    RULE_setClause = 49
    RULE_matchedClause = 50
    RULE_notMatchedClause = 51
    RULE_matchedAction = 52
    RULE_notMatchedAction = 53
    RULE_assignmentList = 54
    RULE_assignment = 55
    RULE_whereClause = 56
    RULE_havingClause = 57
    RULE_hint = 58
    RULE_hintStatement = 59
    RULE_fromClause = 60
    RULE_aggregationClause = 61
    RULE_groupingSet = 62
    RULE_pivotClause = 63
    RULE_pivotColumn = 64
    RULE_pivotValue = 65
    RULE_lateralView = 66
    RULE_setQuantifier = 67
    RULE_relation = 68
    RULE_joinRelation = 69
    RULE_joinType = 70
    RULE_joinCriteria = 71
    RULE_sample = 72
    RULE_sampleMethod = 73
    RULE_identifierList = 74
    RULE_identifierSeq = 75
    RULE_orderedIdentifierList = 76
    RULE_orderedIdentifier = 77
    RULE_identifierCommentList = 78
    RULE_identifierComment = 79
    RULE_relationPrimary = 80
    RULE_inlineTable = 81
    RULE_functionTable = 82
    RULE_tableAlias = 83
    RULE_rowFormat = 84
    RULE_multipartIdentifierList = 85
    RULE_multipartIdentifier = 86
    RULE_tableIdentifier = 87
    RULE_functionIdentifier = 88
    RULE_namedExpression = 89
    RULE_namedExpressionSeq = 90
    RULE_transformList = 91
    RULE_transform = 92
    RULE_transformArgument = 93
    RULE_expression = 94
    RULE_booleanExpression = 95
    RULE_predicate = 96
    RULE_valueExpression = 97
    RULE_primaryExpression = 98
    RULE_constant = 99
    RULE_comparisonOperator = 100
    RULE_arithmeticOperator = 101
    RULE_predicateOperator = 102
    RULE_booleanValue = 103
    RULE_interval = 104
    RULE_errorCapturingMultiUnitsInterval = 105
    RULE_multiUnitsInterval = 106
    RULE_errorCapturingUnitToUnitInterval = 107
    RULE_unitToUnitInterval = 108
    RULE_intervalValue = 109
    RULE_colPosition = 110
    RULE_dataType = 111
    RULE_qualifiedColTypeWithPositionList = 112
    RULE_qualifiedColTypeWithPosition = 113
    RULE_colTypeList = 114
    RULE_colType = 115
    RULE_complexColTypeList = 116
    RULE_complexColType = 117
    RULE_whenClause = 118
    RULE_windowClause = 119
    RULE_namedWindow = 120
    RULE_windowSpec = 121
    RULE_windowFrame = 122
    RULE_frameBound = 123
    RULE_qualifiedNameList = 124
    RULE_functionName = 125
    RULE_qualifiedName = 126
    RULE_errorCapturingIdentifier = 127
    RULE_identifier = 128
    RULE_strictIdentifier = 129
    RULE_quotedIdentifier = 130
    RULE_number = 131
    RULE_alterColumnAction = 132
    RULE_strictNonReserved = 133
    RULE_nonReserved = 134

    ruleNames =  [ "singleStatement", "singleExpression", "singleTableIdentifier", 
                   "singleMultipartIdentifier", "singleFunctionIdentifier", 
                   "singleDataType", "singleTableSchema", "statement", "configKey", 
                   "unsupportedHiveNativeCommands", "createTableHeader", 
                   "replaceTableHeader", "bucketSpec", "skewSpec", "locationSpec", 
                   "commentSpec", "query", "insertInto", "partitionSpecLocation", 
                   "partitionSpec", "partitionVal", "namespace", "describeFuncName", 
                   "describeColName", "ctes", "namedQuery", "tableProvider", 
                   "createTableClauses", "tablePropertyList", "tableProperty", 
                   "tablePropertyKey", "tablePropertyValue", "constantList", 
                   "nestedConstantList", "createFileFormat", "fileFormat", 
                   "storageHandler", "resource", "dmlStatementNoWith", "queryOrganization", 
                   "multiInsertQueryBody", "queryTerm", "queryPrimary", 
                   "sortItem", "fromStatement", "fromStatementBody", "querySpecification", 
                   "transformClause", "selectClause", "setClause", "matchedClause", 
                   "notMatchedClause", "matchedAction", "notMatchedAction", 
                   "assignmentList", "assignment", "whereClause", "havingClause", 
                   "hint", "hintStatement", "fromClause", "aggregationClause", 
                   "groupingSet", "pivotClause", "pivotColumn", "pivotValue", 
                   "lateralView", "setQuantifier", "relation", "joinRelation", 
                   "joinType", "joinCriteria", "sample", "sampleMethod", 
                   "identifierList", "identifierSeq", "orderedIdentifierList", 
                   "orderedIdentifier", "identifierCommentList", "identifierComment", 
                   "relationPrimary", "inlineTable", "functionTable", "tableAlias", 
                   "rowFormat", "multipartIdentifierList", "multipartIdentifier", 
                   "tableIdentifier", "functionIdentifier", "namedExpression", 
                   "namedExpressionSeq", "transformList", "transform", "transformArgument", 
                   "expression", "booleanExpression", "predicate", "valueExpression", 
                   "primaryExpression", "constant", "comparisonOperator", 
                   "arithmeticOperator", "predicateOperator", "booleanValue", 
                   "interval", "errorCapturingMultiUnitsInterval", "multiUnitsInterval", 
                   "errorCapturingUnitToUnitInterval", "unitToUnitInterval", 
                   "intervalValue", "colPosition", "dataType", "qualifiedColTypeWithPositionList", 
                   "qualifiedColTypeWithPosition", "colTypeList", "colType", 
                   "complexColTypeList", "complexColType", "whenClause", 
                   "windowClause", "namedWindow", "windowSpec", "windowFrame", 
                   "frameBound", "qualifiedNameList", "functionName", "qualifiedName", 
                   "errorCapturingIdentifier", "identifier", "strictIdentifier", 
                   "quotedIdentifier", "number", "alterColumnAction", "strictNonReserved", 
                   "nonReserved" ]

    EOF = Token.EOF
    T__0=1
    T__1=2
    T__2=3
    T__3=4
    T__4=5
    T__5=6
    T__6=7
    T__7=8
    T__8=9
    T__9=10
    T__10=11
    ADD=12
    AFTER=13
    ALL=14
    ALTER=15
    ANALYZE=16
    AND=17
    ANTI=18
    ANY=19
    ARCHIVE=20
    ARRAY=21
    AS=22
    ASC=23
    AT=24
    AUTHORIZATION=25
    BETWEEN=26
    BOTH=27
    BUCKET=28
    BUCKETS=29
    BY=30
    CACHE=31
    CASCADE=32
    CASE=33
    CAST=34
    CHANGE=35
    CHECK=36
    CLEAR=37
    CLUSTER=38
    CLUSTERED=39
    CODEGEN=40
    COLLATE=41
    COLLECTION=42
    COLUMN=43
    COLUMNS=44
    COMMENT=45
    COMMIT=46
    COMPACT=47
    COMPACTIONS=48
    COMPUTE=49
    CONCATENATE=50
    CONSTRAINT=51
    COST=52
    CREATE=53
    CROSS=54
    CUBE=55
    CURRENT=56
    CURRENT_DATE=57
    CURRENT_TIME=58
    CURRENT_TIMESTAMP=59
    CURRENT_USER=60
    DATA=61
    DATABASE=62
    DATABASES=63
    DBPROPERTIES=64
    DEFINED=65
    DELETE=66
    DELIMITED=67
    DESC=68
    DESCRIBE=69
    DFS=70
    DIRECTORIES=71
    DIRECTORY=72
    DISTINCT=73
    DISTRIBUTE=74
    DIV=75
    DROP=76
    ELSE=77
    END=78
    ESCAPE=79
    ESCAPED=80
    EXCEPT=81
    EXCHANGE=82
    EXISTS=83
    EXPLAIN=84
    EXPORT=85
    EXTENDED=86
    EXTERNAL=87
    EXTRACT=88
    FALSE=89
    FETCH=90
    FIELDS=91
    FILTER=92
    FILEFORMAT=93
    FIRST=94
    FOLLOWING=95
    FOR=96
    FOREIGN=97
    FORMAT=98
    FORMATTED=99
    FROM=100
    FULL=101
    FUNCTION=102
    FUNCTIONS=103
    GLOBAL=104
    GRANT=105
    GROUP=106
    GROUPING=107
    HAVING=108
    IF=109
    IGNORE=110
    IMPORT=111
    IN=112
    INDEX=113
    INDEXES=114
    INNER=115
    INPATH=116
    INPUTFORMAT=117
    INSERT=118
    INTERSECT=119
    INTERVAL=120
    INTO=121
    IS=122
    ITEMS=123
    JOIN=124
    KEYS=125
    LAST=126
    LATERAL=127
    LAZY=128
    LEADING=129
    LEFT=130
    LIKE=131
    LIMIT=132
    LINES=133
    LIST=134
    LOAD=135
    LOCAL=136
    LOCATION=137
    LOCK=138
    LOCKS=139
    LOGICAL=140
    MACRO=141
    MAP=142
    MATCHED=143
    MERGE=144
    MSCK=145
    NAMESPACE=146
    NAMESPACES=147
    NATURAL=148
    NO=149
    NOT=150
    NULL=151
    NULLS=152
    OF=153
    ON=154
    ONLY=155
    OPTION=156
    OPTIONS=157
    OR=158
    ORDER=159
    OUT=160
    OUTER=161
    OUTPUTFORMAT=162
    OVER=163
    OVERLAPS=164
    OVERLAY=165
    OVERWRITE=166
    PARTITION=167
    PARTITIONED=168
    PARTITIONS=169
    PERCENTLIT=170
    PIVOT=171
    PLACING=172
    POSITION=173
    PRECEDING=174
    PRIMARY=175
    PRINCIPALS=176
    PROPERTIES=177
    PURGE=178
    QUERY=179
    RANGE=180
    RECORDREADER=181
    RECORDWRITER=182
    RECOVER=183
    REDUCE=184
    REFERENCES=185
    REFRESH=186
    RENAME=187
    REPAIR=188
    REPLACE=189
    RESET=190
    RESTRICT=191
    REVOKE=192
    RIGHT=193
    RLIKE=194
    ROLE=195
    ROLES=196
    ROLLBACK=197
    ROLLUP=198
    ROW=199
    ROWS=200
    SCHEMA=201
    SELECT=202
    SEMI=203
    SEPARATED=204
    SERDE=205
    SERDEPROPERTIES=206
    SESSION_USER=207
    SET=208
    SETMINUS=209
    SETS=210
    SHOW=211
    SKEWED=212
    SOME=213
    SORT=214
    SORTED=215
    START=216
    STATISTICS=217
    STORED=218
    STRATIFY=219
    STRUCT=220
    SUBSTR=221
    SUBSTRING=222
    TABLE=223
    TABLES=224
    TABLESAMPLE=225
    TBLPROPERTIES=226
    TEMPORARY=227
    TERMINATED=228
    THEN=229
    TIME=230
    TO=231
    TOUCH=232
    TRAILING=233
    TRANSACTION=234
    TRANSACTIONS=235
    TRANSFORM=236
    TRIM=237
    TRUE=238
    TRUNCATE=239
    TYPE=240
    UNARCHIVE=241
    UNBOUNDED=242
    UNCACHE=243
    UNION=244
    UNIQUE=245
    UNKNOWN=246
    UNLOCK=247
    UNSET=248
    UPDATE=249
    USE=250
    USER=251
    USING=252
    VALUES=253
    VIEW=254
    VIEWS=255
    WHEN=256
    WHERE=257
    WINDOW=258
    WITH=259
    ZONE=260
    EQ=261
    NSEQ=262
    NEQ=263
    NEQJ=264
    LT=265
    LTE=266
    GT=267
    GTE=268
    PLUS=269
    MINUS=270
    ASTERISK=271
    SLASH=272
    PERCENT=273
    TILDE=274
    AMPERSAND=275
    PIPE=276
    CONCAT_PIPE=277
    HAT=278
    STRING=279
    BIGINT_LITERAL=280
    SMALLINT_LITERAL=281
    TINYINT_LITERAL=282
    INTEGER_VALUE=283
    EXPONENT_VALUE=284
    DECIMAL_VALUE=285
    FLOAT_LITERAL=286
    DOUBLE_LITERAL=287
    BIGDECIMAL_LITERAL=288
    IDENTIFIER=289
    BACKQUOTED_IDENTIFIER=290
    SIMPLE_COMMENT=291
    BRACKETED_COMMENT=292
    WS=293
    UNRECOGNIZED=294

    def __init__(self, input:TokenStream, output:TextIO = sys.stdout):
        super().__init__(input, output)
        self.checkVersion("4.8")
        self._interp = ParserATNSimulator(self, self.atn, self.decisionsToDFA, self.sharedContextCache)
        self._predicates = None




    class SingleStatementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def statement(self):
            return self.getTypedRuleContext(SparkSQLParser.StatementContext,0)


        def EOF(self):
            return self.getToken(SparkSQLParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_singleStatement

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleStatement" ):
                listener.enterSingleStatement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleStatement" ):
                listener.exitSingleStatement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleStatement" ):
                return visitor.visitSingleStatement(self)
            else:
                return visitor.visitChildren(self)




    def singleStatement(self):

        localctx = SparkSQLParser.SingleStatementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 0, self.RULE_singleStatement)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 270
            self.statement()
            self.state = 274
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__0:
                self.state = 271
                self.match(SparkSQLParser.T__0)
                self.state = 276
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 277
            self.match(SparkSQLParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SingleExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def namedExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.NamedExpressionContext,0)


        def EOF(self):
            return self.getToken(SparkSQLParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_singleExpression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleExpression" ):
                listener.enterSingleExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleExpression" ):
                listener.exitSingleExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleExpression" ):
                return visitor.visitSingleExpression(self)
            else:
                return visitor.visitChildren(self)




    def singleExpression(self):

        localctx = SparkSQLParser.SingleExpressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 2, self.RULE_singleExpression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 279
            self.namedExpression()
            self.state = 280
            self.match(SparkSQLParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SingleTableIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.TableIdentifierContext,0)


        def EOF(self):
            return self.getToken(SparkSQLParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_singleTableIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleTableIdentifier" ):
                listener.enterSingleTableIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleTableIdentifier" ):
                listener.exitSingleTableIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleTableIdentifier" ):
                return visitor.visitSingleTableIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def singleTableIdentifier(self):

        localctx = SparkSQLParser.SingleTableIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 4, self.RULE_singleTableIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 282
            self.tableIdentifier()
            self.state = 283
            self.match(SparkSQLParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SingleMultipartIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def EOF(self):
            return self.getToken(SparkSQLParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_singleMultipartIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleMultipartIdentifier" ):
                listener.enterSingleMultipartIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleMultipartIdentifier" ):
                listener.exitSingleMultipartIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleMultipartIdentifier" ):
                return visitor.visitSingleMultipartIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def singleMultipartIdentifier(self):

        localctx = SparkSQLParser.SingleMultipartIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 6, self.RULE_singleMultipartIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 285
            self.multipartIdentifier()
            self.state = 286
            self.match(SparkSQLParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SingleFunctionIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def functionIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.FunctionIdentifierContext,0)


        def EOF(self):
            return self.getToken(SparkSQLParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_singleFunctionIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleFunctionIdentifier" ):
                listener.enterSingleFunctionIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleFunctionIdentifier" ):
                listener.exitSingleFunctionIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleFunctionIdentifier" ):
                return visitor.visitSingleFunctionIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def singleFunctionIdentifier(self):

        localctx = SparkSQLParser.SingleFunctionIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 8, self.RULE_singleFunctionIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 288
            self.functionIdentifier()
            self.state = 289
            self.match(SparkSQLParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SingleDataTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def dataType(self):
            return self.getTypedRuleContext(SparkSQLParser.DataTypeContext,0)


        def EOF(self):
            return self.getToken(SparkSQLParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_singleDataType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleDataType" ):
                listener.enterSingleDataType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleDataType" ):
                listener.exitSingleDataType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleDataType" ):
                return visitor.visitSingleDataType(self)
            else:
                return visitor.visitChildren(self)




    def singleDataType(self):

        localctx = SparkSQLParser.SingleDataTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 10, self.RULE_singleDataType)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 291
            self.dataType()
            self.state = 292
            self.match(SparkSQLParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SingleTableSchemaContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def colTypeList(self):
            return self.getTypedRuleContext(SparkSQLParser.ColTypeListContext,0)


        def EOF(self):
            return self.getToken(SparkSQLParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_singleTableSchema

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleTableSchema" ):
                listener.enterSingleTableSchema(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleTableSchema" ):
                listener.exitSingleTableSchema(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleTableSchema" ):
                return visitor.visitSingleTableSchema(self)
            else:
                return visitor.visitChildren(self)




    def singleTableSchema(self):

        localctx = SparkSQLParser.SingleTableSchemaContext(self, self._ctx, self.state)
        self.enterRule(localctx, 12, self.RULE_singleTableSchema)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 294
            self.colTypeList()
            self.state = 295
            self.match(SparkSQLParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class StatementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_statement

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class ExplainContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def EXPLAIN(self):
            return self.getToken(SparkSQLParser.EXPLAIN, 0)
        def statement(self):
            return self.getTypedRuleContext(SparkSQLParser.StatementContext,0)

        def LOGICAL(self):
            return self.getToken(SparkSQLParser.LOGICAL, 0)
        def FORMATTED(self):
            return self.getToken(SparkSQLParser.FORMATTED, 0)
        def EXTENDED(self):
            return self.getToken(SparkSQLParser.EXTENDED, 0)
        def CODEGEN(self):
            return self.getToken(SparkSQLParser.CODEGEN, 0)
        def COST(self):
            return self.getToken(SparkSQLParser.COST, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExplain" ):
                listener.enterExplain(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExplain" ):
                listener.exitExplain(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExplain" ):
                return visitor.visitExplain(self)
            else:
                return visitor.visitChildren(self)


    class ResetConfigurationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def RESET(self):
            return self.getToken(SparkSQLParser.RESET, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterResetConfiguration" ):
                listener.enterResetConfiguration(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitResetConfiguration" ):
                listener.exitResetConfiguration(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitResetConfiguration" ):
                return visitor.visitResetConfiguration(self)
            else:
                return visitor.visitChildren(self)


    class AlterViewQueryContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAlterViewQuery" ):
                listener.enterAlterViewQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAlterViewQuery" ):
                listener.exitAlterViewQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAlterViewQuery" ):
                return visitor.visitAlterViewQuery(self)
            else:
                return visitor.visitChildren(self)


    class UseContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def USE(self):
            return self.getToken(SparkSQLParser.USE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def NAMESPACE(self):
            return self.getToken(SparkSQLParser.NAMESPACE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUse" ):
                listener.enterUse(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUse" ):
                listener.exitUse(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUse" ):
                return visitor.visitUse(self)
            else:
                return visitor.visitChildren(self)


    class DropNamespaceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)
        def namespace(self):
            return self.getTypedRuleContext(SparkSQLParser.NamespaceContext,0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def RESTRICT(self):
            return self.getToken(SparkSQLParser.RESTRICT, 0)
        def CASCADE(self):
            return self.getToken(SparkSQLParser.CASCADE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropNamespace" ):
                listener.enterDropNamespace(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropNamespace" ):
                listener.exitDropNamespace(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropNamespace" ):
                return visitor.visitDropNamespace(self)
            else:
                return visitor.visitChildren(self)


    class CreateTempViewUsingContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)
        def TEMPORARY(self):
            return self.getToken(SparkSQLParser.TEMPORARY, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.TableIdentifierContext,0)

        def tableProvider(self):
            return self.getTypedRuleContext(SparkSQLParser.TableProviderContext,0)

        def OR(self):
            return self.getToken(SparkSQLParser.OR, 0)
        def REPLACE(self):
            return self.getToken(SparkSQLParser.REPLACE, 0)
        def GLOBAL(self):
            return self.getToken(SparkSQLParser.GLOBAL, 0)
        def colTypeList(self):
            return self.getTypedRuleContext(SparkSQLParser.ColTypeListContext,0)

        def OPTIONS(self):
            return self.getToken(SparkSQLParser.OPTIONS, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTempViewUsing" ):
                listener.enterCreateTempViewUsing(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTempViewUsing" ):
                listener.exitCreateTempViewUsing(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTempViewUsing" ):
                return visitor.visitCreateTempViewUsing(self)
            else:
                return visitor.visitChildren(self)


    class RenameTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.r_from = None # MultipartIdentifierContext
            self.to = None # MultipartIdentifierContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def RENAME(self):
            return self.getToken(SparkSQLParser.RENAME, 0)
        def TO(self):
            return self.getToken(SparkSQLParser.TO, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def multipartIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultipartIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRenameTable" ):
                listener.enterRenameTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRenameTable" ):
                listener.exitRenameTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRenameTable" ):
                return visitor.visitRenameTable(self)
            else:
                return visitor.visitChildren(self)


    class FailNativeCommandContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def ROLE(self):
            return self.getToken(SparkSQLParser.ROLE, 0)
        def unsupportedHiveNativeCommands(self):
            return self.getTypedRuleContext(SparkSQLParser.UnsupportedHiveNativeCommandsContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFailNativeCommand" ):
                listener.enterFailNativeCommand(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFailNativeCommand" ):
                listener.exitFailNativeCommand(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFailNativeCommand" ):
                return visitor.visitFailNativeCommand(self)
            else:
                return visitor.visitChildren(self)


    class ClearCacheContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CLEAR(self):
            return self.getToken(SparkSQLParser.CLEAR, 0)
        def CACHE(self):
            return self.getToken(SparkSQLParser.CACHE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterClearCache" ):
                listener.enterClearCache(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitClearCache" ):
                listener.exitClearCache(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitClearCache" ):
                return visitor.visitClearCache(self)
            else:
                return visitor.visitChildren(self)


    class DropViewContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropView" ):
                listener.enterDropView(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropView" ):
                listener.exitDropView(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropView" ):
                return visitor.visitDropView(self)
            else:
                return visitor.visitChildren(self)


    class ShowTablesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def TABLES(self):
            return self.getToken(SparkSQLParser.TABLES, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowTables" ):
                listener.enterShowTables(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowTables" ):
                listener.exitShowTables(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowTables" ):
                return visitor.visitShowTables(self)
            else:
                return visitor.visitChildren(self)


    class RecoverPartitionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def RECOVER(self):
            return self.getToken(SparkSQLParser.RECOVER, 0)
        def PARTITIONS(self):
            return self.getToken(SparkSQLParser.PARTITIONS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRecoverPartitions" ):
                listener.enterRecoverPartitions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRecoverPartitions" ):
                listener.exitRecoverPartitions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRecoverPartitions" ):
                return visitor.visitRecoverPartitions(self)
            else:
                return visitor.visitChildren(self)


    class ShowCurrentNamespaceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def CURRENT(self):
            return self.getToken(SparkSQLParser.CURRENT, 0)
        def NAMESPACE(self):
            return self.getToken(SparkSQLParser.NAMESPACE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowCurrentNamespace" ):
                listener.enterShowCurrentNamespace(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowCurrentNamespace" ):
                listener.exitShowCurrentNamespace(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowCurrentNamespace" ):
                return visitor.visitShowCurrentNamespace(self)
            else:
                return visitor.visitChildren(self)


    class RenameTablePartitionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.r_from = None # PartitionSpecContext
            self.to = None # PartitionSpecContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def RENAME(self):
            return self.getToken(SparkSQLParser.RENAME, 0)
        def TO(self):
            return self.getToken(SparkSQLParser.TO, 0)
        def partitionSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.PartitionSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRenameTablePartition" ):
                listener.enterRenameTablePartition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRenameTablePartition" ):
                listener.exitRenameTablePartition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRenameTablePartition" ):
                return visitor.visitRenameTablePartition(self)
            else:
                return visitor.visitChildren(self)


    class RepairTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def MSCK(self):
            return self.getToken(SparkSQLParser.MSCK, 0)
        def REPAIR(self):
            return self.getToken(SparkSQLParser.REPAIR, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRepairTable" ):
                listener.enterRepairTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRepairTable" ):
                listener.exitRepairTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRepairTable" ):
                return visitor.visitRepairTable(self)
            else:
                return visitor.visitChildren(self)


    class RefreshResourceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def REFRESH(self):
            return self.getToken(SparkSQLParser.REFRESH, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRefreshResource" ):
                listener.enterRefreshResource(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRefreshResource" ):
                listener.exitRefreshResource(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRefreshResource" ):
                return visitor.visitRefreshResource(self)
            else:
                return visitor.visitChildren(self)


    class ShowCreateTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)
        def SERDE(self):
            return self.getToken(SparkSQLParser.SERDE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowCreateTable" ):
                listener.enterShowCreateTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowCreateTable" ):
                listener.exitShowCreateTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowCreateTable" ):
                return visitor.visitShowCreateTable(self)
            else:
                return visitor.visitChildren(self)


    class ShowNamespacesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def DATABASES(self):
            return self.getToken(SparkSQLParser.DATABASES, 0)
        def NAMESPACES(self):
            return self.getToken(SparkSQLParser.NAMESPACES, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowNamespaces" ):
                listener.enterShowNamespaces(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowNamespaces" ):
                listener.exitShowNamespaces(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowNamespaces" ):
                return visitor.visitShowNamespaces(self)
            else:
                return visitor.visitChildren(self)


    class ShowColumnsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.table = None # MultipartIdentifierContext
            self.ns = None # MultipartIdentifierContext
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def COLUMNS(self):
            return self.getToken(SparkSQLParser.COLUMNS, 0)
        def FROM(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.FROM)
            else:
                return self.getToken(SparkSQLParser.FROM, i)
        def IN(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.IN)
            else:
                return self.getToken(SparkSQLParser.IN, i)
        def multipartIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultipartIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowColumns" ):
                listener.enterShowColumns(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowColumns" ):
                listener.exitShowColumns(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowColumns" ):
                return visitor.visitShowColumns(self)
            else:
                return visitor.visitChildren(self)


    class ReplaceTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def replaceTableHeader(self):
            return self.getTypedRuleContext(SparkSQLParser.ReplaceTableHeaderContext,0)

        def tableProvider(self):
            return self.getTypedRuleContext(SparkSQLParser.TableProviderContext,0)

        def createTableClauses(self):
            return self.getTypedRuleContext(SparkSQLParser.CreateTableClausesContext,0)

        def colTypeList(self):
            return self.getTypedRuleContext(SparkSQLParser.ColTypeListContext,0)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterReplaceTable" ):
                listener.enterReplaceTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitReplaceTable" ):
                listener.exitReplaceTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitReplaceTable" ):
                return visitor.visitReplaceTable(self)
            else:
                return visitor.visitChildren(self)


    class AddTablePartitionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def ADD(self):
            return self.getToken(SparkSQLParser.ADD, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def partitionSpecLocation(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.PartitionSpecLocationContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.PartitionSpecLocationContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAddTablePartition" ):
                listener.enterAddTablePartition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAddTablePartition" ):
                listener.exitAddTablePartition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAddTablePartition" ):
                return visitor.visitAddTablePartition(self)
            else:
                return visitor.visitChildren(self)


    class SetNamespaceLocationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def namespace(self):
            return self.getTypedRuleContext(SparkSQLParser.NamespaceContext,0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def locationSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.LocationSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetNamespaceLocation" ):
                listener.enterSetNamespaceLocation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetNamespaceLocation" ):
                listener.exitSetNamespaceLocation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetNamespaceLocation" ):
                return visitor.visitSetNamespaceLocation(self)
            else:
                return visitor.visitChildren(self)


    class RefreshTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def REFRESH(self):
            return self.getToken(SparkSQLParser.REFRESH, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRefreshTable" ):
                listener.enterRefreshTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRefreshTable" ):
                listener.exitRefreshTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRefreshTable" ):
                return visitor.visitRefreshTable(self)
            else:
                return visitor.visitChildren(self)


    class SetNamespacePropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def namespace(self):
            return self.getTypedRuleContext(SparkSQLParser.NamespaceContext,0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)

        def DBPROPERTIES(self):
            return self.getToken(SparkSQLParser.DBPROPERTIES, 0)
        def PROPERTIES(self):
            return self.getToken(SparkSQLParser.PROPERTIES, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetNamespaceProperties" ):
                listener.enterSetNamespaceProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetNamespaceProperties" ):
                listener.exitSetNamespaceProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetNamespaceProperties" ):
                return visitor.visitSetNamespaceProperties(self)
            else:
                return visitor.visitChildren(self)


    class ManageResourceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.op = None # Token
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def ADD(self):
            return self.getToken(SparkSQLParser.ADD, 0)
        def LIST(self):
            return self.getToken(SparkSQLParser.LIST, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterManageResource" ):
                listener.enterManageResource(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitManageResource" ):
                listener.exitManageResource(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitManageResource" ):
                return visitor.visitManageResource(self)
            else:
                return visitor.visitChildren(self)


    class SetQuotedConfigurationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def configKey(self):
            return self.getTypedRuleContext(SparkSQLParser.ConfigKeyContext,0)

        def EQ(self):
            return self.getToken(SparkSQLParser.EQ, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetQuotedConfiguration" ):
                listener.enterSetQuotedConfiguration(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetQuotedConfiguration" ):
                listener.exitSetQuotedConfiguration(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetQuotedConfiguration" ):
                return visitor.visitSetQuotedConfiguration(self)
            else:
                return visitor.visitChildren(self)


    class AnalyzeContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ANALYZE(self):
            return self.getToken(SparkSQLParser.ANALYZE, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def COMPUTE(self):
            return self.getToken(SparkSQLParser.COMPUTE, 0)
        def STATISTICS(self):
            return self.getToken(SparkSQLParser.STATISTICS, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def FOR(self):
            return self.getToken(SparkSQLParser.FOR, 0)
        def COLUMNS(self):
            return self.getToken(SparkSQLParser.COLUMNS, 0)
        def identifierSeq(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierSeqContext,0)

        def ALL(self):
            return self.getToken(SparkSQLParser.ALL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAnalyze" ):
                listener.enterAnalyze(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAnalyze" ):
                listener.exitAnalyze(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAnalyze" ):
                return visitor.visitAnalyze(self)
            else:
                return visitor.visitChildren(self)


    class CreateHiveTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.columns = None # ColTypeListContext
            self.partitionColumns = None # ColTypeListContext
            self.partitionColumnNames = None # IdentifierListContext
            self.tableProps = None # TablePropertyListContext
            self.copyFrom(ctx)

        def createTableHeader(self):
            return self.getTypedRuleContext(SparkSQLParser.CreateTableHeaderContext,0)

        def commentSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.CommentSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,i)

        def bucketSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.BucketSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.BucketSpecContext,i)

        def skewSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.SkewSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.SkewSpecContext,i)

        def rowFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.RowFormatContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.RowFormatContext,i)

        def createFileFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.CreateFileFormatContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.CreateFileFormatContext,i)

        def locationSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.LocationSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.LocationSpecContext,i)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def colTypeList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ColTypeListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ColTypeListContext,i)

        def PARTITIONED(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.PARTITIONED)
            else:
                return self.getToken(SparkSQLParser.PARTITIONED, i)
        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.BY)
            else:
                return self.getToken(SparkSQLParser.BY, i)
        def TBLPROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.TBLPROPERTIES)
            else:
                return self.getToken(SparkSQLParser.TBLPROPERTIES, i)
        def identifierList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,i)

        def tablePropertyList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TablePropertyListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,i)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateHiveTable" ):
                listener.enterCreateHiveTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateHiveTable" ):
                listener.exitCreateHiveTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateHiveTable" ):
                return visitor.visitCreateHiveTable(self)
            else:
                return visitor.visitChildren(self)


    class CreateFunctionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.className = None # Token
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)
        def FUNCTION(self):
            return self.getToken(SparkSQLParser.FUNCTION, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def OR(self):
            return self.getToken(SparkSQLParser.OR, 0)
        def REPLACE(self):
            return self.getToken(SparkSQLParser.REPLACE, 0)
        def TEMPORARY(self):
            return self.getToken(SparkSQLParser.TEMPORARY, 0)
        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def USING(self):
            return self.getToken(SparkSQLParser.USING, 0)
        def resource(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ResourceContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ResourceContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateFunction" ):
                listener.enterCreateFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateFunction" ):
                listener.exitCreateFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateFunction" ):
                return visitor.visitCreateFunction(self)
            else:
                return visitor.visitChildren(self)


    class ShowTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.ns = None # MultipartIdentifierContext
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def EXTENDED(self):
            return self.getToken(SparkSQLParser.EXTENDED, 0)
        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)

        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowTable" ):
                listener.enterShowTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowTable" ):
                listener.exitShowTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowTable" ):
                return visitor.visitShowTable(self)
            else:
                return visitor.visitChildren(self)


    class HiveReplaceColumnsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.table = None # MultipartIdentifierContext
            self.columns = None # QualifiedColTypeWithPositionListContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def REPLACE(self):
            return self.getToken(SparkSQLParser.REPLACE, 0)
        def COLUMNS(self):
            return self.getToken(SparkSQLParser.COLUMNS, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def qualifiedColTypeWithPositionList(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedColTypeWithPositionListContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterHiveReplaceColumns" ):
                listener.enterHiveReplaceColumns(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitHiveReplaceColumns" ):
                listener.exitHiveReplaceColumns(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitHiveReplaceColumns" ):
                return visitor.visitHiveReplaceColumns(self)
            else:
                return visitor.visitChildren(self)


    class CommentNamespaceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.comment = None # Token
            self.copyFrom(ctx)

        def COMMENT(self):
            return self.getToken(SparkSQLParser.COMMENT, 0)
        def ON(self):
            return self.getToken(SparkSQLParser.ON, 0)
        def namespace(self):
            return self.getTypedRuleContext(SparkSQLParser.NamespaceContext,0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def IS(self):
            return self.getToken(SparkSQLParser.IS, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCommentNamespace" ):
                listener.enterCommentNamespace(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCommentNamespace" ):
                listener.exitCommentNamespace(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCommentNamespace" ):
                return visitor.visitCommentNamespace(self)
            else:
                return visitor.visitChildren(self)


    class ResetQuotedConfigurationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def RESET(self):
            return self.getToken(SparkSQLParser.RESET, 0)
        def configKey(self):
            return self.getTypedRuleContext(SparkSQLParser.ConfigKeyContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterResetQuotedConfiguration" ):
                listener.enterResetQuotedConfiguration(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitResetQuotedConfiguration" ):
                listener.exitResetQuotedConfiguration(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitResetQuotedConfiguration" ):
                return visitor.visitResetQuotedConfiguration(self)
            else:
                return visitor.visitChildren(self)


    class CreateTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def createTableHeader(self):
            return self.getTypedRuleContext(SparkSQLParser.CreateTableHeaderContext,0)

        def tableProvider(self):
            return self.getTypedRuleContext(SparkSQLParser.TableProviderContext,0)

        def createTableClauses(self):
            return self.getTypedRuleContext(SparkSQLParser.CreateTableClausesContext,0)

        def colTypeList(self):
            return self.getTypedRuleContext(SparkSQLParser.ColTypeListContext,0)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTable" ):
                listener.enterCreateTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTable" ):
                listener.exitCreateTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTable" ):
                return visitor.visitCreateTable(self)
            else:
                return visitor.visitChildren(self)


    class DmlStatementContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def dmlStatementNoWith(self):
            return self.getTypedRuleContext(SparkSQLParser.DmlStatementNoWithContext,0)

        def ctes(self):
            return self.getTypedRuleContext(SparkSQLParser.CtesContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDmlStatement" ):
                listener.enterDmlStatement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDmlStatement" ):
                listener.exitDmlStatement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDmlStatement" ):
                return visitor.visitDmlStatement(self)
            else:
                return visitor.visitChildren(self)


    class CreateTableLikeContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.target = None # TableIdentifierContext
            self.source = None # TableIdentifierContext
            self.tableProps = None # TablePropertyListContext
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)
        def tableIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TableIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TableIdentifierContext,i)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def tableProvider(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TableProviderContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TableProviderContext,i)

        def rowFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.RowFormatContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.RowFormatContext,i)

        def createFileFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.CreateFileFormatContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.CreateFileFormatContext,i)

        def locationSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.LocationSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.LocationSpecContext,i)

        def TBLPROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.TBLPROPERTIES)
            else:
                return self.getToken(SparkSQLParser.TBLPROPERTIES, i)
        def tablePropertyList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TablePropertyListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTableLike" ):
                listener.enterCreateTableLike(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTableLike" ):
                listener.exitCreateTableLike(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTableLike" ):
                return visitor.visitCreateTableLike(self)
            else:
                return visitor.visitChildren(self)


    class UncacheTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def UNCACHE(self):
            return self.getToken(SparkSQLParser.UNCACHE, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUncacheTable" ):
                listener.enterUncacheTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUncacheTable" ):
                listener.exitUncacheTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUncacheTable" ):
                return visitor.visitUncacheTable(self)
            else:
                return visitor.visitChildren(self)


    class DropFunctionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)
        def FUNCTION(self):
            return self.getToken(SparkSQLParser.FUNCTION, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def TEMPORARY(self):
            return self.getToken(SparkSQLParser.TEMPORARY, 0)
        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropFunction" ):
                listener.enterDropFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropFunction" ):
                listener.exitDropFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropFunction" ):
                return visitor.visitDropFunction(self)
            else:
                return visitor.visitChildren(self)


    class DescribeRelationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.option = None # Token
            self.copyFrom(ctx)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def DESC(self):
            return self.getToken(SparkSQLParser.DESC, 0)
        def DESCRIBE(self):
            return self.getToken(SparkSQLParser.DESCRIBE, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)

        def describeColName(self):
            return self.getTypedRuleContext(SparkSQLParser.DescribeColNameContext,0)

        def EXTENDED(self):
            return self.getToken(SparkSQLParser.EXTENDED, 0)
        def FORMATTED(self):
            return self.getToken(SparkSQLParser.FORMATTED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeRelation" ):
                listener.enterDescribeRelation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeRelation" ):
                listener.exitDescribeRelation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeRelation" ):
                return visitor.visitDescribeRelation(self)
            else:
                return visitor.visitChildren(self)


    class LoadDataContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.path = None # Token
            self.copyFrom(ctx)

        def LOAD(self):
            return self.getToken(SparkSQLParser.LOAD, 0)
        def DATA(self):
            return self.getToken(SparkSQLParser.DATA, 0)
        def INPATH(self):
            return self.getToken(SparkSQLParser.INPATH, 0)
        def INTO(self):
            return self.getToken(SparkSQLParser.INTO, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def LOCAL(self):
            return self.getToken(SparkSQLParser.LOCAL, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSQLParser.OVERWRITE, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLoadData" ):
                listener.enterLoadData(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLoadData" ):
                listener.exitLoadData(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLoadData" ):
                return visitor.visitLoadData(self)
            else:
                return visitor.visitChildren(self)


    class ShowPartitionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def PARTITIONS(self):
            return self.getToken(SparkSQLParser.PARTITIONS, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowPartitions" ):
                listener.enterShowPartitions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowPartitions" ):
                listener.exitShowPartitions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowPartitions" ):
                return visitor.visitShowPartitions(self)
            else:
                return visitor.visitChildren(self)


    class DescribeFunctionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def FUNCTION(self):
            return self.getToken(SparkSQLParser.FUNCTION, 0)
        def describeFuncName(self):
            return self.getTypedRuleContext(SparkSQLParser.DescribeFuncNameContext,0)

        def DESC(self):
            return self.getToken(SparkSQLParser.DESC, 0)
        def DESCRIBE(self):
            return self.getToken(SparkSQLParser.DESCRIBE, 0)
        def EXTENDED(self):
            return self.getToken(SparkSQLParser.EXTENDED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeFunction" ):
                listener.enterDescribeFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeFunction" ):
                listener.exitDescribeFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeFunction" ):
                return visitor.visitDescribeFunction(self)
            else:
                return visitor.visitChildren(self)


    class RenameTableColumnContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.table = None # MultipartIdentifierContext
            self.r_from = None # MultipartIdentifierContext
            self.to = None # ErrorCapturingIdentifierContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def RENAME(self):
            return self.getToken(SparkSQLParser.RENAME, 0)
        def COLUMN(self):
            return self.getToken(SparkSQLParser.COLUMN, 0)
        def TO(self):
            return self.getToken(SparkSQLParser.TO, 0)
        def multipartIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultipartIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,i)

        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRenameTableColumn" ):
                listener.enterRenameTableColumn(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRenameTableColumn" ):
                listener.exitRenameTableColumn(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRenameTableColumn" ):
                return visitor.visitRenameTableColumn(self)
            else:
                return visitor.visitChildren(self)


    class StatementDefaultContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStatementDefault" ):
                listener.enterStatementDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStatementDefault" ):
                listener.exitStatementDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStatementDefault" ):
                return visitor.visitStatementDefault(self)
            else:
                return visitor.visitChildren(self)


    class HiveChangeColumnContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.table = None # MultipartIdentifierContext
            self.colName = None # MultipartIdentifierContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def CHANGE(self):
            return self.getToken(SparkSQLParser.CHANGE, 0)
        def colType(self):
            return self.getTypedRuleContext(SparkSQLParser.ColTypeContext,0)

        def multipartIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultipartIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,i)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)

        def COLUMN(self):
            return self.getToken(SparkSQLParser.COLUMN, 0)
        def colPosition(self):
            return self.getTypedRuleContext(SparkSQLParser.ColPositionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterHiveChangeColumn" ):
                listener.enterHiveChangeColumn(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitHiveChangeColumn" ):
                listener.exitHiveChangeColumn(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitHiveChangeColumn" ):
                return visitor.visitHiveChangeColumn(self)
            else:
                return visitor.visitChildren(self)


    class SetTimeZoneContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.timezone = None # Token
            self.copyFrom(ctx)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def TIME(self):
            return self.getToken(SparkSQLParser.TIME, 0)
        def ZONE(self):
            return self.getToken(SparkSQLParser.ZONE, 0)
        def interval(self):
            return self.getTypedRuleContext(SparkSQLParser.IntervalContext,0)

        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def LOCAL(self):
            return self.getToken(SparkSQLParser.LOCAL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetTimeZone" ):
                listener.enterSetTimeZone(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetTimeZone" ):
                listener.exitSetTimeZone(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetTimeZone" ):
                return visitor.visitSetTimeZone(self)
            else:
                return visitor.visitChildren(self)


    class DescribeQueryContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def DESC(self):
            return self.getToken(SparkSQLParser.DESC, 0)
        def DESCRIBE(self):
            return self.getToken(SparkSQLParser.DESCRIBE, 0)
        def QUERY(self):
            return self.getToken(SparkSQLParser.QUERY, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeQuery" ):
                listener.enterDescribeQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeQuery" ):
                listener.exitDescribeQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeQuery" ):
                return visitor.visitDescribeQuery(self)
            else:
                return visitor.visitChildren(self)


    class TruncateTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def TRUNCATE(self):
            return self.getToken(SparkSQLParser.TRUNCATE, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTruncateTable" ):
                listener.enterTruncateTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTruncateTable" ):
                listener.exitTruncateTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTruncateTable" ):
                return visitor.visitTruncateTable(self)
            else:
                return visitor.visitChildren(self)


    class SetTableSerDeContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def SERDE(self):
            return self.getToken(SparkSQLParser.SERDE, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)

        def WITH(self):
            return self.getToken(SparkSQLParser.WITH, 0)
        def SERDEPROPERTIES(self):
            return self.getToken(SparkSQLParser.SERDEPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetTableSerDe" ):
                listener.enterSetTableSerDe(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetTableSerDe" ):
                listener.exitSetTableSerDe(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetTableSerDe" ):
                return visitor.visitSetTableSerDe(self)
            else:
                return visitor.visitChildren(self)


    class CreateViewContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)
        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def OR(self):
            return self.getToken(SparkSQLParser.OR, 0)
        def REPLACE(self):
            return self.getToken(SparkSQLParser.REPLACE, 0)
        def TEMPORARY(self):
            return self.getToken(SparkSQLParser.TEMPORARY, 0)
        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def identifierCommentList(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierCommentListContext,0)

        def commentSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.CommentSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,i)

        def PARTITIONED(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.PARTITIONED)
            else:
                return self.getToken(SparkSQLParser.PARTITIONED, i)
        def ON(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.ON)
            else:
                return self.getToken(SparkSQLParser.ON, i)
        def identifierList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,i)

        def TBLPROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.TBLPROPERTIES)
            else:
                return self.getToken(SparkSQLParser.TBLPROPERTIES, i)
        def tablePropertyList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TablePropertyListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,i)

        def GLOBAL(self):
            return self.getToken(SparkSQLParser.GLOBAL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateView" ):
                listener.enterCreateView(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateView" ):
                listener.exitCreateView(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateView" ):
                return visitor.visitCreateView(self)
            else:
                return visitor.visitChildren(self)


    class DropTablePartitionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)
        def partitionSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.PartitionSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,i)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def PURGE(self):
            return self.getToken(SparkSQLParser.PURGE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropTablePartitions" ):
                listener.enterDropTablePartitions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropTablePartitions" ):
                listener.exitDropTablePartitions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropTablePartitions" ):
                return visitor.visitDropTablePartitions(self)
            else:
                return visitor.visitChildren(self)


    class SetConfigurationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetConfiguration" ):
                listener.enterSetConfiguration(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetConfiguration" ):
                listener.exitSetConfiguration(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetConfiguration" ):
                return visitor.visitSetConfiguration(self)
            else:
                return visitor.visitChildren(self)


    class DropTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def PURGE(self):
            return self.getToken(SparkSQLParser.PURGE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropTable" ):
                listener.enterDropTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropTable" ):
                listener.exitDropTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropTable" ):
                return visitor.visitDropTable(self)
            else:
                return visitor.visitChildren(self)


    class DescribeNamespaceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def namespace(self):
            return self.getTypedRuleContext(SparkSQLParser.NamespaceContext,0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def DESC(self):
            return self.getToken(SparkSQLParser.DESC, 0)
        def DESCRIBE(self):
            return self.getToken(SparkSQLParser.DESCRIBE, 0)
        def EXTENDED(self):
            return self.getToken(SparkSQLParser.EXTENDED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeNamespace" ):
                listener.enterDescribeNamespace(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeNamespace" ):
                listener.exitDescribeNamespace(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeNamespace" ):
                return visitor.visitDescribeNamespace(self)
            else:
                return visitor.visitChildren(self)


    class AlterTableAlterColumnContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.table = None # MultipartIdentifierContext
            self.column = None # MultipartIdentifierContext
            self.copyFrom(ctx)

        def ALTER(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.ALTER)
            else:
                return self.getToken(SparkSQLParser.ALTER, i)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultipartIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,i)

        def CHANGE(self):
            return self.getToken(SparkSQLParser.CHANGE, 0)
        def COLUMN(self):
            return self.getToken(SparkSQLParser.COLUMN, 0)
        def alterColumnAction(self):
            return self.getTypedRuleContext(SparkSQLParser.AlterColumnActionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAlterTableAlterColumn" ):
                listener.enterAlterTableAlterColumn(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAlterTableAlterColumn" ):
                listener.exitAlterTableAlterColumn(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAlterTableAlterColumn" ):
                return visitor.visitAlterTableAlterColumn(self)
            else:
                return visitor.visitChildren(self)


    class RefreshFunctionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def REFRESH(self):
            return self.getToken(SparkSQLParser.REFRESH, 0)
        def FUNCTION(self):
            return self.getToken(SparkSQLParser.FUNCTION, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRefreshFunction" ):
                listener.enterRefreshFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRefreshFunction" ):
                listener.exitRefreshFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRefreshFunction" ):
                return visitor.visitRefreshFunction(self)
            else:
                return visitor.visitChildren(self)


    class CommentTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.comment = None # Token
            self.copyFrom(ctx)

        def COMMENT(self):
            return self.getToken(SparkSQLParser.COMMENT, 0)
        def ON(self):
            return self.getToken(SparkSQLParser.ON, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def IS(self):
            return self.getToken(SparkSQLParser.IS, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCommentTable" ):
                listener.enterCommentTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCommentTable" ):
                listener.exitCommentTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCommentTable" ):
                return visitor.visitCommentTable(self)
            else:
                return visitor.visitChildren(self)


    class CreateNamespaceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)
        def namespace(self):
            return self.getTypedRuleContext(SparkSQLParser.NamespaceContext,0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def commentSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.CommentSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,i)

        def locationSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.LocationSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.LocationSpecContext,i)

        def WITH(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.WITH)
            else:
                return self.getToken(SparkSQLParser.WITH, i)
        def tablePropertyList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TablePropertyListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,i)

        def DBPROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.DBPROPERTIES)
            else:
                return self.getToken(SparkSQLParser.DBPROPERTIES, i)
        def PROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.PROPERTIES)
            else:
                return self.getToken(SparkSQLParser.PROPERTIES, i)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateNamespace" ):
                listener.enterCreateNamespace(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateNamespace" ):
                listener.exitCreateNamespace(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateNamespace" ):
                return visitor.visitCreateNamespace(self)
            else:
                return visitor.visitChildren(self)


    class ShowTblPropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.table = None # MultipartIdentifierContext
            self.key = None # TablePropertyKeyContext
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def TBLPROPERTIES(self):
            return self.getToken(SparkSQLParser.TBLPROPERTIES, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def tablePropertyKey(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyKeyContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowTblProperties" ):
                listener.enterShowTblProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowTblProperties" ):
                listener.exitShowTblProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowTblProperties" ):
                return visitor.visitShowTblProperties(self)
            else:
                return visitor.visitChildren(self)


    class UnsetTablePropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def UNSET(self):
            return self.getToken(SparkSQLParser.UNSET, 0)
        def TBLPROPERTIES(self):
            return self.getToken(SparkSQLParser.TBLPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)
        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnsetTableProperties" ):
                listener.enterUnsetTableProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnsetTableProperties" ):
                listener.exitUnsetTableProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnsetTableProperties" ):
                return visitor.visitUnsetTableProperties(self)
            else:
                return visitor.visitChildren(self)


    class SetTableLocationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def locationSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.LocationSpecContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetTableLocation" ):
                listener.enterSetTableLocation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetTableLocation" ):
                listener.exitSetTableLocation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetTableLocation" ):
                return visitor.visitSetTableLocation(self)
            else:
                return visitor.visitChildren(self)


    class DropTableColumnsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.columns = None # MultipartIdentifierListContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)
        def COLUMN(self):
            return self.getToken(SparkSQLParser.COLUMN, 0)
        def COLUMNS(self):
            return self.getToken(SparkSQLParser.COLUMNS, 0)
        def multipartIdentifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropTableColumns" ):
                listener.enterDropTableColumns(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropTableColumns" ):
                listener.exitDropTableColumns(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropTableColumns" ):
                return visitor.visitDropTableColumns(self)
            else:
                return visitor.visitChildren(self)


    class ShowViewsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def VIEWS(self):
            return self.getToken(SparkSQLParser.VIEWS, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowViews" ):
                listener.enterShowViews(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowViews" ):
                listener.exitShowViews(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowViews" ):
                return visitor.visitShowViews(self)
            else:
                return visitor.visitChildren(self)


    class ShowFunctionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)
        def FUNCTIONS(self):
            return self.getToken(SparkSQLParser.FUNCTIONS, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowFunctions" ):
                listener.enterShowFunctions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowFunctions" ):
                listener.exitShowFunctions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowFunctions" ):
                return visitor.visitShowFunctions(self)
            else:
                return visitor.visitChildren(self)


    class CacheTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.options = None # TablePropertyListContext
            self.copyFrom(ctx)

        def CACHE(self):
            return self.getToken(SparkSQLParser.CACHE, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def LAZY(self):
            return self.getToken(SparkSQLParser.LAZY, 0)
        def OPTIONS(self):
            return self.getToken(SparkSQLParser.OPTIONS, 0)
        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCacheTable" ):
                listener.enterCacheTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCacheTable" ):
                listener.exitCacheTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCacheTable" ):
                return visitor.visitCacheTable(self)
            else:
                return visitor.visitChildren(self)


    class AddTableColumnsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.columns = None # QualifiedColTypeWithPositionListContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def ADD(self):
            return self.getToken(SparkSQLParser.ADD, 0)
        def COLUMN(self):
            return self.getToken(SparkSQLParser.COLUMN, 0)
        def COLUMNS(self):
            return self.getToken(SparkSQLParser.COLUMNS, 0)
        def qualifiedColTypeWithPositionList(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedColTypeWithPositionListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAddTableColumns" ):
                listener.enterAddTableColumns(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAddTableColumns" ):
                listener.exitAddTableColumns(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAddTableColumns" ):
                return visitor.visitAddTableColumns(self)
            else:
                return visitor.visitChildren(self)


    class SetTablePropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)
        def TBLPROPERTIES(self):
            return self.getToken(SparkSQLParser.TBLPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetTableProperties" ):
                listener.enterSetTableProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetTableProperties" ):
                listener.exitSetTableProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetTableProperties" ):
                return visitor.visitSetTableProperties(self)
            else:
                return visitor.visitChildren(self)



    def statement(self):

        localctx = SparkSQLParser.StatementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 14, self.RULE_statement)
        self._la = 0 # Token type
        try:
            self.state = 1041
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,110,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.StatementDefaultContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 297
                self.query()
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.DmlStatementContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 299
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.WITH:
                    self.state = 298
                    self.ctes()


                self.state = 301
                self.dmlStatementNoWith()
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.UseContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 302
                self.match(SparkSQLParser.USE)
                self.state = 304
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,2,self._ctx)
                if la_ == 1:
                    self.state = 303
                    self.match(SparkSQLParser.NAMESPACE)


                self.state = 306
                self.multipartIdentifier()
                pass

            elif la_ == 4:
                localctx = SparkSQLParser.CreateNamespaceContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 307
                self.match(SparkSQLParser.CREATE)
                self.state = 308
                self.namespace()
                self.state = 312
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,3,self._ctx)
                if la_ == 1:
                    self.state = 309
                    self.match(SparkSQLParser.IF)
                    self.state = 310
                    self.match(SparkSQLParser.NOT)
                    self.state = 311
                    self.match(SparkSQLParser.EXISTS)


                self.state = 314
                self.multipartIdentifier()
                self.state = 322
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.COMMENT or _la==SparkSQLParser.LOCATION or _la==SparkSQLParser.WITH:
                    self.state = 320
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSQLParser.COMMENT]:
                        self.state = 315
                        self.commentSpec()
                        pass
                    elif token in [SparkSQLParser.LOCATION]:
                        self.state = 316
                        self.locationSpec()
                        pass
                    elif token in [SparkSQLParser.WITH]:
                        self.state = 317
                        self.match(SparkSQLParser.WITH)
                        self.state = 318
                        _la = self._input.LA(1)
                        if not(_la==SparkSQLParser.DBPROPERTIES or _la==SparkSQLParser.PROPERTIES):
                            self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 319
                        self.tablePropertyList()
                        pass
                    else:
                        raise NoViableAltException(self)

                    self.state = 324
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                pass

            elif la_ == 5:
                localctx = SparkSQLParser.SetNamespacePropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 325
                self.match(SparkSQLParser.ALTER)
                self.state = 326
                self.namespace()
                self.state = 327
                self.multipartIdentifier()
                self.state = 328
                self.match(SparkSQLParser.SET)
                self.state = 329
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.DBPROPERTIES or _la==SparkSQLParser.PROPERTIES):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 330
                self.tablePropertyList()
                pass

            elif la_ == 6:
                localctx = SparkSQLParser.SetNamespaceLocationContext(self, localctx)
                self.enterOuterAlt(localctx, 6)
                self.state = 332
                self.match(SparkSQLParser.ALTER)
                self.state = 333
                self.namespace()
                self.state = 334
                self.multipartIdentifier()
                self.state = 335
                self.match(SparkSQLParser.SET)
                self.state = 336
                self.locationSpec()
                pass

            elif la_ == 7:
                localctx = SparkSQLParser.DropNamespaceContext(self, localctx)
                self.enterOuterAlt(localctx, 7)
                self.state = 338
                self.match(SparkSQLParser.DROP)
                self.state = 339
                self.namespace()
                self.state = 342
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,6,self._ctx)
                if la_ == 1:
                    self.state = 340
                    self.match(SparkSQLParser.IF)
                    self.state = 341
                    self.match(SparkSQLParser.EXISTS)


                self.state = 344
                self.multipartIdentifier()
                self.state = 346
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.CASCADE or _la==SparkSQLParser.RESTRICT:
                    self.state = 345
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.CASCADE or _la==SparkSQLParser.RESTRICT):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                pass

            elif la_ == 8:
                localctx = SparkSQLParser.ShowNamespacesContext(self, localctx)
                self.enterOuterAlt(localctx, 8)
                self.state = 348
                self.match(SparkSQLParser.SHOW)
                self.state = 349
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.DATABASES or _la==SparkSQLParser.NAMESPACES):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 352
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.FROM or _la==SparkSQLParser.IN:
                    self.state = 350
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.FROM or _la==SparkSQLParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 351
                    self.multipartIdentifier()


                self.state = 358
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LIKE or _la==SparkSQLParser.STRING:
                    self.state = 355
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.LIKE:
                        self.state = 354
                        self.match(SparkSQLParser.LIKE)


                    self.state = 357
                    localctx.pattern = self.match(SparkSQLParser.STRING)


                pass

            elif la_ == 9:
                localctx = SparkSQLParser.CreateTableContext(self, localctx)
                self.enterOuterAlt(localctx, 9)
                self.state = 360
                self.createTableHeader()
                self.state = 365
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1:
                    self.state = 361
                    self.match(SparkSQLParser.T__1)
                    self.state = 362
                    self.colTypeList()
                    self.state = 363
                    self.match(SparkSQLParser.T__2)


                self.state = 367
                self.tableProvider()
                self.state = 368
                self.createTableClauses()
                self.state = 373
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1 or _la==SparkSQLParser.AS or _la==SparkSQLParser.FROM or _la==SparkSQLParser.MAP or ((((_la - 184)) & ~0x3f) == 0 and ((1 << (_la - 184)) & ((1 << (SparkSQLParser.REDUCE - 184)) | (1 << (SparkSQLParser.SELECT - 184)) | (1 << (SparkSQLParser.TABLE - 184)))) != 0) or _la==SparkSQLParser.VALUES or _la==SparkSQLParser.WITH:
                    self.state = 370
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.AS:
                        self.state = 369
                        self.match(SparkSQLParser.AS)


                    self.state = 372
                    self.query()


                pass

            elif la_ == 10:
                localctx = SparkSQLParser.CreateHiveTableContext(self, localctx)
                self.enterOuterAlt(localctx, 10)
                self.state = 375
                self.createTableHeader()
                self.state = 380
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,14,self._ctx)
                if la_ == 1:
                    self.state = 376
                    self.match(SparkSQLParser.T__1)
                    self.state = 377
                    localctx.columns = self.colTypeList()
                    self.state = 378
                    self.match(SparkSQLParser.T__2)


                self.state = 403
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.CLUSTERED or _la==SparkSQLParser.COMMENT or ((((_la - 137)) & ~0x3f) == 0 and ((1 << (_la - 137)) & ((1 << (SparkSQLParser.LOCATION - 137)) | (1 << (SparkSQLParser.PARTITIONED - 137)) | (1 << (SparkSQLParser.ROW - 137)))) != 0) or ((((_la - 212)) & ~0x3f) == 0 and ((1 << (_la - 212)) & ((1 << (SparkSQLParser.SKEWED - 212)) | (1 << (SparkSQLParser.STORED - 212)) | (1 << (SparkSQLParser.TBLPROPERTIES - 212)))) != 0):
                    self.state = 401
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSQLParser.COMMENT]:
                        self.state = 382
                        self.commentSpec()
                        pass
                    elif token in [SparkSQLParser.PARTITIONED]:
                        self.state = 392
                        self._errHandler.sync(self)
                        la_ = self._interp.adaptivePredict(self._input,15,self._ctx)
                        if la_ == 1:
                            self.state = 383
                            self.match(SparkSQLParser.PARTITIONED)
                            self.state = 384
                            self.match(SparkSQLParser.BY)
                            self.state = 385
                            self.match(SparkSQLParser.T__1)
                            self.state = 386
                            localctx.partitionColumns = self.colTypeList()
                            self.state = 387
                            self.match(SparkSQLParser.T__2)
                            pass

                        elif la_ == 2:
                            self.state = 389
                            self.match(SparkSQLParser.PARTITIONED)
                            self.state = 390
                            self.match(SparkSQLParser.BY)
                            self.state = 391
                            localctx.partitionColumnNames = self.identifierList()
                            pass


                        pass
                    elif token in [SparkSQLParser.CLUSTERED]:
                        self.state = 394
                        self.bucketSpec()
                        pass
                    elif token in [SparkSQLParser.SKEWED]:
                        self.state = 395
                        self.skewSpec()
                        pass
                    elif token in [SparkSQLParser.ROW]:
                        self.state = 396
                        self.rowFormat()
                        pass
                    elif token in [SparkSQLParser.STORED]:
                        self.state = 397
                        self.createFileFormat()
                        pass
                    elif token in [SparkSQLParser.LOCATION]:
                        self.state = 398
                        self.locationSpec()
                        pass
                    elif token in [SparkSQLParser.TBLPROPERTIES]:
                        self.state = 399
                        self.match(SparkSQLParser.TBLPROPERTIES)
                        self.state = 400
                        localctx.tableProps = self.tablePropertyList()
                        pass
                    else:
                        raise NoViableAltException(self)

                    self.state = 405
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 410
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1 or _la==SparkSQLParser.AS or _la==SparkSQLParser.FROM or _la==SparkSQLParser.MAP or ((((_la - 184)) & ~0x3f) == 0 and ((1 << (_la - 184)) & ((1 << (SparkSQLParser.REDUCE - 184)) | (1 << (SparkSQLParser.SELECT - 184)) | (1 << (SparkSQLParser.TABLE - 184)))) != 0) or _la==SparkSQLParser.VALUES or _la==SparkSQLParser.WITH:
                    self.state = 407
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.AS:
                        self.state = 406
                        self.match(SparkSQLParser.AS)


                    self.state = 409
                    self.query()


                pass

            elif la_ == 11:
                localctx = SparkSQLParser.CreateTableLikeContext(self, localctx)
                self.enterOuterAlt(localctx, 11)
                self.state = 412
                self.match(SparkSQLParser.CREATE)
                self.state = 413
                self.match(SparkSQLParser.TABLE)
                self.state = 417
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,20,self._ctx)
                if la_ == 1:
                    self.state = 414
                    self.match(SparkSQLParser.IF)
                    self.state = 415
                    self.match(SparkSQLParser.NOT)
                    self.state = 416
                    self.match(SparkSQLParser.EXISTS)


                self.state = 419
                localctx.target = self.tableIdentifier()
                self.state = 420
                self.match(SparkSQLParser.LIKE)
                self.state = 421
                localctx.source = self.tableIdentifier()
                self.state = 430
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.LOCATION or _la==SparkSQLParser.ROW or ((((_la - 218)) & ~0x3f) == 0 and ((1 << (_la - 218)) & ((1 << (SparkSQLParser.STORED - 218)) | (1 << (SparkSQLParser.TBLPROPERTIES - 218)) | (1 << (SparkSQLParser.USING - 218)))) != 0):
                    self.state = 428
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSQLParser.USING]:
                        self.state = 422
                        self.tableProvider()
                        pass
                    elif token in [SparkSQLParser.ROW]:
                        self.state = 423
                        self.rowFormat()
                        pass
                    elif token in [SparkSQLParser.STORED]:
                        self.state = 424
                        self.createFileFormat()
                        pass
                    elif token in [SparkSQLParser.LOCATION]:
                        self.state = 425
                        self.locationSpec()
                        pass
                    elif token in [SparkSQLParser.TBLPROPERTIES]:
                        self.state = 426
                        self.match(SparkSQLParser.TBLPROPERTIES)
                        self.state = 427
                        localctx.tableProps = self.tablePropertyList()
                        pass
                    else:
                        raise NoViableAltException(self)

                    self.state = 432
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                pass

            elif la_ == 12:
                localctx = SparkSQLParser.ReplaceTableContext(self, localctx)
                self.enterOuterAlt(localctx, 12)
                self.state = 433
                self.replaceTableHeader()
                self.state = 438
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1:
                    self.state = 434
                    self.match(SparkSQLParser.T__1)
                    self.state = 435
                    self.colTypeList()
                    self.state = 436
                    self.match(SparkSQLParser.T__2)


                self.state = 440
                self.tableProvider()
                self.state = 441
                self.createTableClauses()
                self.state = 446
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1 or _la==SparkSQLParser.AS or _la==SparkSQLParser.FROM or _la==SparkSQLParser.MAP or ((((_la - 184)) & ~0x3f) == 0 and ((1 << (_la - 184)) & ((1 << (SparkSQLParser.REDUCE - 184)) | (1 << (SparkSQLParser.SELECT - 184)) | (1 << (SparkSQLParser.TABLE - 184)))) != 0) or _la==SparkSQLParser.VALUES or _la==SparkSQLParser.WITH:
                    self.state = 443
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.AS:
                        self.state = 442
                        self.match(SparkSQLParser.AS)


                    self.state = 445
                    self.query()


                pass

            elif la_ == 13:
                localctx = SparkSQLParser.AnalyzeContext(self, localctx)
                self.enterOuterAlt(localctx, 13)
                self.state = 448
                self.match(SparkSQLParser.ANALYZE)
                self.state = 449
                self.match(SparkSQLParser.TABLE)
                self.state = 450
                self.multipartIdentifier()
                self.state = 452
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 451
                    self.partitionSpec()


                self.state = 454
                self.match(SparkSQLParser.COMPUTE)
                self.state = 455
                self.match(SparkSQLParser.STATISTICS)
                self.state = 463
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,27,self._ctx)
                if la_ == 1:
                    self.state = 456
                    self.identifier()

                elif la_ == 2:
                    self.state = 457
                    self.match(SparkSQLParser.FOR)
                    self.state = 458
                    self.match(SparkSQLParser.COLUMNS)
                    self.state = 459
                    self.identifierSeq()

                elif la_ == 3:
                    self.state = 460
                    self.match(SparkSQLParser.FOR)
                    self.state = 461
                    self.match(SparkSQLParser.ALL)
                    self.state = 462
                    self.match(SparkSQLParser.COLUMNS)


                pass

            elif la_ == 14:
                localctx = SparkSQLParser.AddTableColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 14)
                self.state = 465
                self.match(SparkSQLParser.ALTER)
                self.state = 466
                self.match(SparkSQLParser.TABLE)
                self.state = 467
                self.multipartIdentifier()
                self.state = 468
                self.match(SparkSQLParser.ADD)
                self.state = 469
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.COLUMN or _la==SparkSQLParser.COLUMNS):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 470
                localctx.columns = self.qualifiedColTypeWithPositionList()
                pass

            elif la_ == 15:
                localctx = SparkSQLParser.AddTableColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 15)
                self.state = 472
                self.match(SparkSQLParser.ALTER)
                self.state = 473
                self.match(SparkSQLParser.TABLE)
                self.state = 474
                self.multipartIdentifier()
                self.state = 475
                self.match(SparkSQLParser.ADD)
                self.state = 476
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.COLUMN or _la==SparkSQLParser.COLUMNS):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 477
                self.match(SparkSQLParser.T__1)
                self.state = 478
                localctx.columns = self.qualifiedColTypeWithPositionList()
                self.state = 479
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 16:
                localctx = SparkSQLParser.RenameTableColumnContext(self, localctx)
                self.enterOuterAlt(localctx, 16)
                self.state = 481
                self.match(SparkSQLParser.ALTER)
                self.state = 482
                self.match(SparkSQLParser.TABLE)
                self.state = 483
                localctx.table = self.multipartIdentifier()
                self.state = 484
                self.match(SparkSQLParser.RENAME)
                self.state = 485
                self.match(SparkSQLParser.COLUMN)
                self.state = 486
                localctx.r_from = self.multipartIdentifier()
                self.state = 487
                self.match(SparkSQLParser.TO)
                self.state = 488
                localctx.to = self.errorCapturingIdentifier()
                pass

            elif la_ == 17:
                localctx = SparkSQLParser.DropTableColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 17)
                self.state = 490
                self.match(SparkSQLParser.ALTER)
                self.state = 491
                self.match(SparkSQLParser.TABLE)
                self.state = 492
                self.multipartIdentifier()
                self.state = 493
                self.match(SparkSQLParser.DROP)
                self.state = 494
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.COLUMN or _la==SparkSQLParser.COLUMNS):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 495
                self.match(SparkSQLParser.T__1)
                self.state = 496
                localctx.columns = self.multipartIdentifierList()
                self.state = 497
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 18:
                localctx = SparkSQLParser.DropTableColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 18)
                self.state = 499
                self.match(SparkSQLParser.ALTER)
                self.state = 500
                self.match(SparkSQLParser.TABLE)
                self.state = 501
                self.multipartIdentifier()
                self.state = 502
                self.match(SparkSQLParser.DROP)
                self.state = 503
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.COLUMN or _la==SparkSQLParser.COLUMNS):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 504
                localctx.columns = self.multipartIdentifierList()
                pass

            elif la_ == 19:
                localctx = SparkSQLParser.RenameTableContext(self, localctx)
                self.enterOuterAlt(localctx, 19)
                self.state = 506
                self.match(SparkSQLParser.ALTER)
                self.state = 507
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.TABLE or _la==SparkSQLParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 508
                localctx.r_from = self.multipartIdentifier()
                self.state = 509
                self.match(SparkSQLParser.RENAME)
                self.state = 510
                self.match(SparkSQLParser.TO)
                self.state = 511
                localctx.to = self.multipartIdentifier()
                pass

            elif la_ == 20:
                localctx = SparkSQLParser.SetTablePropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 20)
                self.state = 513
                self.match(SparkSQLParser.ALTER)
                self.state = 514
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.TABLE or _la==SparkSQLParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 515
                self.multipartIdentifier()
                self.state = 516
                self.match(SparkSQLParser.SET)
                self.state = 517
                self.match(SparkSQLParser.TBLPROPERTIES)
                self.state = 518
                self.tablePropertyList()
                pass

            elif la_ == 21:
                localctx = SparkSQLParser.UnsetTablePropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 21)
                self.state = 520
                self.match(SparkSQLParser.ALTER)
                self.state = 521
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.TABLE or _la==SparkSQLParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 522
                self.multipartIdentifier()
                self.state = 523
                self.match(SparkSQLParser.UNSET)
                self.state = 524
                self.match(SparkSQLParser.TBLPROPERTIES)
                self.state = 527
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.IF:
                    self.state = 525
                    self.match(SparkSQLParser.IF)
                    self.state = 526
                    self.match(SparkSQLParser.EXISTS)


                self.state = 529
                self.tablePropertyList()
                pass

            elif la_ == 22:
                localctx = SparkSQLParser.AlterTableAlterColumnContext(self, localctx)
                self.enterOuterAlt(localctx, 22)
                self.state = 531
                self.match(SparkSQLParser.ALTER)
                self.state = 532
                self.match(SparkSQLParser.TABLE)
                self.state = 533
                localctx.table = self.multipartIdentifier()
                self.state = 534
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.ALTER or _la==SparkSQLParser.CHANGE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 536
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,29,self._ctx)
                if la_ == 1:
                    self.state = 535
                    self.match(SparkSQLParser.COLUMN)


                self.state = 538
                localctx.column = self.multipartIdentifier()
                self.state = 540
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.AFTER or _la==SparkSQLParser.COMMENT or _la==SparkSQLParser.DROP or _la==SparkSQLParser.FIRST or _la==SparkSQLParser.SET or _la==SparkSQLParser.TYPE:
                    self.state = 539
                    self.alterColumnAction()


                pass

            elif la_ == 23:
                localctx = SparkSQLParser.HiveChangeColumnContext(self, localctx)
                self.enterOuterAlt(localctx, 23)
                self.state = 542
                self.match(SparkSQLParser.ALTER)
                self.state = 543
                self.match(SparkSQLParser.TABLE)
                self.state = 544
                localctx.table = self.multipartIdentifier()
                self.state = 546
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 545
                    self.partitionSpec()


                self.state = 548
                self.match(SparkSQLParser.CHANGE)
                self.state = 550
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,32,self._ctx)
                if la_ == 1:
                    self.state = 549
                    self.match(SparkSQLParser.COLUMN)


                self.state = 552
                localctx.colName = self.multipartIdentifier()
                self.state = 553
                self.colType()
                self.state = 555
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.AFTER or _la==SparkSQLParser.FIRST:
                    self.state = 554
                    self.colPosition()


                pass

            elif la_ == 24:
                localctx = SparkSQLParser.HiveReplaceColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 24)
                self.state = 557
                self.match(SparkSQLParser.ALTER)
                self.state = 558
                self.match(SparkSQLParser.TABLE)
                self.state = 559
                localctx.table = self.multipartIdentifier()
                self.state = 561
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 560
                    self.partitionSpec()


                self.state = 563
                self.match(SparkSQLParser.REPLACE)
                self.state = 564
                self.match(SparkSQLParser.COLUMNS)
                self.state = 565
                self.match(SparkSQLParser.T__1)
                self.state = 566
                localctx.columns = self.qualifiedColTypeWithPositionList()
                self.state = 567
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 25:
                localctx = SparkSQLParser.SetTableSerDeContext(self, localctx)
                self.enterOuterAlt(localctx, 25)
                self.state = 569
                self.match(SparkSQLParser.ALTER)
                self.state = 570
                self.match(SparkSQLParser.TABLE)
                self.state = 571
                self.multipartIdentifier()
                self.state = 573
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 572
                    self.partitionSpec()


                self.state = 575
                self.match(SparkSQLParser.SET)
                self.state = 576
                self.match(SparkSQLParser.SERDE)
                self.state = 577
                self.match(SparkSQLParser.STRING)
                self.state = 581
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.WITH:
                    self.state = 578
                    self.match(SparkSQLParser.WITH)
                    self.state = 579
                    self.match(SparkSQLParser.SERDEPROPERTIES)
                    self.state = 580
                    self.tablePropertyList()


                pass

            elif la_ == 26:
                localctx = SparkSQLParser.SetTableSerDeContext(self, localctx)
                self.enterOuterAlt(localctx, 26)
                self.state = 583
                self.match(SparkSQLParser.ALTER)
                self.state = 584
                self.match(SparkSQLParser.TABLE)
                self.state = 585
                self.multipartIdentifier()
                self.state = 587
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 586
                    self.partitionSpec()


                self.state = 589
                self.match(SparkSQLParser.SET)
                self.state = 590
                self.match(SparkSQLParser.SERDEPROPERTIES)
                self.state = 591
                self.tablePropertyList()
                pass

            elif la_ == 27:
                localctx = SparkSQLParser.AddTablePartitionContext(self, localctx)
                self.enterOuterAlt(localctx, 27)
                self.state = 593
                self.match(SparkSQLParser.ALTER)
                self.state = 594
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.TABLE or _la==SparkSQLParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 595
                self.multipartIdentifier()
                self.state = 596
                self.match(SparkSQLParser.ADD)
                self.state = 600
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.IF:
                    self.state = 597
                    self.match(SparkSQLParser.IF)
                    self.state = 598
                    self.match(SparkSQLParser.NOT)
                    self.state = 599
                    self.match(SparkSQLParser.EXISTS)


                self.state = 603 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 602
                    self.partitionSpecLocation()
                    self.state = 605 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSQLParser.PARTITION):
                        break

                pass

            elif la_ == 28:
                localctx = SparkSQLParser.RenameTablePartitionContext(self, localctx)
                self.enterOuterAlt(localctx, 28)
                self.state = 607
                self.match(SparkSQLParser.ALTER)
                self.state = 608
                self.match(SparkSQLParser.TABLE)
                self.state = 609
                self.multipartIdentifier()
                self.state = 610
                localctx.r_from = self.partitionSpec()
                self.state = 611
                self.match(SparkSQLParser.RENAME)
                self.state = 612
                self.match(SparkSQLParser.TO)
                self.state = 613
                localctx.to = self.partitionSpec()
                pass

            elif la_ == 29:
                localctx = SparkSQLParser.DropTablePartitionsContext(self, localctx)
                self.enterOuterAlt(localctx, 29)
                self.state = 615
                self.match(SparkSQLParser.ALTER)
                self.state = 616
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.TABLE or _la==SparkSQLParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 617
                self.multipartIdentifier()
                self.state = 618
                self.match(SparkSQLParser.DROP)
                self.state = 621
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.IF:
                    self.state = 619
                    self.match(SparkSQLParser.IF)
                    self.state = 620
                    self.match(SparkSQLParser.EXISTS)


                self.state = 623
                self.partitionSpec()
                self.state = 628
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 624
                    self.match(SparkSQLParser.T__3)
                    self.state = 625
                    self.partitionSpec()
                    self.state = 630
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 632
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PURGE:
                    self.state = 631
                    self.match(SparkSQLParser.PURGE)


                pass

            elif la_ == 30:
                localctx = SparkSQLParser.SetTableLocationContext(self, localctx)
                self.enterOuterAlt(localctx, 30)
                self.state = 634
                self.match(SparkSQLParser.ALTER)
                self.state = 635
                self.match(SparkSQLParser.TABLE)
                self.state = 636
                self.multipartIdentifier()
                self.state = 638
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 637
                    self.partitionSpec()


                self.state = 640
                self.match(SparkSQLParser.SET)
                self.state = 641
                self.locationSpec()
                pass

            elif la_ == 31:
                localctx = SparkSQLParser.RecoverPartitionsContext(self, localctx)
                self.enterOuterAlt(localctx, 31)
                self.state = 643
                self.match(SparkSQLParser.ALTER)
                self.state = 644
                self.match(SparkSQLParser.TABLE)
                self.state = 645
                self.multipartIdentifier()
                self.state = 646
                self.match(SparkSQLParser.RECOVER)
                self.state = 647
                self.match(SparkSQLParser.PARTITIONS)
                pass

            elif la_ == 32:
                localctx = SparkSQLParser.DropTableContext(self, localctx)
                self.enterOuterAlt(localctx, 32)
                self.state = 649
                self.match(SparkSQLParser.DROP)
                self.state = 650
                self.match(SparkSQLParser.TABLE)
                self.state = 653
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,44,self._ctx)
                if la_ == 1:
                    self.state = 651
                    self.match(SparkSQLParser.IF)
                    self.state = 652
                    self.match(SparkSQLParser.EXISTS)


                self.state = 655
                self.multipartIdentifier()
                self.state = 657
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PURGE:
                    self.state = 656
                    self.match(SparkSQLParser.PURGE)


                pass

            elif la_ == 33:
                localctx = SparkSQLParser.DropViewContext(self, localctx)
                self.enterOuterAlt(localctx, 33)
                self.state = 659
                self.match(SparkSQLParser.DROP)
                self.state = 660
                self.match(SparkSQLParser.VIEW)
                self.state = 663
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,46,self._ctx)
                if la_ == 1:
                    self.state = 661
                    self.match(SparkSQLParser.IF)
                    self.state = 662
                    self.match(SparkSQLParser.EXISTS)


                self.state = 665
                self.multipartIdentifier()
                pass

            elif la_ == 34:
                localctx = SparkSQLParser.CreateViewContext(self, localctx)
                self.enterOuterAlt(localctx, 34)
                self.state = 666
                self.match(SparkSQLParser.CREATE)
                self.state = 669
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OR:
                    self.state = 667
                    self.match(SparkSQLParser.OR)
                    self.state = 668
                    self.match(SparkSQLParser.REPLACE)


                self.state = 675
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.GLOBAL or _la==SparkSQLParser.TEMPORARY:
                    self.state = 672
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.GLOBAL:
                        self.state = 671
                        self.match(SparkSQLParser.GLOBAL)


                    self.state = 674
                    self.match(SparkSQLParser.TEMPORARY)


                self.state = 677
                self.match(SparkSQLParser.VIEW)
                self.state = 681
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,50,self._ctx)
                if la_ == 1:
                    self.state = 678
                    self.match(SparkSQLParser.IF)
                    self.state = 679
                    self.match(SparkSQLParser.NOT)
                    self.state = 680
                    self.match(SparkSQLParser.EXISTS)


                self.state = 683
                self.multipartIdentifier()
                self.state = 685
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1:
                    self.state = 684
                    self.identifierCommentList()


                self.state = 695
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.COMMENT or _la==SparkSQLParser.PARTITIONED or _la==SparkSQLParser.TBLPROPERTIES:
                    self.state = 693
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSQLParser.COMMENT]:
                        self.state = 687
                        self.commentSpec()
                        pass
                    elif token in [SparkSQLParser.PARTITIONED]:
                        self.state = 688
                        self.match(SparkSQLParser.PARTITIONED)
                        self.state = 689
                        self.match(SparkSQLParser.ON)
                        self.state = 690
                        self.identifierList()
                        pass
                    elif token in [SparkSQLParser.TBLPROPERTIES]:
                        self.state = 691
                        self.match(SparkSQLParser.TBLPROPERTIES)
                        self.state = 692
                        self.tablePropertyList()
                        pass
                    else:
                        raise NoViableAltException(self)

                    self.state = 697
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 698
                self.match(SparkSQLParser.AS)
                self.state = 699
                self.query()
                pass

            elif la_ == 35:
                localctx = SparkSQLParser.CreateTempViewUsingContext(self, localctx)
                self.enterOuterAlt(localctx, 35)
                self.state = 701
                self.match(SparkSQLParser.CREATE)
                self.state = 704
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OR:
                    self.state = 702
                    self.match(SparkSQLParser.OR)
                    self.state = 703
                    self.match(SparkSQLParser.REPLACE)


                self.state = 707
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.GLOBAL:
                    self.state = 706
                    self.match(SparkSQLParser.GLOBAL)


                self.state = 709
                self.match(SparkSQLParser.TEMPORARY)
                self.state = 710
                self.match(SparkSQLParser.VIEW)
                self.state = 711
                self.tableIdentifier()
                self.state = 716
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1:
                    self.state = 712
                    self.match(SparkSQLParser.T__1)
                    self.state = 713
                    self.colTypeList()
                    self.state = 714
                    self.match(SparkSQLParser.T__2)


                self.state = 718
                self.tableProvider()
                self.state = 721
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OPTIONS:
                    self.state = 719
                    self.match(SparkSQLParser.OPTIONS)
                    self.state = 720
                    self.tablePropertyList()


                pass

            elif la_ == 36:
                localctx = SparkSQLParser.AlterViewQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 36)
                self.state = 723
                self.match(SparkSQLParser.ALTER)
                self.state = 724
                self.match(SparkSQLParser.VIEW)
                self.state = 725
                self.multipartIdentifier()
                self.state = 727
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.AS:
                    self.state = 726
                    self.match(SparkSQLParser.AS)


                self.state = 729
                self.query()
                pass

            elif la_ == 37:
                localctx = SparkSQLParser.CreateFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 37)
                self.state = 731
                self.match(SparkSQLParser.CREATE)
                self.state = 734
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OR:
                    self.state = 732
                    self.match(SparkSQLParser.OR)
                    self.state = 733
                    self.match(SparkSQLParser.REPLACE)


                self.state = 737
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.TEMPORARY:
                    self.state = 736
                    self.match(SparkSQLParser.TEMPORARY)


                self.state = 739
                self.match(SparkSQLParser.FUNCTION)
                self.state = 743
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,61,self._ctx)
                if la_ == 1:
                    self.state = 740
                    self.match(SparkSQLParser.IF)
                    self.state = 741
                    self.match(SparkSQLParser.NOT)
                    self.state = 742
                    self.match(SparkSQLParser.EXISTS)


                self.state = 745
                self.multipartIdentifier()
                self.state = 746
                self.match(SparkSQLParser.AS)
                self.state = 747
                localctx.className = self.match(SparkSQLParser.STRING)
                self.state = 757
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.USING:
                    self.state = 748
                    self.match(SparkSQLParser.USING)
                    self.state = 749
                    self.resource()
                    self.state = 754
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 750
                        self.match(SparkSQLParser.T__3)
                        self.state = 751
                        self.resource()
                        self.state = 756
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                pass

            elif la_ == 38:
                localctx = SparkSQLParser.DropFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 38)
                self.state = 759
                self.match(SparkSQLParser.DROP)
                self.state = 761
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.TEMPORARY:
                    self.state = 760
                    self.match(SparkSQLParser.TEMPORARY)


                self.state = 763
                self.match(SparkSQLParser.FUNCTION)
                self.state = 766
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,65,self._ctx)
                if la_ == 1:
                    self.state = 764
                    self.match(SparkSQLParser.IF)
                    self.state = 765
                    self.match(SparkSQLParser.EXISTS)


                self.state = 768
                self.multipartIdentifier()
                pass

            elif la_ == 39:
                localctx = SparkSQLParser.ExplainContext(self, localctx)
                self.enterOuterAlt(localctx, 39)
                self.state = 769
                self.match(SparkSQLParser.EXPLAIN)
                self.state = 771
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.CODEGEN or _la==SparkSQLParser.COST or ((((_la - 86)) & ~0x3f) == 0 and ((1 << (_la - 86)) & ((1 << (SparkSQLParser.EXTENDED - 86)) | (1 << (SparkSQLParser.FORMATTED - 86)) | (1 << (SparkSQLParser.LOGICAL - 86)))) != 0):
                    self.state = 770
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.CODEGEN or _la==SparkSQLParser.COST or ((((_la - 86)) & ~0x3f) == 0 and ((1 << (_la - 86)) & ((1 << (SparkSQLParser.EXTENDED - 86)) | (1 << (SparkSQLParser.FORMATTED - 86)) | (1 << (SparkSQLParser.LOGICAL - 86)))) != 0)):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                self.state = 773
                self.statement()
                pass

            elif la_ == 40:
                localctx = SparkSQLParser.ShowTablesContext(self, localctx)
                self.enterOuterAlt(localctx, 40)
                self.state = 774
                self.match(SparkSQLParser.SHOW)
                self.state = 775
                self.match(SparkSQLParser.TABLES)
                self.state = 778
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.FROM or _la==SparkSQLParser.IN:
                    self.state = 776
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.FROM or _la==SparkSQLParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 777
                    self.multipartIdentifier()


                self.state = 784
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LIKE or _la==SparkSQLParser.STRING:
                    self.state = 781
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.LIKE:
                        self.state = 780
                        self.match(SparkSQLParser.LIKE)


                    self.state = 783
                    localctx.pattern = self.match(SparkSQLParser.STRING)


                pass

            elif la_ == 41:
                localctx = SparkSQLParser.ShowTableContext(self, localctx)
                self.enterOuterAlt(localctx, 41)
                self.state = 786
                self.match(SparkSQLParser.SHOW)
                self.state = 787
                self.match(SparkSQLParser.TABLE)
                self.state = 788
                self.match(SparkSQLParser.EXTENDED)
                self.state = 791
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.FROM or _la==SparkSQLParser.IN:
                    self.state = 789
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.FROM or _la==SparkSQLParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 790
                    localctx.ns = self.multipartIdentifier()


                self.state = 793
                self.match(SparkSQLParser.LIKE)
                self.state = 794
                localctx.pattern = self.match(SparkSQLParser.STRING)
                self.state = 796
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 795
                    self.partitionSpec()


                pass

            elif la_ == 42:
                localctx = SparkSQLParser.ShowTblPropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 42)
                self.state = 798
                self.match(SparkSQLParser.SHOW)
                self.state = 799
                self.match(SparkSQLParser.TBLPROPERTIES)
                self.state = 800
                localctx.table = self.multipartIdentifier()
                self.state = 805
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1:
                    self.state = 801
                    self.match(SparkSQLParser.T__1)
                    self.state = 802
                    localctx.key = self.tablePropertyKey()
                    self.state = 803
                    self.match(SparkSQLParser.T__2)


                pass

            elif la_ == 43:
                localctx = SparkSQLParser.ShowColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 43)
                self.state = 807
                self.match(SparkSQLParser.SHOW)
                self.state = 808
                self.match(SparkSQLParser.COLUMNS)
                self.state = 809
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.FROM or _la==SparkSQLParser.IN):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 810
                localctx.table = self.multipartIdentifier()
                self.state = 813
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.FROM or _la==SparkSQLParser.IN:
                    self.state = 811
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.FROM or _la==SparkSQLParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 812
                    localctx.ns = self.multipartIdentifier()


                pass

            elif la_ == 44:
                localctx = SparkSQLParser.ShowViewsContext(self, localctx)
                self.enterOuterAlt(localctx, 44)
                self.state = 815
                self.match(SparkSQLParser.SHOW)
                self.state = 816
                self.match(SparkSQLParser.VIEWS)
                self.state = 819
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.FROM or _la==SparkSQLParser.IN:
                    self.state = 817
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.FROM or _la==SparkSQLParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 818
                    self.multipartIdentifier()


                self.state = 825
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LIKE or _la==SparkSQLParser.STRING:
                    self.state = 822
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.LIKE:
                        self.state = 821
                        self.match(SparkSQLParser.LIKE)


                    self.state = 824
                    localctx.pattern = self.match(SparkSQLParser.STRING)


                pass

            elif la_ == 45:
                localctx = SparkSQLParser.ShowPartitionsContext(self, localctx)
                self.enterOuterAlt(localctx, 45)
                self.state = 827
                self.match(SparkSQLParser.SHOW)
                self.state = 828
                self.match(SparkSQLParser.PARTITIONS)
                self.state = 829
                self.multipartIdentifier()
                self.state = 831
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 830
                    self.partitionSpec()


                pass

            elif la_ == 46:
                localctx = SparkSQLParser.ShowFunctionsContext(self, localctx)
                self.enterOuterAlt(localctx, 46)
                self.state = 833
                self.match(SparkSQLParser.SHOW)
                self.state = 835
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,78,self._ctx)
                if la_ == 1:
                    self.state = 834
                    self.identifier()


                self.state = 837
                self.match(SparkSQLParser.FUNCTIONS)
                self.state = 845
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.STRING - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                    self.state = 839
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,79,self._ctx)
                    if la_ == 1:
                        self.state = 838
                        self.match(SparkSQLParser.LIKE)


                    self.state = 843
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSQLParser.ADD, SparkSQLParser.AFTER, SparkSQLParser.ALL, SparkSQLParser.ALTER, SparkSQLParser.ANALYZE, SparkSQLParser.AND, SparkSQLParser.ANTI, SparkSQLParser.ANY, SparkSQLParser.ARCHIVE, SparkSQLParser.ARRAY, SparkSQLParser.AS, SparkSQLParser.ASC, SparkSQLParser.AT, SparkSQLParser.AUTHORIZATION, SparkSQLParser.BETWEEN, SparkSQLParser.BOTH, SparkSQLParser.BUCKET, SparkSQLParser.BUCKETS, SparkSQLParser.BY, SparkSQLParser.CACHE, SparkSQLParser.CASCADE, SparkSQLParser.CASE, SparkSQLParser.CAST, SparkSQLParser.CHANGE, SparkSQLParser.CHECK, SparkSQLParser.CLEAR, SparkSQLParser.CLUSTER, SparkSQLParser.CLUSTERED, SparkSQLParser.CODEGEN, SparkSQLParser.COLLATE, SparkSQLParser.COLLECTION, SparkSQLParser.COLUMN, SparkSQLParser.COLUMNS, SparkSQLParser.COMMENT, SparkSQLParser.COMMIT, SparkSQLParser.COMPACT, SparkSQLParser.COMPACTIONS, SparkSQLParser.COMPUTE, SparkSQLParser.CONCATENATE, SparkSQLParser.CONSTRAINT, SparkSQLParser.COST, SparkSQLParser.CREATE, SparkSQLParser.CROSS, SparkSQLParser.CUBE, SparkSQLParser.CURRENT, SparkSQLParser.CURRENT_DATE, SparkSQLParser.CURRENT_TIME, SparkSQLParser.CURRENT_TIMESTAMP, SparkSQLParser.CURRENT_USER, SparkSQLParser.DATA, SparkSQLParser.DATABASE, SparkSQLParser.DATABASES, SparkSQLParser.DBPROPERTIES, SparkSQLParser.DEFINED, SparkSQLParser.DELETE, SparkSQLParser.DELIMITED, SparkSQLParser.DESC, SparkSQLParser.DESCRIBE, SparkSQLParser.DFS, SparkSQLParser.DIRECTORIES, SparkSQLParser.DIRECTORY, SparkSQLParser.DISTINCT, SparkSQLParser.DISTRIBUTE, SparkSQLParser.DIV, SparkSQLParser.DROP, SparkSQLParser.ELSE, SparkSQLParser.END, SparkSQLParser.ESCAPE, SparkSQLParser.ESCAPED, SparkSQLParser.EXCEPT, SparkSQLParser.EXCHANGE, SparkSQLParser.EXISTS, SparkSQLParser.EXPLAIN, SparkSQLParser.EXPORT, SparkSQLParser.EXTENDED, SparkSQLParser.EXTERNAL, SparkSQLParser.EXTRACT, SparkSQLParser.FALSE, SparkSQLParser.FETCH, SparkSQLParser.FIELDS, SparkSQLParser.FILTER, SparkSQLParser.FILEFORMAT, SparkSQLParser.FIRST, SparkSQLParser.FOLLOWING, SparkSQLParser.FOR, SparkSQLParser.FOREIGN, SparkSQLParser.FORMAT, SparkSQLParser.FORMATTED, SparkSQLParser.FROM, SparkSQLParser.FULL, SparkSQLParser.FUNCTION, SparkSQLParser.FUNCTIONS, SparkSQLParser.GLOBAL, SparkSQLParser.GRANT, SparkSQLParser.GROUP, SparkSQLParser.GROUPING, SparkSQLParser.HAVING, SparkSQLParser.IF, SparkSQLParser.IGNORE, SparkSQLParser.IMPORT, SparkSQLParser.IN, SparkSQLParser.INDEX, SparkSQLParser.INDEXES, SparkSQLParser.INNER, SparkSQLParser.INPATH, SparkSQLParser.INPUTFORMAT, SparkSQLParser.INSERT, SparkSQLParser.INTERSECT, SparkSQLParser.INTERVAL, SparkSQLParser.INTO, SparkSQLParser.IS, SparkSQLParser.ITEMS, SparkSQLParser.JOIN, SparkSQLParser.KEYS, SparkSQLParser.LAST, SparkSQLParser.LATERAL, SparkSQLParser.LAZY, SparkSQLParser.LEADING, SparkSQLParser.LEFT, SparkSQLParser.LIKE, SparkSQLParser.LIMIT, SparkSQLParser.LINES, SparkSQLParser.LIST, SparkSQLParser.LOAD, SparkSQLParser.LOCAL, SparkSQLParser.LOCATION, SparkSQLParser.LOCK, SparkSQLParser.LOCKS, SparkSQLParser.LOGICAL, SparkSQLParser.MACRO, SparkSQLParser.MAP, SparkSQLParser.MATCHED, SparkSQLParser.MERGE, SparkSQLParser.MSCK, SparkSQLParser.NAMESPACE, SparkSQLParser.NAMESPACES, SparkSQLParser.NATURAL, SparkSQLParser.NO, SparkSQLParser.NOT, SparkSQLParser.NULL, SparkSQLParser.NULLS, SparkSQLParser.OF, SparkSQLParser.ON, SparkSQLParser.ONLY, SparkSQLParser.OPTION, SparkSQLParser.OPTIONS, SparkSQLParser.OR, SparkSQLParser.ORDER, SparkSQLParser.OUT, SparkSQLParser.OUTER, SparkSQLParser.OUTPUTFORMAT, SparkSQLParser.OVER, SparkSQLParser.OVERLAPS, SparkSQLParser.OVERLAY, SparkSQLParser.OVERWRITE, SparkSQLParser.PARTITION, SparkSQLParser.PARTITIONED, SparkSQLParser.PARTITIONS, SparkSQLParser.PERCENTLIT, SparkSQLParser.PIVOT, SparkSQLParser.PLACING, SparkSQLParser.POSITION, SparkSQLParser.PRECEDING, SparkSQLParser.PRIMARY, SparkSQLParser.PRINCIPALS, SparkSQLParser.PROPERTIES, SparkSQLParser.PURGE, SparkSQLParser.QUERY, SparkSQLParser.RANGE, SparkSQLParser.RECORDREADER, SparkSQLParser.RECORDWRITER, SparkSQLParser.RECOVER, SparkSQLParser.REDUCE, SparkSQLParser.REFERENCES, SparkSQLParser.REFRESH, SparkSQLParser.RENAME, SparkSQLParser.REPAIR, SparkSQLParser.REPLACE, SparkSQLParser.RESET, SparkSQLParser.RESTRICT, SparkSQLParser.REVOKE, SparkSQLParser.RIGHT, SparkSQLParser.RLIKE, SparkSQLParser.ROLE, SparkSQLParser.ROLES, SparkSQLParser.ROLLBACK, SparkSQLParser.ROLLUP, SparkSQLParser.ROW, SparkSQLParser.ROWS, SparkSQLParser.SCHEMA, SparkSQLParser.SELECT, SparkSQLParser.SEMI, SparkSQLParser.SEPARATED, SparkSQLParser.SERDE, SparkSQLParser.SERDEPROPERTIES, SparkSQLParser.SESSION_USER, SparkSQLParser.SET, SparkSQLParser.SETMINUS, SparkSQLParser.SETS, SparkSQLParser.SHOW, SparkSQLParser.SKEWED, SparkSQLParser.SOME, SparkSQLParser.SORT, SparkSQLParser.SORTED, SparkSQLParser.START, SparkSQLParser.STATISTICS, SparkSQLParser.STORED, SparkSQLParser.STRATIFY, SparkSQLParser.STRUCT, SparkSQLParser.SUBSTR, SparkSQLParser.SUBSTRING, SparkSQLParser.TABLE, SparkSQLParser.TABLES, SparkSQLParser.TABLESAMPLE, SparkSQLParser.TBLPROPERTIES, SparkSQLParser.TEMPORARY, SparkSQLParser.TERMINATED, SparkSQLParser.THEN, SparkSQLParser.TIME, SparkSQLParser.TO, SparkSQLParser.TOUCH, SparkSQLParser.TRAILING, SparkSQLParser.TRANSACTION, SparkSQLParser.TRANSACTIONS, SparkSQLParser.TRANSFORM, SparkSQLParser.TRIM, SparkSQLParser.TRUE, SparkSQLParser.TRUNCATE, SparkSQLParser.TYPE, SparkSQLParser.UNARCHIVE, SparkSQLParser.UNBOUNDED, SparkSQLParser.UNCACHE, SparkSQLParser.UNION, SparkSQLParser.UNIQUE, SparkSQLParser.UNKNOWN, SparkSQLParser.UNLOCK, SparkSQLParser.UNSET, SparkSQLParser.UPDATE, SparkSQLParser.USE, SparkSQLParser.USER, SparkSQLParser.USING, SparkSQLParser.VALUES, SparkSQLParser.VIEW, SparkSQLParser.VIEWS, SparkSQLParser.WHEN, SparkSQLParser.WHERE, SparkSQLParser.WINDOW, SparkSQLParser.WITH, SparkSQLParser.ZONE, SparkSQLParser.IDENTIFIER, SparkSQLParser.BACKQUOTED_IDENTIFIER]:
                        self.state = 841
                        self.multipartIdentifier()
                        pass
                    elif token in [SparkSQLParser.STRING]:
                        self.state = 842
                        localctx.pattern = self.match(SparkSQLParser.STRING)
                        pass
                    else:
                        raise NoViableAltException(self)



                pass

            elif la_ == 47:
                localctx = SparkSQLParser.ShowCreateTableContext(self, localctx)
                self.enterOuterAlt(localctx, 47)
                self.state = 847
                self.match(SparkSQLParser.SHOW)
                self.state = 848
                self.match(SparkSQLParser.CREATE)
                self.state = 849
                self.match(SparkSQLParser.TABLE)
                self.state = 850
                self.multipartIdentifier()
                self.state = 853
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.AS:
                    self.state = 851
                    self.match(SparkSQLParser.AS)
                    self.state = 852
                    self.match(SparkSQLParser.SERDE)


                pass

            elif la_ == 48:
                localctx = SparkSQLParser.ShowCurrentNamespaceContext(self, localctx)
                self.enterOuterAlt(localctx, 48)
                self.state = 855
                self.match(SparkSQLParser.SHOW)
                self.state = 856
                self.match(SparkSQLParser.CURRENT)
                self.state = 857
                self.match(SparkSQLParser.NAMESPACE)
                pass

            elif la_ == 49:
                localctx = SparkSQLParser.DescribeFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 49)
                self.state = 858
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.DESC or _la==SparkSQLParser.DESCRIBE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 859
                self.match(SparkSQLParser.FUNCTION)
                self.state = 861
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,83,self._ctx)
                if la_ == 1:
                    self.state = 860
                    self.match(SparkSQLParser.EXTENDED)


                self.state = 863
                self.describeFuncName()
                pass

            elif la_ == 50:
                localctx = SparkSQLParser.DescribeNamespaceContext(self, localctx)
                self.enterOuterAlt(localctx, 50)
                self.state = 864
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.DESC or _la==SparkSQLParser.DESCRIBE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 865
                self.namespace()
                self.state = 867
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,84,self._ctx)
                if la_ == 1:
                    self.state = 866
                    self.match(SparkSQLParser.EXTENDED)


                self.state = 869
                self.multipartIdentifier()
                pass

            elif la_ == 51:
                localctx = SparkSQLParser.DescribeRelationContext(self, localctx)
                self.enterOuterAlt(localctx, 51)
                self.state = 871
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.DESC or _la==SparkSQLParser.DESCRIBE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 873
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,85,self._ctx)
                if la_ == 1:
                    self.state = 872
                    self.match(SparkSQLParser.TABLE)


                self.state = 876
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,86,self._ctx)
                if la_ == 1:
                    self.state = 875
                    localctx.option = self._input.LT(1)
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.EXTENDED or _la==SparkSQLParser.FORMATTED):
                        localctx.option = self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                self.state = 878
                self.multipartIdentifier()
                self.state = 880
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,87,self._ctx)
                if la_ == 1:
                    self.state = 879
                    self.partitionSpec()


                self.state = 883
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                    self.state = 882
                    self.describeColName()


                pass

            elif la_ == 52:
                localctx = SparkSQLParser.DescribeQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 52)
                self.state = 885
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.DESC or _la==SparkSQLParser.DESCRIBE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 887
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.QUERY:
                    self.state = 886
                    self.match(SparkSQLParser.QUERY)


                self.state = 889
                self.query()
                pass

            elif la_ == 53:
                localctx = SparkSQLParser.CommentNamespaceContext(self, localctx)
                self.enterOuterAlt(localctx, 53)
                self.state = 890
                self.match(SparkSQLParser.COMMENT)
                self.state = 891
                self.match(SparkSQLParser.ON)
                self.state = 892
                self.namespace()
                self.state = 893
                self.multipartIdentifier()
                self.state = 894
                self.match(SparkSQLParser.IS)
                self.state = 895
                localctx.comment = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.NULL or _la==SparkSQLParser.STRING):
                    localctx.comment = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass

            elif la_ == 54:
                localctx = SparkSQLParser.CommentTableContext(self, localctx)
                self.enterOuterAlt(localctx, 54)
                self.state = 897
                self.match(SparkSQLParser.COMMENT)
                self.state = 898
                self.match(SparkSQLParser.ON)
                self.state = 899
                self.match(SparkSQLParser.TABLE)
                self.state = 900
                self.multipartIdentifier()
                self.state = 901
                self.match(SparkSQLParser.IS)
                self.state = 902
                localctx.comment = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.NULL or _la==SparkSQLParser.STRING):
                    localctx.comment = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass

            elif la_ == 55:
                localctx = SparkSQLParser.RefreshTableContext(self, localctx)
                self.enterOuterAlt(localctx, 55)
                self.state = 904
                self.match(SparkSQLParser.REFRESH)
                self.state = 905
                self.match(SparkSQLParser.TABLE)
                self.state = 906
                self.multipartIdentifier()
                pass

            elif la_ == 56:
                localctx = SparkSQLParser.RefreshFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 56)
                self.state = 907
                self.match(SparkSQLParser.REFRESH)
                self.state = 908
                self.match(SparkSQLParser.FUNCTION)
                self.state = 909
                self.multipartIdentifier()
                pass

            elif la_ == 57:
                localctx = SparkSQLParser.RefreshResourceContext(self, localctx)
                self.enterOuterAlt(localctx, 57)
                self.state = 910
                self.match(SparkSQLParser.REFRESH)
                self.state = 918
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,91,self._ctx)
                if la_ == 1:
                    self.state = 911
                    self.match(SparkSQLParser.STRING)
                    pass

                elif la_ == 2:
                    self.state = 915
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,90,self._ctx)
                    while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                        if _alt==1+1:
                            self.state = 912
                            self.matchWildcard() 
                        self.state = 917
                        self._errHandler.sync(self)
                        _alt = self._interp.adaptivePredict(self._input,90,self._ctx)

                    pass


                pass

            elif la_ == 58:
                localctx = SparkSQLParser.CacheTableContext(self, localctx)
                self.enterOuterAlt(localctx, 58)
                self.state = 920
                self.match(SparkSQLParser.CACHE)
                self.state = 922
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LAZY:
                    self.state = 921
                    self.match(SparkSQLParser.LAZY)


                self.state = 924
                self.match(SparkSQLParser.TABLE)
                self.state = 925
                self.multipartIdentifier()
                self.state = 928
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OPTIONS:
                    self.state = 926
                    self.match(SparkSQLParser.OPTIONS)
                    self.state = 927
                    localctx.options = self.tablePropertyList()


                self.state = 934
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__1 or _la==SparkSQLParser.AS or _la==SparkSQLParser.FROM or _la==SparkSQLParser.MAP or ((((_la - 184)) & ~0x3f) == 0 and ((1 << (_la - 184)) & ((1 << (SparkSQLParser.REDUCE - 184)) | (1 << (SparkSQLParser.SELECT - 184)) | (1 << (SparkSQLParser.TABLE - 184)))) != 0) or _la==SparkSQLParser.VALUES or _la==SparkSQLParser.WITH:
                    self.state = 931
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.AS:
                        self.state = 930
                        self.match(SparkSQLParser.AS)


                    self.state = 933
                    self.query()


                pass

            elif la_ == 59:
                localctx = SparkSQLParser.UncacheTableContext(self, localctx)
                self.enterOuterAlt(localctx, 59)
                self.state = 936
                self.match(SparkSQLParser.UNCACHE)
                self.state = 937
                self.match(SparkSQLParser.TABLE)
                self.state = 940
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,96,self._ctx)
                if la_ == 1:
                    self.state = 938
                    self.match(SparkSQLParser.IF)
                    self.state = 939
                    self.match(SparkSQLParser.EXISTS)


                self.state = 942
                self.multipartIdentifier()
                pass

            elif la_ == 60:
                localctx = SparkSQLParser.ClearCacheContext(self, localctx)
                self.enterOuterAlt(localctx, 60)
                self.state = 943
                self.match(SparkSQLParser.CLEAR)
                self.state = 944
                self.match(SparkSQLParser.CACHE)
                pass

            elif la_ == 61:
                localctx = SparkSQLParser.LoadDataContext(self, localctx)
                self.enterOuterAlt(localctx, 61)
                self.state = 945
                self.match(SparkSQLParser.LOAD)
                self.state = 946
                self.match(SparkSQLParser.DATA)
                self.state = 948
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LOCAL:
                    self.state = 947
                    self.match(SparkSQLParser.LOCAL)


                self.state = 950
                self.match(SparkSQLParser.INPATH)
                self.state = 951
                localctx.path = self.match(SparkSQLParser.STRING)
                self.state = 953
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OVERWRITE:
                    self.state = 952
                    self.match(SparkSQLParser.OVERWRITE)


                self.state = 955
                self.match(SparkSQLParser.INTO)
                self.state = 956
                self.match(SparkSQLParser.TABLE)
                self.state = 957
                self.multipartIdentifier()
                self.state = 959
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 958
                    self.partitionSpec()


                pass

            elif la_ == 62:
                localctx = SparkSQLParser.TruncateTableContext(self, localctx)
                self.enterOuterAlt(localctx, 62)
                self.state = 961
                self.match(SparkSQLParser.TRUNCATE)
                self.state = 962
                self.match(SparkSQLParser.TABLE)
                self.state = 963
                self.multipartIdentifier()
                self.state = 965
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 964
                    self.partitionSpec()


                pass

            elif la_ == 63:
                localctx = SparkSQLParser.RepairTableContext(self, localctx)
                self.enterOuterAlt(localctx, 63)
                self.state = 967
                self.match(SparkSQLParser.MSCK)
                self.state = 968
                self.match(SparkSQLParser.REPAIR)
                self.state = 969
                self.match(SparkSQLParser.TABLE)
                self.state = 970
                self.multipartIdentifier()
                pass

            elif la_ == 64:
                localctx = SparkSQLParser.ManageResourceContext(self, localctx)
                self.enterOuterAlt(localctx, 64)
                self.state = 971
                localctx.op = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.ADD or _la==SparkSQLParser.LIST):
                    localctx.op = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 972
                self.identifier()
                self.state = 980
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,102,self._ctx)
                if la_ == 1:
                    self.state = 973
                    self.match(SparkSQLParser.STRING)
                    pass

                elif la_ == 2:
                    self.state = 977
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,101,self._ctx)
                    while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                        if _alt==1+1:
                            self.state = 974
                            self.matchWildcard() 
                        self.state = 979
                        self._errHandler.sync(self)
                        _alt = self._interp.adaptivePredict(self._input,101,self._ctx)

                    pass


                pass

            elif la_ == 65:
                localctx = SparkSQLParser.FailNativeCommandContext(self, localctx)
                self.enterOuterAlt(localctx, 65)
                self.state = 982
                self.match(SparkSQLParser.SET)
                self.state = 983
                self.match(SparkSQLParser.ROLE)
                self.state = 987
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,103,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 984
                        self.matchWildcard() 
                    self.state = 989
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,103,self._ctx)

                pass

            elif la_ == 66:
                localctx = SparkSQLParser.SetTimeZoneContext(self, localctx)
                self.enterOuterAlt(localctx, 66)
                self.state = 990
                self.match(SparkSQLParser.SET)
                self.state = 991
                self.match(SparkSQLParser.TIME)
                self.state = 992
                self.match(SparkSQLParser.ZONE)
                self.state = 993
                self.interval()
                pass

            elif la_ == 67:
                localctx = SparkSQLParser.SetTimeZoneContext(self, localctx)
                self.enterOuterAlt(localctx, 67)
                self.state = 994
                self.match(SparkSQLParser.SET)
                self.state = 995
                self.match(SparkSQLParser.TIME)
                self.state = 996
                self.match(SparkSQLParser.ZONE)
                self.state = 997
                localctx.timezone = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.LOCAL or _la==SparkSQLParser.STRING):
                    localctx.timezone = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass

            elif la_ == 68:
                localctx = SparkSQLParser.SetTimeZoneContext(self, localctx)
                self.enterOuterAlt(localctx, 68)
                self.state = 998
                self.match(SparkSQLParser.SET)
                self.state = 999
                self.match(SparkSQLParser.TIME)
                self.state = 1000
                self.match(SparkSQLParser.ZONE)
                self.state = 1004
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,104,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 1001
                        self.matchWildcard() 
                    self.state = 1006
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,104,self._ctx)

                pass

            elif la_ == 69:
                localctx = SparkSQLParser.SetQuotedConfigurationContext(self, localctx)
                self.enterOuterAlt(localctx, 69)
                self.state = 1007
                self.match(SparkSQLParser.SET)
                self.state = 1008
                self.configKey()
                self.state = 1016
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.EQ:
                    self.state = 1009
                    self.match(SparkSQLParser.EQ)
                    self.state = 1013
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,105,self._ctx)
                    while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                        if _alt==1+1:
                            self.state = 1010
                            self.matchWildcard() 
                        self.state = 1015
                        self._errHandler.sync(self)
                        _alt = self._interp.adaptivePredict(self._input,105,self._ctx)



                pass

            elif la_ == 70:
                localctx = SparkSQLParser.SetConfigurationContext(self, localctx)
                self.enterOuterAlt(localctx, 70)
                self.state = 1018
                self.match(SparkSQLParser.SET)
                self.state = 1022
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,107,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 1019
                        self.matchWildcard() 
                    self.state = 1024
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,107,self._ctx)

                pass

            elif la_ == 71:
                localctx = SparkSQLParser.ResetQuotedConfigurationContext(self, localctx)
                self.enterOuterAlt(localctx, 71)
                self.state = 1025
                self.match(SparkSQLParser.RESET)
                self.state = 1026
                self.configKey()
                pass

            elif la_ == 72:
                localctx = SparkSQLParser.ResetConfigurationContext(self, localctx)
                self.enterOuterAlt(localctx, 72)
                self.state = 1027
                self.match(SparkSQLParser.RESET)
                self.state = 1031
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,108,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 1028
                        self.matchWildcard() 
                    self.state = 1033
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,108,self._ctx)

                pass

            elif la_ == 73:
                localctx = SparkSQLParser.FailNativeCommandContext(self, localctx)
                self.enterOuterAlt(localctx, 73)
                self.state = 1034
                self.unsupportedHiveNativeCommands()
                self.state = 1038
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,109,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 1035
                        self.matchWildcard() 
                    self.state = 1040
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,109,self._ctx)

                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ConfigKeyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def quotedIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.QuotedIdentifierContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_configKey

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterConfigKey" ):
                listener.enterConfigKey(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitConfigKey" ):
                listener.exitConfigKey(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitConfigKey" ):
                return visitor.visitConfigKey(self)
            else:
                return visitor.visitChildren(self)




    def configKey(self):

        localctx = SparkSQLParser.ConfigKeyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 16, self.RULE_configKey)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1043
            self.quotedIdentifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class UnsupportedHiveNativeCommandsContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.kw1 = None # Token
            self.kw2 = None # Token
            self.kw3 = None # Token
            self.kw4 = None # Token
            self.kw5 = None # Token
            self.kw6 = None # Token

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)

        def ROLE(self):
            return self.getToken(SparkSQLParser.ROLE, 0)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)

        def GRANT(self):
            return self.getToken(SparkSQLParser.GRANT, 0)

        def REVOKE(self):
            return self.getToken(SparkSQLParser.REVOKE, 0)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)

        def PRINCIPALS(self):
            return self.getToken(SparkSQLParser.PRINCIPALS, 0)

        def ROLES(self):
            return self.getToken(SparkSQLParser.ROLES, 0)

        def CURRENT(self):
            return self.getToken(SparkSQLParser.CURRENT, 0)

        def EXPORT(self):
            return self.getToken(SparkSQLParser.EXPORT, 0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)

        def IMPORT(self):
            return self.getToken(SparkSQLParser.IMPORT, 0)

        def COMPACTIONS(self):
            return self.getToken(SparkSQLParser.COMPACTIONS, 0)

        def TRANSACTIONS(self):
            return self.getToken(SparkSQLParser.TRANSACTIONS, 0)

        def INDEXES(self):
            return self.getToken(SparkSQLParser.INDEXES, 0)

        def LOCKS(self):
            return self.getToken(SparkSQLParser.LOCKS, 0)

        def INDEX(self):
            return self.getToken(SparkSQLParser.INDEX, 0)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)

        def LOCK(self):
            return self.getToken(SparkSQLParser.LOCK, 0)

        def DATABASE(self):
            return self.getToken(SparkSQLParser.DATABASE, 0)

        def UNLOCK(self):
            return self.getToken(SparkSQLParser.UNLOCK, 0)

        def TEMPORARY(self):
            return self.getToken(SparkSQLParser.TEMPORARY, 0)

        def MACRO(self):
            return self.getToken(SparkSQLParser.MACRO, 0)

        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.TableIdentifierContext,0)


        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def CLUSTERED(self):
            return self.getToken(SparkSQLParser.CLUSTERED, 0)

        def BY(self):
            return self.getToken(SparkSQLParser.BY, 0)

        def SORTED(self):
            return self.getToken(SparkSQLParser.SORTED, 0)

        def SKEWED(self):
            return self.getToken(SparkSQLParser.SKEWED, 0)

        def STORED(self):
            return self.getToken(SparkSQLParser.STORED, 0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def DIRECTORIES(self):
            return self.getToken(SparkSQLParser.DIRECTORIES, 0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)

        def LOCATION(self):
            return self.getToken(SparkSQLParser.LOCATION, 0)

        def EXCHANGE(self):
            return self.getToken(SparkSQLParser.EXCHANGE, 0)

        def PARTITION(self):
            return self.getToken(SparkSQLParser.PARTITION, 0)

        def ARCHIVE(self):
            return self.getToken(SparkSQLParser.ARCHIVE, 0)

        def UNARCHIVE(self):
            return self.getToken(SparkSQLParser.UNARCHIVE, 0)

        def TOUCH(self):
            return self.getToken(SparkSQLParser.TOUCH, 0)

        def COMPACT(self):
            return self.getToken(SparkSQLParser.COMPACT, 0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)


        def CONCATENATE(self):
            return self.getToken(SparkSQLParser.CONCATENATE, 0)

        def FILEFORMAT(self):
            return self.getToken(SparkSQLParser.FILEFORMAT, 0)

        def REPLACE(self):
            return self.getToken(SparkSQLParser.REPLACE, 0)

        def COLUMNS(self):
            return self.getToken(SparkSQLParser.COLUMNS, 0)

        def START(self):
            return self.getToken(SparkSQLParser.START, 0)

        def TRANSACTION(self):
            return self.getToken(SparkSQLParser.TRANSACTION, 0)

        def COMMIT(self):
            return self.getToken(SparkSQLParser.COMMIT, 0)

        def ROLLBACK(self):
            return self.getToken(SparkSQLParser.ROLLBACK, 0)

        def DFS(self):
            return self.getToken(SparkSQLParser.DFS, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_unsupportedHiveNativeCommands

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnsupportedHiveNativeCommands" ):
                listener.enterUnsupportedHiveNativeCommands(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnsupportedHiveNativeCommands" ):
                listener.exitUnsupportedHiveNativeCommands(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnsupportedHiveNativeCommands" ):
                return visitor.visitUnsupportedHiveNativeCommands(self)
            else:
                return visitor.visitChildren(self)




    def unsupportedHiveNativeCommands(self):

        localctx = SparkSQLParser.UnsupportedHiveNativeCommandsContext(self, self._ctx, self.state)
        self.enterRule(localctx, 18, self.RULE_unsupportedHiveNativeCommands)
        self._la = 0 # Token type
        try:
            self.state = 1213
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,118,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1045
                localctx.kw1 = self.match(SparkSQLParser.CREATE)
                self.state = 1046
                localctx.kw2 = self.match(SparkSQLParser.ROLE)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1047
                localctx.kw1 = self.match(SparkSQLParser.DROP)
                self.state = 1048
                localctx.kw2 = self.match(SparkSQLParser.ROLE)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 1049
                localctx.kw1 = self.match(SparkSQLParser.GRANT)
                self.state = 1051
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,111,self._ctx)
                if la_ == 1:
                    self.state = 1050
                    localctx.kw2 = self.match(SparkSQLParser.ROLE)


                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 1053
                localctx.kw1 = self.match(SparkSQLParser.REVOKE)
                self.state = 1055
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,112,self._ctx)
                if la_ == 1:
                    self.state = 1054
                    localctx.kw2 = self.match(SparkSQLParser.ROLE)


                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 1057
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1058
                localctx.kw2 = self.match(SparkSQLParser.GRANT)
                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 1059
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1060
                localctx.kw2 = self.match(SparkSQLParser.ROLE)
                self.state = 1062
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,113,self._ctx)
                if la_ == 1:
                    self.state = 1061
                    localctx.kw3 = self.match(SparkSQLParser.GRANT)


                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 1064
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1065
                localctx.kw2 = self.match(SparkSQLParser.PRINCIPALS)
                pass

            elif la_ == 8:
                self.enterOuterAlt(localctx, 8)
                self.state = 1066
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1067
                localctx.kw2 = self.match(SparkSQLParser.ROLES)
                pass

            elif la_ == 9:
                self.enterOuterAlt(localctx, 9)
                self.state = 1068
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1069
                localctx.kw2 = self.match(SparkSQLParser.CURRENT)
                self.state = 1070
                localctx.kw3 = self.match(SparkSQLParser.ROLES)
                pass

            elif la_ == 10:
                self.enterOuterAlt(localctx, 10)
                self.state = 1071
                localctx.kw1 = self.match(SparkSQLParser.EXPORT)
                self.state = 1072
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                pass

            elif la_ == 11:
                self.enterOuterAlt(localctx, 11)
                self.state = 1073
                localctx.kw1 = self.match(SparkSQLParser.IMPORT)
                self.state = 1074
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                pass

            elif la_ == 12:
                self.enterOuterAlt(localctx, 12)
                self.state = 1075
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1076
                localctx.kw2 = self.match(SparkSQLParser.COMPACTIONS)
                pass

            elif la_ == 13:
                self.enterOuterAlt(localctx, 13)
                self.state = 1077
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1078
                localctx.kw2 = self.match(SparkSQLParser.CREATE)
                self.state = 1079
                localctx.kw3 = self.match(SparkSQLParser.TABLE)
                pass

            elif la_ == 14:
                self.enterOuterAlt(localctx, 14)
                self.state = 1080
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1081
                localctx.kw2 = self.match(SparkSQLParser.TRANSACTIONS)
                pass

            elif la_ == 15:
                self.enterOuterAlt(localctx, 15)
                self.state = 1082
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1083
                localctx.kw2 = self.match(SparkSQLParser.INDEXES)
                pass

            elif la_ == 16:
                self.enterOuterAlt(localctx, 16)
                self.state = 1084
                localctx.kw1 = self.match(SparkSQLParser.SHOW)
                self.state = 1085
                localctx.kw2 = self.match(SparkSQLParser.LOCKS)
                pass

            elif la_ == 17:
                self.enterOuterAlt(localctx, 17)
                self.state = 1086
                localctx.kw1 = self.match(SparkSQLParser.CREATE)
                self.state = 1087
                localctx.kw2 = self.match(SparkSQLParser.INDEX)
                pass

            elif la_ == 18:
                self.enterOuterAlt(localctx, 18)
                self.state = 1088
                localctx.kw1 = self.match(SparkSQLParser.DROP)
                self.state = 1089
                localctx.kw2 = self.match(SparkSQLParser.INDEX)
                pass

            elif la_ == 19:
                self.enterOuterAlt(localctx, 19)
                self.state = 1090
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1091
                localctx.kw2 = self.match(SparkSQLParser.INDEX)
                pass

            elif la_ == 20:
                self.enterOuterAlt(localctx, 20)
                self.state = 1092
                localctx.kw1 = self.match(SparkSQLParser.LOCK)
                self.state = 1093
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                pass

            elif la_ == 21:
                self.enterOuterAlt(localctx, 21)
                self.state = 1094
                localctx.kw1 = self.match(SparkSQLParser.LOCK)
                self.state = 1095
                localctx.kw2 = self.match(SparkSQLParser.DATABASE)
                pass

            elif la_ == 22:
                self.enterOuterAlt(localctx, 22)
                self.state = 1096
                localctx.kw1 = self.match(SparkSQLParser.UNLOCK)
                self.state = 1097
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                pass

            elif la_ == 23:
                self.enterOuterAlt(localctx, 23)
                self.state = 1098
                localctx.kw1 = self.match(SparkSQLParser.UNLOCK)
                self.state = 1099
                localctx.kw2 = self.match(SparkSQLParser.DATABASE)
                pass

            elif la_ == 24:
                self.enterOuterAlt(localctx, 24)
                self.state = 1100
                localctx.kw1 = self.match(SparkSQLParser.CREATE)
                self.state = 1101
                localctx.kw2 = self.match(SparkSQLParser.TEMPORARY)
                self.state = 1102
                localctx.kw3 = self.match(SparkSQLParser.MACRO)
                pass

            elif la_ == 25:
                self.enterOuterAlt(localctx, 25)
                self.state = 1103
                localctx.kw1 = self.match(SparkSQLParser.DROP)
                self.state = 1104
                localctx.kw2 = self.match(SparkSQLParser.TEMPORARY)
                self.state = 1105
                localctx.kw3 = self.match(SparkSQLParser.MACRO)
                pass

            elif la_ == 26:
                self.enterOuterAlt(localctx, 26)
                self.state = 1106
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1107
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1108
                self.tableIdentifier()
                self.state = 1109
                localctx.kw3 = self.match(SparkSQLParser.NOT)
                self.state = 1110
                localctx.kw4 = self.match(SparkSQLParser.CLUSTERED)
                pass

            elif la_ == 27:
                self.enterOuterAlt(localctx, 27)
                self.state = 1112
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1113
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1114
                self.tableIdentifier()
                self.state = 1115
                localctx.kw3 = self.match(SparkSQLParser.CLUSTERED)
                self.state = 1116
                localctx.kw4 = self.match(SparkSQLParser.BY)
                pass

            elif la_ == 28:
                self.enterOuterAlt(localctx, 28)
                self.state = 1118
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1119
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1120
                self.tableIdentifier()
                self.state = 1121
                localctx.kw3 = self.match(SparkSQLParser.NOT)
                self.state = 1122
                localctx.kw4 = self.match(SparkSQLParser.SORTED)
                pass

            elif la_ == 29:
                self.enterOuterAlt(localctx, 29)
                self.state = 1124
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1125
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1126
                self.tableIdentifier()
                self.state = 1127
                localctx.kw3 = self.match(SparkSQLParser.SKEWED)
                self.state = 1128
                localctx.kw4 = self.match(SparkSQLParser.BY)
                pass

            elif la_ == 30:
                self.enterOuterAlt(localctx, 30)
                self.state = 1130
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1131
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1132
                self.tableIdentifier()
                self.state = 1133
                localctx.kw3 = self.match(SparkSQLParser.NOT)
                self.state = 1134
                localctx.kw4 = self.match(SparkSQLParser.SKEWED)
                pass

            elif la_ == 31:
                self.enterOuterAlt(localctx, 31)
                self.state = 1136
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1137
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1138
                self.tableIdentifier()
                self.state = 1139
                localctx.kw3 = self.match(SparkSQLParser.NOT)
                self.state = 1140
                localctx.kw4 = self.match(SparkSQLParser.STORED)
                self.state = 1141
                localctx.kw5 = self.match(SparkSQLParser.AS)
                self.state = 1142
                localctx.kw6 = self.match(SparkSQLParser.DIRECTORIES)
                pass

            elif la_ == 32:
                self.enterOuterAlt(localctx, 32)
                self.state = 1144
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1145
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1146
                self.tableIdentifier()
                self.state = 1147
                localctx.kw3 = self.match(SparkSQLParser.SET)
                self.state = 1148
                localctx.kw4 = self.match(SparkSQLParser.SKEWED)
                self.state = 1149
                localctx.kw5 = self.match(SparkSQLParser.LOCATION)
                pass

            elif la_ == 33:
                self.enterOuterAlt(localctx, 33)
                self.state = 1151
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1152
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1153
                self.tableIdentifier()
                self.state = 1154
                localctx.kw3 = self.match(SparkSQLParser.EXCHANGE)
                self.state = 1155
                localctx.kw4 = self.match(SparkSQLParser.PARTITION)
                pass

            elif la_ == 34:
                self.enterOuterAlt(localctx, 34)
                self.state = 1157
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1158
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1159
                self.tableIdentifier()
                self.state = 1160
                localctx.kw3 = self.match(SparkSQLParser.ARCHIVE)
                self.state = 1161
                localctx.kw4 = self.match(SparkSQLParser.PARTITION)
                pass

            elif la_ == 35:
                self.enterOuterAlt(localctx, 35)
                self.state = 1163
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1164
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1165
                self.tableIdentifier()
                self.state = 1166
                localctx.kw3 = self.match(SparkSQLParser.UNARCHIVE)
                self.state = 1167
                localctx.kw4 = self.match(SparkSQLParser.PARTITION)
                pass

            elif la_ == 36:
                self.enterOuterAlt(localctx, 36)
                self.state = 1169
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1170
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1171
                self.tableIdentifier()
                self.state = 1172
                localctx.kw3 = self.match(SparkSQLParser.TOUCH)
                pass

            elif la_ == 37:
                self.enterOuterAlt(localctx, 37)
                self.state = 1174
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1175
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1176
                self.tableIdentifier()
                self.state = 1178
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 1177
                    self.partitionSpec()


                self.state = 1180
                localctx.kw3 = self.match(SparkSQLParser.COMPACT)
                pass

            elif la_ == 38:
                self.enterOuterAlt(localctx, 38)
                self.state = 1182
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1183
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1184
                self.tableIdentifier()
                self.state = 1186
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 1185
                    self.partitionSpec()


                self.state = 1188
                localctx.kw3 = self.match(SparkSQLParser.CONCATENATE)
                pass

            elif la_ == 39:
                self.enterOuterAlt(localctx, 39)
                self.state = 1190
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1191
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1192
                self.tableIdentifier()
                self.state = 1194
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 1193
                    self.partitionSpec()


                self.state = 1196
                localctx.kw3 = self.match(SparkSQLParser.SET)
                self.state = 1197
                localctx.kw4 = self.match(SparkSQLParser.FILEFORMAT)
                pass

            elif la_ == 40:
                self.enterOuterAlt(localctx, 40)
                self.state = 1199
                localctx.kw1 = self.match(SparkSQLParser.ALTER)
                self.state = 1200
                localctx.kw2 = self.match(SparkSQLParser.TABLE)
                self.state = 1201
                self.tableIdentifier()
                self.state = 1203
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 1202
                    self.partitionSpec()


                self.state = 1205
                localctx.kw3 = self.match(SparkSQLParser.REPLACE)
                self.state = 1206
                localctx.kw4 = self.match(SparkSQLParser.COLUMNS)
                pass

            elif la_ == 41:
                self.enterOuterAlt(localctx, 41)
                self.state = 1208
                localctx.kw1 = self.match(SparkSQLParser.START)
                self.state = 1209
                localctx.kw2 = self.match(SparkSQLParser.TRANSACTION)
                pass

            elif la_ == 42:
                self.enterOuterAlt(localctx, 42)
                self.state = 1210
                localctx.kw1 = self.match(SparkSQLParser.COMMIT)
                pass

            elif la_ == 43:
                self.enterOuterAlt(localctx, 43)
                self.state = 1211
                localctx.kw1 = self.match(SparkSQLParser.ROLLBACK)
                pass

            elif la_ == 44:
                self.enterOuterAlt(localctx, 44)
                self.state = 1212
                localctx.kw1 = self.match(SparkSQLParser.DFS)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class CreateTableHeaderContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def TEMPORARY(self):
            return self.getToken(SparkSQLParser.TEMPORARY, 0)

        def EXTERNAL(self):
            return self.getToken(SparkSQLParser.EXTERNAL, 0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_createTableHeader

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTableHeader" ):
                listener.enterCreateTableHeader(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTableHeader" ):
                listener.exitCreateTableHeader(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTableHeader" ):
                return visitor.visitCreateTableHeader(self)
            else:
                return visitor.visitChildren(self)




    def createTableHeader(self):

        localctx = SparkSQLParser.CreateTableHeaderContext(self, self._ctx, self.state)
        self.enterRule(localctx, 20, self.RULE_createTableHeader)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1215
            self.match(SparkSQLParser.CREATE)
            self.state = 1217
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.TEMPORARY:
                self.state = 1216
                self.match(SparkSQLParser.TEMPORARY)


            self.state = 1220
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.EXTERNAL:
                self.state = 1219
                self.match(SparkSQLParser.EXTERNAL)


            self.state = 1222
            self.match(SparkSQLParser.TABLE)
            self.state = 1226
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,121,self._ctx)
            if la_ == 1:
                self.state = 1223
                self.match(SparkSQLParser.IF)
                self.state = 1224
                self.match(SparkSQLParser.NOT)
                self.state = 1225
                self.match(SparkSQLParser.EXISTS)


            self.state = 1228
            self.multipartIdentifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ReplaceTableHeaderContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def REPLACE(self):
            return self.getToken(SparkSQLParser.REPLACE, 0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)

        def OR(self):
            return self.getToken(SparkSQLParser.OR, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_replaceTableHeader

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterReplaceTableHeader" ):
                listener.enterReplaceTableHeader(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitReplaceTableHeader" ):
                listener.exitReplaceTableHeader(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitReplaceTableHeader" ):
                return visitor.visitReplaceTableHeader(self)
            else:
                return visitor.visitChildren(self)




    def replaceTableHeader(self):

        localctx = SparkSQLParser.ReplaceTableHeaderContext(self, self._ctx, self.state)
        self.enterRule(localctx, 22, self.RULE_replaceTableHeader)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1232
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.CREATE:
                self.state = 1230
                self.match(SparkSQLParser.CREATE)
                self.state = 1231
                self.match(SparkSQLParser.OR)


            self.state = 1234
            self.match(SparkSQLParser.REPLACE)
            self.state = 1235
            self.match(SparkSQLParser.TABLE)
            self.state = 1236
            self.multipartIdentifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class BucketSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def CLUSTERED(self):
            return self.getToken(SparkSQLParser.CLUSTERED, 0)

        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.BY)
            else:
                return self.getToken(SparkSQLParser.BY, i)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,0)


        def INTO(self):
            return self.getToken(SparkSQLParser.INTO, 0)

        def INTEGER_VALUE(self):
            return self.getToken(SparkSQLParser.INTEGER_VALUE, 0)

        def BUCKETS(self):
            return self.getToken(SparkSQLParser.BUCKETS, 0)

        def SORTED(self):
            return self.getToken(SparkSQLParser.SORTED, 0)

        def orderedIdentifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.OrderedIdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_bucketSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBucketSpec" ):
                listener.enterBucketSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBucketSpec" ):
                listener.exitBucketSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBucketSpec" ):
                return visitor.visitBucketSpec(self)
            else:
                return visitor.visitChildren(self)




    def bucketSpec(self):

        localctx = SparkSQLParser.BucketSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 24, self.RULE_bucketSpec)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1238
            self.match(SparkSQLParser.CLUSTERED)
            self.state = 1239
            self.match(SparkSQLParser.BY)
            self.state = 1240
            self.identifierList()
            self.state = 1244
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.SORTED:
                self.state = 1241
                self.match(SparkSQLParser.SORTED)
                self.state = 1242
                self.match(SparkSQLParser.BY)
                self.state = 1243
                self.orderedIdentifierList()


            self.state = 1246
            self.match(SparkSQLParser.INTO)
            self.state = 1247
            self.match(SparkSQLParser.INTEGER_VALUE)
            self.state = 1248
            self.match(SparkSQLParser.BUCKETS)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SkewSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def SKEWED(self):
            return self.getToken(SparkSQLParser.SKEWED, 0)

        def BY(self):
            return self.getToken(SparkSQLParser.BY, 0)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,0)


        def ON(self):
            return self.getToken(SparkSQLParser.ON, 0)

        def constantList(self):
            return self.getTypedRuleContext(SparkSQLParser.ConstantListContext,0)


        def nestedConstantList(self):
            return self.getTypedRuleContext(SparkSQLParser.NestedConstantListContext,0)


        def STORED(self):
            return self.getToken(SparkSQLParser.STORED, 0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def DIRECTORIES(self):
            return self.getToken(SparkSQLParser.DIRECTORIES, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_skewSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSkewSpec" ):
                listener.enterSkewSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSkewSpec" ):
                listener.exitSkewSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSkewSpec" ):
                return visitor.visitSkewSpec(self)
            else:
                return visitor.visitChildren(self)




    def skewSpec(self):

        localctx = SparkSQLParser.SkewSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 26, self.RULE_skewSpec)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1250
            self.match(SparkSQLParser.SKEWED)
            self.state = 1251
            self.match(SparkSQLParser.BY)
            self.state = 1252
            self.identifierList()
            self.state = 1253
            self.match(SparkSQLParser.ON)
            self.state = 1256
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,124,self._ctx)
            if la_ == 1:
                self.state = 1254
                self.constantList()
                pass

            elif la_ == 2:
                self.state = 1255
                self.nestedConstantList()
                pass


            self.state = 1261
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,125,self._ctx)
            if la_ == 1:
                self.state = 1258
                self.match(SparkSQLParser.STORED)
                self.state = 1259
                self.match(SparkSQLParser.AS)
                self.state = 1260
                self.match(SparkSQLParser.DIRECTORIES)


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class LocationSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def LOCATION(self):
            return self.getToken(SparkSQLParser.LOCATION, 0)

        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_locationSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLocationSpec" ):
                listener.enterLocationSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLocationSpec" ):
                listener.exitLocationSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLocationSpec" ):
                return visitor.visitLocationSpec(self)
            else:
                return visitor.visitChildren(self)




    def locationSpec(self):

        localctx = SparkSQLParser.LocationSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 28, self.RULE_locationSpec)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1263
            self.match(SparkSQLParser.LOCATION)
            self.state = 1264
            self.match(SparkSQLParser.STRING)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class CommentSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def COMMENT(self):
            return self.getToken(SparkSQLParser.COMMENT, 0)

        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_commentSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCommentSpec" ):
                listener.enterCommentSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCommentSpec" ):
                listener.exitCommentSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCommentSpec" ):
                return visitor.visitCommentSpec(self)
            else:
                return visitor.visitChildren(self)




    def commentSpec(self):

        localctx = SparkSQLParser.CommentSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 30, self.RULE_commentSpec)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1266
            self.match(SparkSQLParser.COMMENT)
            self.state = 1267
            self.match(SparkSQLParser.STRING)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QueryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def queryTerm(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryTermContext,0)


        def queryOrganization(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryOrganizationContext,0)


        def ctes(self):
            return self.getTypedRuleContext(SparkSQLParser.CtesContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_query

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuery" ):
                listener.enterQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuery" ):
                listener.exitQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuery" ):
                return visitor.visitQuery(self)
            else:
                return visitor.visitChildren(self)




    def query(self):

        localctx = SparkSQLParser.QueryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 32, self.RULE_query)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1270
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.WITH:
                self.state = 1269
                self.ctes()


            self.state = 1272
            self.queryTerm(0)
            self.state = 1273
            self.queryOrganization()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class InsertIntoContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_insertInto

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class InsertOverwriteHiveDirContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.InsertIntoContext
            super().__init__(parser)
            self.path = None # Token
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSQLParser.INSERT, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSQLParser.OVERWRITE, 0)
        def DIRECTORY(self):
            return self.getToken(SparkSQLParser.DIRECTORY, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def LOCAL(self):
            return self.getToken(SparkSQLParser.LOCAL, 0)
        def rowFormat(self):
            return self.getTypedRuleContext(SparkSQLParser.RowFormatContext,0)

        def createFileFormat(self):
            return self.getTypedRuleContext(SparkSQLParser.CreateFileFormatContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertOverwriteHiveDir" ):
                listener.enterInsertOverwriteHiveDir(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertOverwriteHiveDir" ):
                listener.exitInsertOverwriteHiveDir(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertOverwriteHiveDir" ):
                return visitor.visitInsertOverwriteHiveDir(self)
            else:
                return visitor.visitChildren(self)


    class InsertOverwriteDirContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.InsertIntoContext
            super().__init__(parser)
            self.path = None # Token
            self.options = None # TablePropertyListContext
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSQLParser.INSERT, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSQLParser.OVERWRITE, 0)
        def DIRECTORY(self):
            return self.getToken(SparkSQLParser.DIRECTORY, 0)
        def tableProvider(self):
            return self.getTypedRuleContext(SparkSQLParser.TableProviderContext,0)

        def LOCAL(self):
            return self.getToken(SparkSQLParser.LOCAL, 0)
        def OPTIONS(self):
            return self.getToken(SparkSQLParser.OPTIONS, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertOverwriteDir" ):
                listener.enterInsertOverwriteDir(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertOverwriteDir" ):
                listener.exitInsertOverwriteDir(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertOverwriteDir" ):
                return visitor.visitInsertOverwriteDir(self)
            else:
                return visitor.visitChildren(self)


    class InsertOverwriteTableContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.InsertIntoContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSQLParser.INSERT, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSQLParser.OVERWRITE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertOverwriteTable" ):
                listener.enterInsertOverwriteTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertOverwriteTable" ):
                listener.exitInsertOverwriteTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertOverwriteTable" ):
                return visitor.visitInsertOverwriteTable(self)
            else:
                return visitor.visitChildren(self)


    class InsertIntoTableContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.InsertIntoContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSQLParser.INSERT, 0)
        def INTO(self):
            return self.getToken(SparkSQLParser.INTO, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertIntoTable" ):
                listener.enterInsertIntoTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertIntoTable" ):
                listener.exitInsertIntoTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertIntoTable" ):
                return visitor.visitInsertIntoTable(self)
            else:
                return visitor.visitChildren(self)



    def insertInto(self):

        localctx = SparkSQLParser.InsertIntoContext(self, self._ctx, self.state)
        self.enterRule(localctx, 34, self.RULE_insertInto)
        self._la = 0 # Token type
        try:
            self.state = 1330
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,139,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.InsertOverwriteTableContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1275
                self.match(SparkSQLParser.INSERT)
                self.state = 1276
                self.match(SparkSQLParser.OVERWRITE)
                self.state = 1278
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,127,self._ctx)
                if la_ == 1:
                    self.state = 1277
                    self.match(SparkSQLParser.TABLE)


                self.state = 1280
                self.multipartIdentifier()
                self.state = 1287
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 1281
                    self.partitionSpec()
                    self.state = 1285
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.IF:
                        self.state = 1282
                        self.match(SparkSQLParser.IF)
                        self.state = 1283
                        self.match(SparkSQLParser.NOT)
                        self.state = 1284
                        self.match(SparkSQLParser.EXISTS)




                pass

            elif la_ == 2:
                localctx = SparkSQLParser.InsertIntoTableContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1289
                self.match(SparkSQLParser.INSERT)
                self.state = 1290
                self.match(SparkSQLParser.INTO)
                self.state = 1292
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,130,self._ctx)
                if la_ == 1:
                    self.state = 1291
                    self.match(SparkSQLParser.TABLE)


                self.state = 1294
                self.multipartIdentifier()
                self.state = 1296
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PARTITION:
                    self.state = 1295
                    self.partitionSpec()


                self.state = 1301
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.IF:
                    self.state = 1298
                    self.match(SparkSQLParser.IF)
                    self.state = 1299
                    self.match(SparkSQLParser.NOT)
                    self.state = 1300
                    self.match(SparkSQLParser.EXISTS)


                pass

            elif la_ == 3:
                localctx = SparkSQLParser.InsertOverwriteHiveDirContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 1303
                self.match(SparkSQLParser.INSERT)
                self.state = 1304
                self.match(SparkSQLParser.OVERWRITE)
                self.state = 1306
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LOCAL:
                    self.state = 1305
                    self.match(SparkSQLParser.LOCAL)


                self.state = 1308
                self.match(SparkSQLParser.DIRECTORY)
                self.state = 1309
                localctx.path = self.match(SparkSQLParser.STRING)
                self.state = 1311
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.ROW:
                    self.state = 1310
                    self.rowFormat()


                self.state = 1314
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.STORED:
                    self.state = 1313
                    self.createFileFormat()


                pass

            elif la_ == 4:
                localctx = SparkSQLParser.InsertOverwriteDirContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 1316
                self.match(SparkSQLParser.INSERT)
                self.state = 1317
                self.match(SparkSQLParser.OVERWRITE)
                self.state = 1319
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LOCAL:
                    self.state = 1318
                    self.match(SparkSQLParser.LOCAL)


                self.state = 1321
                self.match(SparkSQLParser.DIRECTORY)
                self.state = 1323
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.STRING:
                    self.state = 1322
                    localctx.path = self.match(SparkSQLParser.STRING)


                self.state = 1325
                self.tableProvider()
                self.state = 1328
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OPTIONS:
                    self.state = 1326
                    self.match(SparkSQLParser.OPTIONS)
                    self.state = 1327
                    localctx.options = self.tablePropertyList()


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class PartitionSpecLocationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.PartitionSpecContext,0)


        def locationSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.LocationSpecContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_partitionSpecLocation

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPartitionSpecLocation" ):
                listener.enterPartitionSpecLocation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPartitionSpecLocation" ):
                listener.exitPartitionSpecLocation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPartitionSpecLocation" ):
                return visitor.visitPartitionSpecLocation(self)
            else:
                return visitor.visitChildren(self)




    def partitionSpecLocation(self):

        localctx = SparkSQLParser.PartitionSpecLocationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 36, self.RULE_partitionSpecLocation)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1332
            self.partitionSpec()
            self.state = 1334
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.LOCATION:
                self.state = 1333
                self.locationSpec()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class PartitionSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def PARTITION(self):
            return self.getToken(SparkSQLParser.PARTITION, 0)

        def partitionVal(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.PartitionValContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.PartitionValContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_partitionSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPartitionSpec" ):
                listener.enterPartitionSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPartitionSpec" ):
                listener.exitPartitionSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPartitionSpec" ):
                return visitor.visitPartitionSpec(self)
            else:
                return visitor.visitChildren(self)




    def partitionSpec(self):

        localctx = SparkSQLParser.PartitionSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 38, self.RULE_partitionSpec)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1336
            self.match(SparkSQLParser.PARTITION)
            self.state = 1337
            self.match(SparkSQLParser.T__1)
            self.state = 1338
            self.partitionVal()
            self.state = 1343
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 1339
                self.match(SparkSQLParser.T__3)
                self.state = 1340
                self.partitionVal()
                self.state = 1345
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1346
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class PartitionValContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def EQ(self):
            return self.getToken(SparkSQLParser.EQ, 0)

        def constant(self):
            return self.getTypedRuleContext(SparkSQLParser.ConstantContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_partitionVal

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPartitionVal" ):
                listener.enterPartitionVal(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPartitionVal" ):
                listener.exitPartitionVal(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPartitionVal" ):
                return visitor.visitPartitionVal(self)
            else:
                return visitor.visitChildren(self)




    def partitionVal(self):

        localctx = SparkSQLParser.PartitionValContext(self, self._ctx, self.state)
        self.enterRule(localctx, 40, self.RULE_partitionVal)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1348
            self.identifier()
            self.state = 1351
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.EQ:
                self.state = 1349
                self.match(SparkSQLParser.EQ)
                self.state = 1350
                self.constant()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NamespaceContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def NAMESPACE(self):
            return self.getToken(SparkSQLParser.NAMESPACE, 0)

        def DATABASE(self):
            return self.getToken(SparkSQLParser.DATABASE, 0)

        def SCHEMA(self):
            return self.getToken(SparkSQLParser.SCHEMA, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_namespace

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamespace" ):
                listener.enterNamespace(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamespace" ):
                listener.exitNamespace(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamespace" ):
                return visitor.visitNamespace(self)
            else:
                return visitor.visitChildren(self)




    def namespace(self):

        localctx = SparkSQLParser.NamespaceContext(self, self._ctx, self.state)
        self.enterRule(localctx, 42, self.RULE_namespace)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1353
            _la = self._input.LA(1)
            if not(_la==SparkSQLParser.DATABASE or _la==SparkSQLParser.NAMESPACE or _la==SparkSQLParser.SCHEMA):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class DescribeFuncNameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,0)


        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def comparisonOperator(self):
            return self.getTypedRuleContext(SparkSQLParser.ComparisonOperatorContext,0)


        def arithmeticOperator(self):
            return self.getTypedRuleContext(SparkSQLParser.ArithmeticOperatorContext,0)


        def predicateOperator(self):
            return self.getTypedRuleContext(SparkSQLParser.PredicateOperatorContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_describeFuncName

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeFuncName" ):
                listener.enterDescribeFuncName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeFuncName" ):
                listener.exitDescribeFuncName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeFuncName" ):
                return visitor.visitDescribeFuncName(self)
            else:
                return visitor.visitChildren(self)




    def describeFuncName(self):

        localctx = SparkSQLParser.DescribeFuncNameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 44, self.RULE_describeFuncName)
        try:
            self.state = 1360
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,143,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1355
                self.qualifiedName()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1356
                self.match(SparkSQLParser.STRING)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 1357
                self.comparisonOperator()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 1358
                self.arithmeticOperator()
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 1359
                self.predicateOperator()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class DescribeColNameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._identifier = None # IdentifierContext
            self.nameParts = list() # of IdentifierContexts

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_describeColName

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeColName" ):
                listener.enterDescribeColName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeColName" ):
                listener.exitDescribeColName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeColName" ):
                return visitor.visitDescribeColName(self)
            else:
                return visitor.visitChildren(self)




    def describeColName(self):

        localctx = SparkSQLParser.DescribeColNameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 46, self.RULE_describeColName)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1362
            localctx._identifier = self.identifier()
            localctx.nameParts.append(localctx._identifier)
            self.state = 1367
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__4:
                self.state = 1363
                self.match(SparkSQLParser.T__4)
                self.state = 1364
                localctx._identifier = self.identifier()
                localctx.nameParts.append(localctx._identifier)
                self.state = 1369
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class CtesContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def WITH(self):
            return self.getToken(SparkSQLParser.WITH, 0)

        def namedQuery(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.NamedQueryContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.NamedQueryContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_ctes

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCtes" ):
                listener.enterCtes(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCtes" ):
                listener.exitCtes(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCtes" ):
                return visitor.visitCtes(self)
            else:
                return visitor.visitChildren(self)




    def ctes(self):

        localctx = SparkSQLParser.CtesContext(self, self._ctx, self.state)
        self.enterRule(localctx, 48, self.RULE_ctes)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1370
            self.match(SparkSQLParser.WITH)
            self.state = 1371
            self.namedQuery()
            self.state = 1376
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 1372
                self.match(SparkSQLParser.T__3)
                self.state = 1373
                self.namedQuery()
                self.state = 1378
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NamedQueryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.name = None # ErrorCapturingIdentifierContext
            self.columnAliases = None # IdentifierListContext

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)


        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_namedQuery

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedQuery" ):
                listener.enterNamedQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedQuery" ):
                listener.exitNamedQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedQuery" ):
                return visitor.visitNamedQuery(self)
            else:
                return visitor.visitChildren(self)




    def namedQuery(self):

        localctx = SparkSQLParser.NamedQueryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 50, self.RULE_namedQuery)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1379
            localctx.name = self.errorCapturingIdentifier()
            self.state = 1381
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,146,self._ctx)
            if la_ == 1:
                self.state = 1380
                localctx.columnAliases = self.identifierList()


            self.state = 1384
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.AS:
                self.state = 1383
                self.match(SparkSQLParser.AS)


            self.state = 1386
            self.match(SparkSQLParser.T__1)
            self.state = 1387
            self.query()
            self.state = 1388
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TableProviderContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def USING(self):
            return self.getToken(SparkSQLParser.USING, 0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_tableProvider

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableProvider" ):
                listener.enterTableProvider(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableProvider" ):
                listener.exitTableProvider(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableProvider" ):
                return visitor.visitTableProvider(self)
            else:
                return visitor.visitChildren(self)




    def tableProvider(self):

        localctx = SparkSQLParser.TableProviderContext(self, self._ctx, self.state)
        self.enterRule(localctx, 52, self.RULE_tableProvider)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1390
            self.match(SparkSQLParser.USING)
            self.state = 1391
            self.multipartIdentifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class CreateTableClausesContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.options = None # TablePropertyListContext
            self.partitioning = None # TransformListContext
            self.tableProps = None # TablePropertyListContext

        def bucketSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.BucketSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.BucketSpecContext,i)


        def locationSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.LocationSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.LocationSpecContext,i)


        def commentSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.CommentSpecContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,i)


        def OPTIONS(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.OPTIONS)
            else:
                return self.getToken(SparkSQLParser.OPTIONS, i)

        def PARTITIONED(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.PARTITIONED)
            else:
                return self.getToken(SparkSQLParser.PARTITIONED, i)

        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.BY)
            else:
                return self.getToken(SparkSQLParser.BY, i)

        def TBLPROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.TBLPROPERTIES)
            else:
                return self.getToken(SparkSQLParser.TBLPROPERTIES, i)

        def tablePropertyList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TablePropertyListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,i)


        def transformList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TransformListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TransformListContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_createTableClauses

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTableClauses" ):
                listener.enterCreateTableClauses(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTableClauses" ):
                listener.exitCreateTableClauses(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTableClauses" ):
                return visitor.visitCreateTableClauses(self)
            else:
                return visitor.visitChildren(self)




    def createTableClauses(self):

        localctx = SparkSQLParser.CreateTableClausesContext(self, self._ctx, self.state)
        self.enterRule(localctx, 54, self.RULE_createTableClauses)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1405
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.CLUSTERED or _la==SparkSQLParser.COMMENT or ((((_la - 137)) & ~0x3f) == 0 and ((1 << (_la - 137)) & ((1 << (SparkSQLParser.LOCATION - 137)) | (1 << (SparkSQLParser.OPTIONS - 137)) | (1 << (SparkSQLParser.PARTITIONED - 137)))) != 0) or _la==SparkSQLParser.TBLPROPERTIES:
                self.state = 1403
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSQLParser.OPTIONS]:
                    self.state = 1393
                    self.match(SparkSQLParser.OPTIONS)
                    self.state = 1394
                    localctx.options = self.tablePropertyList()
                    pass
                elif token in [SparkSQLParser.PARTITIONED]:
                    self.state = 1395
                    self.match(SparkSQLParser.PARTITIONED)
                    self.state = 1396
                    self.match(SparkSQLParser.BY)
                    self.state = 1397
                    localctx.partitioning = self.transformList()
                    pass
                elif token in [SparkSQLParser.CLUSTERED]:
                    self.state = 1398
                    self.bucketSpec()
                    pass
                elif token in [SparkSQLParser.LOCATION]:
                    self.state = 1399
                    self.locationSpec()
                    pass
                elif token in [SparkSQLParser.COMMENT]:
                    self.state = 1400
                    self.commentSpec()
                    pass
                elif token in [SparkSQLParser.TBLPROPERTIES]:
                    self.state = 1401
                    self.match(SparkSQLParser.TBLPROPERTIES)
                    self.state = 1402
                    localctx.tableProps = self.tablePropertyList()
                    pass
                else:
                    raise NoViableAltException(self)

                self.state = 1407
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TablePropertyListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def tableProperty(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TablePropertyContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TablePropertyContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_tablePropertyList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTablePropertyList" ):
                listener.enterTablePropertyList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTablePropertyList" ):
                listener.exitTablePropertyList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTablePropertyList" ):
                return visitor.visitTablePropertyList(self)
            else:
                return visitor.visitChildren(self)




    def tablePropertyList(self):

        localctx = SparkSQLParser.TablePropertyListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 56, self.RULE_tablePropertyList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1408
            self.match(SparkSQLParser.T__1)
            self.state = 1409
            self.tableProperty()
            self.state = 1414
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 1410
                self.match(SparkSQLParser.T__3)
                self.state = 1411
                self.tableProperty()
                self.state = 1416
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1417
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TablePropertyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.key = None # TablePropertyKeyContext
            self.value = None # TablePropertyValueContext

        def tablePropertyKey(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyKeyContext,0)


        def tablePropertyValue(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyValueContext,0)


        def EQ(self):
            return self.getToken(SparkSQLParser.EQ, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_tableProperty

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableProperty" ):
                listener.enterTableProperty(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableProperty" ):
                listener.exitTableProperty(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableProperty" ):
                return visitor.visitTableProperty(self)
            else:
                return visitor.visitChildren(self)




    def tableProperty(self):

        localctx = SparkSQLParser.TablePropertyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 58, self.RULE_tableProperty)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1419
            localctx.key = self.tablePropertyKey()
            self.state = 1424
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.FALSE or ((((_la - 238)) & ~0x3f) == 0 and ((1 << (_la - 238)) & ((1 << (SparkSQLParser.TRUE - 238)) | (1 << (SparkSQLParser.EQ - 238)) | (1 << (SparkSQLParser.STRING - 238)) | (1 << (SparkSQLParser.INTEGER_VALUE - 238)) | (1 << (SparkSQLParser.DECIMAL_VALUE - 238)))) != 0):
                self.state = 1421
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.EQ:
                    self.state = 1420
                    self.match(SparkSQLParser.EQ)


                self.state = 1423
                localctx.value = self.tablePropertyValue()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TablePropertyKeyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)


        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_tablePropertyKey

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTablePropertyKey" ):
                listener.enterTablePropertyKey(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTablePropertyKey" ):
                listener.exitTablePropertyKey(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTablePropertyKey" ):
                return visitor.visitTablePropertyKey(self)
            else:
                return visitor.visitChildren(self)




    def tablePropertyKey(self):

        localctx = SparkSQLParser.TablePropertyKeyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 60, self.RULE_tablePropertyKey)
        self._la = 0 # Token type
        try:
            self.state = 1435
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.ADD, SparkSQLParser.AFTER, SparkSQLParser.ALL, SparkSQLParser.ALTER, SparkSQLParser.ANALYZE, SparkSQLParser.AND, SparkSQLParser.ANTI, SparkSQLParser.ANY, SparkSQLParser.ARCHIVE, SparkSQLParser.ARRAY, SparkSQLParser.AS, SparkSQLParser.ASC, SparkSQLParser.AT, SparkSQLParser.AUTHORIZATION, SparkSQLParser.BETWEEN, SparkSQLParser.BOTH, SparkSQLParser.BUCKET, SparkSQLParser.BUCKETS, SparkSQLParser.BY, SparkSQLParser.CACHE, SparkSQLParser.CASCADE, SparkSQLParser.CASE, SparkSQLParser.CAST, SparkSQLParser.CHANGE, SparkSQLParser.CHECK, SparkSQLParser.CLEAR, SparkSQLParser.CLUSTER, SparkSQLParser.CLUSTERED, SparkSQLParser.CODEGEN, SparkSQLParser.COLLATE, SparkSQLParser.COLLECTION, SparkSQLParser.COLUMN, SparkSQLParser.COLUMNS, SparkSQLParser.COMMENT, SparkSQLParser.COMMIT, SparkSQLParser.COMPACT, SparkSQLParser.COMPACTIONS, SparkSQLParser.COMPUTE, SparkSQLParser.CONCATENATE, SparkSQLParser.CONSTRAINT, SparkSQLParser.COST, SparkSQLParser.CREATE, SparkSQLParser.CROSS, SparkSQLParser.CUBE, SparkSQLParser.CURRENT, SparkSQLParser.CURRENT_DATE, SparkSQLParser.CURRENT_TIME, SparkSQLParser.CURRENT_TIMESTAMP, SparkSQLParser.CURRENT_USER, SparkSQLParser.DATA, SparkSQLParser.DATABASE, SparkSQLParser.DATABASES, SparkSQLParser.DBPROPERTIES, SparkSQLParser.DEFINED, SparkSQLParser.DELETE, SparkSQLParser.DELIMITED, SparkSQLParser.DESC, SparkSQLParser.DESCRIBE, SparkSQLParser.DFS, SparkSQLParser.DIRECTORIES, SparkSQLParser.DIRECTORY, SparkSQLParser.DISTINCT, SparkSQLParser.DISTRIBUTE, SparkSQLParser.DIV, SparkSQLParser.DROP, SparkSQLParser.ELSE, SparkSQLParser.END, SparkSQLParser.ESCAPE, SparkSQLParser.ESCAPED, SparkSQLParser.EXCEPT, SparkSQLParser.EXCHANGE, SparkSQLParser.EXISTS, SparkSQLParser.EXPLAIN, SparkSQLParser.EXPORT, SparkSQLParser.EXTENDED, SparkSQLParser.EXTERNAL, SparkSQLParser.EXTRACT, SparkSQLParser.FALSE, SparkSQLParser.FETCH, SparkSQLParser.FIELDS, SparkSQLParser.FILTER, SparkSQLParser.FILEFORMAT, SparkSQLParser.FIRST, SparkSQLParser.FOLLOWING, SparkSQLParser.FOR, SparkSQLParser.FOREIGN, SparkSQLParser.FORMAT, SparkSQLParser.FORMATTED, SparkSQLParser.FROM, SparkSQLParser.FULL, SparkSQLParser.FUNCTION, SparkSQLParser.FUNCTIONS, SparkSQLParser.GLOBAL, SparkSQLParser.GRANT, SparkSQLParser.GROUP, SparkSQLParser.GROUPING, SparkSQLParser.HAVING, SparkSQLParser.IF, SparkSQLParser.IGNORE, SparkSQLParser.IMPORT, SparkSQLParser.IN, SparkSQLParser.INDEX, SparkSQLParser.INDEXES, SparkSQLParser.INNER, SparkSQLParser.INPATH, SparkSQLParser.INPUTFORMAT, SparkSQLParser.INSERT, SparkSQLParser.INTERSECT, SparkSQLParser.INTERVAL, SparkSQLParser.INTO, SparkSQLParser.IS, SparkSQLParser.ITEMS, SparkSQLParser.JOIN, SparkSQLParser.KEYS, SparkSQLParser.LAST, SparkSQLParser.LATERAL, SparkSQLParser.LAZY, SparkSQLParser.LEADING, SparkSQLParser.LEFT, SparkSQLParser.LIKE, SparkSQLParser.LIMIT, SparkSQLParser.LINES, SparkSQLParser.LIST, SparkSQLParser.LOAD, SparkSQLParser.LOCAL, SparkSQLParser.LOCATION, SparkSQLParser.LOCK, SparkSQLParser.LOCKS, SparkSQLParser.LOGICAL, SparkSQLParser.MACRO, SparkSQLParser.MAP, SparkSQLParser.MATCHED, SparkSQLParser.MERGE, SparkSQLParser.MSCK, SparkSQLParser.NAMESPACE, SparkSQLParser.NAMESPACES, SparkSQLParser.NATURAL, SparkSQLParser.NO, SparkSQLParser.NOT, SparkSQLParser.NULL, SparkSQLParser.NULLS, SparkSQLParser.OF, SparkSQLParser.ON, SparkSQLParser.ONLY, SparkSQLParser.OPTION, SparkSQLParser.OPTIONS, SparkSQLParser.OR, SparkSQLParser.ORDER, SparkSQLParser.OUT, SparkSQLParser.OUTER, SparkSQLParser.OUTPUTFORMAT, SparkSQLParser.OVER, SparkSQLParser.OVERLAPS, SparkSQLParser.OVERLAY, SparkSQLParser.OVERWRITE, SparkSQLParser.PARTITION, SparkSQLParser.PARTITIONED, SparkSQLParser.PARTITIONS, SparkSQLParser.PERCENTLIT, SparkSQLParser.PIVOT, SparkSQLParser.PLACING, SparkSQLParser.POSITION, SparkSQLParser.PRECEDING, SparkSQLParser.PRIMARY, SparkSQLParser.PRINCIPALS, SparkSQLParser.PROPERTIES, SparkSQLParser.PURGE, SparkSQLParser.QUERY, SparkSQLParser.RANGE, SparkSQLParser.RECORDREADER, SparkSQLParser.RECORDWRITER, SparkSQLParser.RECOVER, SparkSQLParser.REDUCE, SparkSQLParser.REFERENCES, SparkSQLParser.REFRESH, SparkSQLParser.RENAME, SparkSQLParser.REPAIR, SparkSQLParser.REPLACE, SparkSQLParser.RESET, SparkSQLParser.RESTRICT, SparkSQLParser.REVOKE, SparkSQLParser.RIGHT, SparkSQLParser.RLIKE, SparkSQLParser.ROLE, SparkSQLParser.ROLES, SparkSQLParser.ROLLBACK, SparkSQLParser.ROLLUP, SparkSQLParser.ROW, SparkSQLParser.ROWS, SparkSQLParser.SCHEMA, SparkSQLParser.SELECT, SparkSQLParser.SEMI, SparkSQLParser.SEPARATED, SparkSQLParser.SERDE, SparkSQLParser.SERDEPROPERTIES, SparkSQLParser.SESSION_USER, SparkSQLParser.SET, SparkSQLParser.SETMINUS, SparkSQLParser.SETS, SparkSQLParser.SHOW, SparkSQLParser.SKEWED, SparkSQLParser.SOME, SparkSQLParser.SORT, SparkSQLParser.SORTED, SparkSQLParser.START, SparkSQLParser.STATISTICS, SparkSQLParser.STORED, SparkSQLParser.STRATIFY, SparkSQLParser.STRUCT, SparkSQLParser.SUBSTR, SparkSQLParser.SUBSTRING, SparkSQLParser.TABLE, SparkSQLParser.TABLES, SparkSQLParser.TABLESAMPLE, SparkSQLParser.TBLPROPERTIES, SparkSQLParser.TEMPORARY, SparkSQLParser.TERMINATED, SparkSQLParser.THEN, SparkSQLParser.TIME, SparkSQLParser.TO, SparkSQLParser.TOUCH, SparkSQLParser.TRAILING, SparkSQLParser.TRANSACTION, SparkSQLParser.TRANSACTIONS, SparkSQLParser.TRANSFORM, SparkSQLParser.TRIM, SparkSQLParser.TRUE, SparkSQLParser.TRUNCATE, SparkSQLParser.TYPE, SparkSQLParser.UNARCHIVE, SparkSQLParser.UNBOUNDED, SparkSQLParser.UNCACHE, SparkSQLParser.UNION, SparkSQLParser.UNIQUE, SparkSQLParser.UNKNOWN, SparkSQLParser.UNLOCK, SparkSQLParser.UNSET, SparkSQLParser.UPDATE, SparkSQLParser.USE, SparkSQLParser.USER, SparkSQLParser.USING, SparkSQLParser.VALUES, SparkSQLParser.VIEW, SparkSQLParser.VIEWS, SparkSQLParser.WHEN, SparkSQLParser.WHERE, SparkSQLParser.WINDOW, SparkSQLParser.WITH, SparkSQLParser.ZONE, SparkSQLParser.IDENTIFIER, SparkSQLParser.BACKQUOTED_IDENTIFIER]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1426
                self.identifier()
                self.state = 1431
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__4:
                    self.state = 1427
                    self.match(SparkSQLParser.T__4)
                    self.state = 1428
                    self.identifier()
                    self.state = 1433
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                pass
            elif token in [SparkSQLParser.STRING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1434
                self.match(SparkSQLParser.STRING)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TablePropertyValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INTEGER_VALUE(self):
            return self.getToken(SparkSQLParser.INTEGER_VALUE, 0)

        def DECIMAL_VALUE(self):
            return self.getToken(SparkSQLParser.DECIMAL_VALUE, 0)

        def booleanValue(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanValueContext,0)


        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_tablePropertyValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTablePropertyValue" ):
                listener.enterTablePropertyValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTablePropertyValue" ):
                listener.exitTablePropertyValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTablePropertyValue" ):
                return visitor.visitTablePropertyValue(self)
            else:
                return visitor.visitChildren(self)




    def tablePropertyValue(self):

        localctx = SparkSQLParser.TablePropertyValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 62, self.RULE_tablePropertyValue)
        try:
            self.state = 1441
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.INTEGER_VALUE]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1437
                self.match(SparkSQLParser.INTEGER_VALUE)
                pass
            elif token in [SparkSQLParser.DECIMAL_VALUE]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1438
                self.match(SparkSQLParser.DECIMAL_VALUE)
                pass
            elif token in [SparkSQLParser.FALSE, SparkSQLParser.TRUE]:
                self.enterOuterAlt(localctx, 3)
                self.state = 1439
                self.booleanValue()
                pass
            elif token in [SparkSQLParser.STRING]:
                self.enterOuterAlt(localctx, 4)
                self.state = 1440
                self.match(SparkSQLParser.STRING)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ConstantListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def constant(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ConstantContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ConstantContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_constantList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterConstantList" ):
                listener.enterConstantList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitConstantList" ):
                listener.exitConstantList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitConstantList" ):
                return visitor.visitConstantList(self)
            else:
                return visitor.visitChildren(self)




    def constantList(self):

        localctx = SparkSQLParser.ConstantListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 64, self.RULE_constantList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1443
            self.match(SparkSQLParser.T__1)
            self.state = 1444
            self.constant()
            self.state = 1449
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 1445
                self.match(SparkSQLParser.T__3)
                self.state = 1446
                self.constant()
                self.state = 1451
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1452
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NestedConstantListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def constantList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ConstantListContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ConstantListContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_nestedConstantList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNestedConstantList" ):
                listener.enterNestedConstantList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNestedConstantList" ):
                listener.exitNestedConstantList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNestedConstantList" ):
                return visitor.visitNestedConstantList(self)
            else:
                return visitor.visitChildren(self)




    def nestedConstantList(self):

        localctx = SparkSQLParser.NestedConstantListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 66, self.RULE_nestedConstantList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1454
            self.match(SparkSQLParser.T__1)
            self.state = 1455
            self.constantList()
            self.state = 1460
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 1456
                self.match(SparkSQLParser.T__3)
                self.state = 1457
                self.constantList()
                self.state = 1462
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1463
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class CreateFileFormatContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def STORED(self):
            return self.getToken(SparkSQLParser.STORED, 0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def fileFormat(self):
            return self.getTypedRuleContext(SparkSQLParser.FileFormatContext,0)


        def BY(self):
            return self.getToken(SparkSQLParser.BY, 0)

        def storageHandler(self):
            return self.getTypedRuleContext(SparkSQLParser.StorageHandlerContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_createFileFormat

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateFileFormat" ):
                listener.enterCreateFileFormat(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateFileFormat" ):
                listener.exitCreateFileFormat(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateFileFormat" ):
                return visitor.visitCreateFileFormat(self)
            else:
                return visitor.visitChildren(self)




    def createFileFormat(self):

        localctx = SparkSQLParser.CreateFileFormatContext(self, self._ctx, self.state)
        self.enterRule(localctx, 68, self.RULE_createFileFormat)
        try:
            self.state = 1471
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,158,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1465
                self.match(SparkSQLParser.STORED)
                self.state = 1466
                self.match(SparkSQLParser.AS)
                self.state = 1467
                self.fileFormat()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1468
                self.match(SparkSQLParser.STORED)
                self.state = 1469
                self.match(SparkSQLParser.BY)
                self.state = 1470
                self.storageHandler()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FileFormatContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_fileFormat

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class TableFileFormatContext(FileFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.FileFormatContext
            super().__init__(parser)
            self.inFmt = None # Token
            self.outFmt = None # Token
            self.copyFrom(ctx)

        def INPUTFORMAT(self):
            return self.getToken(SparkSQLParser.INPUTFORMAT, 0)
        def OUTPUTFORMAT(self):
            return self.getToken(SparkSQLParser.OUTPUTFORMAT, 0)
        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.STRING)
            else:
                return self.getToken(SparkSQLParser.STRING, i)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableFileFormat" ):
                listener.enterTableFileFormat(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableFileFormat" ):
                listener.exitTableFileFormat(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableFileFormat" ):
                return visitor.visitTableFileFormat(self)
            else:
                return visitor.visitChildren(self)


    class GenericFileFormatContext(FileFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.FileFormatContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterGenericFileFormat" ):
                listener.enterGenericFileFormat(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitGenericFileFormat" ):
                listener.exitGenericFileFormat(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitGenericFileFormat" ):
                return visitor.visitGenericFileFormat(self)
            else:
                return visitor.visitChildren(self)



    def fileFormat(self):

        localctx = SparkSQLParser.FileFormatContext(self, self._ctx, self.state)
        self.enterRule(localctx, 70, self.RULE_fileFormat)
        try:
            self.state = 1478
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,159,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.TableFileFormatContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1473
                self.match(SparkSQLParser.INPUTFORMAT)
                self.state = 1474
                localctx.inFmt = self.match(SparkSQLParser.STRING)
                self.state = 1475
                self.match(SparkSQLParser.OUTPUTFORMAT)
                self.state = 1476
                localctx.outFmt = self.match(SparkSQLParser.STRING)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.GenericFileFormatContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1477
                self.identifier()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class StorageHandlerContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def WITH(self):
            return self.getToken(SparkSQLParser.WITH, 0)

        def SERDEPROPERTIES(self):
            return self.getToken(SparkSQLParser.SERDEPROPERTIES, 0)

        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_storageHandler

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStorageHandler" ):
                listener.enterStorageHandler(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStorageHandler" ):
                listener.exitStorageHandler(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStorageHandler" ):
                return visitor.visitStorageHandler(self)
            else:
                return visitor.visitChildren(self)




    def storageHandler(self):

        localctx = SparkSQLParser.StorageHandlerContext(self, self._ctx, self.state)
        self.enterRule(localctx, 72, self.RULE_storageHandler)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1480
            self.match(SparkSQLParser.STRING)
            self.state = 1484
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,160,self._ctx)
            if la_ == 1:
                self.state = 1481
                self.match(SparkSQLParser.WITH)
                self.state = 1482
                self.match(SparkSQLParser.SERDEPROPERTIES)
                self.state = 1483
                self.tablePropertyList()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ResourceContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_resource

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterResource" ):
                listener.enterResource(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitResource" ):
                listener.exitResource(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitResource" ):
                return visitor.visitResource(self)
            else:
                return visitor.visitChildren(self)




    def resource(self):

        localctx = SparkSQLParser.ResourceContext(self, self._ctx, self.state)
        self.enterRule(localctx, 74, self.RULE_resource)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1486
            self.identifier()
            self.state = 1487
            self.match(SparkSQLParser.STRING)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class DmlStatementNoWithContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_dmlStatementNoWith

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class DeleteFromTableContext(DmlStatementNoWithContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.DmlStatementNoWithContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DELETE(self):
            return self.getToken(SparkSQLParser.DELETE, 0)
        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,0)

        def whereClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WhereClauseContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDeleteFromTable" ):
                listener.enterDeleteFromTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDeleteFromTable" ):
                listener.exitDeleteFromTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDeleteFromTable" ):
                return visitor.visitDeleteFromTable(self)
            else:
                return visitor.visitChildren(self)


    class SingleInsertQueryContext(DmlStatementNoWithContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.DmlStatementNoWithContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def insertInto(self):
            return self.getTypedRuleContext(SparkSQLParser.InsertIntoContext,0)

        def queryTerm(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryTermContext,0)

        def queryOrganization(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryOrganizationContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleInsertQuery" ):
                listener.enterSingleInsertQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleInsertQuery" ):
                listener.exitSingleInsertQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleInsertQuery" ):
                return visitor.visitSingleInsertQuery(self)
            else:
                return visitor.visitChildren(self)


    class MultiInsertQueryContext(DmlStatementNoWithContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.DmlStatementNoWithContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def fromClause(self):
            return self.getTypedRuleContext(SparkSQLParser.FromClauseContext,0)

        def multiInsertQueryBody(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultiInsertQueryBodyContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultiInsertQueryBodyContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMultiInsertQuery" ):
                listener.enterMultiInsertQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMultiInsertQuery" ):
                listener.exitMultiInsertQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMultiInsertQuery" ):
                return visitor.visitMultiInsertQuery(self)
            else:
                return visitor.visitChildren(self)


    class UpdateTableContext(DmlStatementNoWithContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.DmlStatementNoWithContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def UPDATE(self):
            return self.getToken(SparkSQLParser.UPDATE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,0)

        def setClause(self):
            return self.getTypedRuleContext(SparkSQLParser.SetClauseContext,0)

        def whereClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WhereClauseContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUpdateTable" ):
                listener.enterUpdateTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUpdateTable" ):
                listener.exitUpdateTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUpdateTable" ):
                return visitor.visitUpdateTable(self)
            else:
                return visitor.visitChildren(self)


    class MergeIntoTableContext(DmlStatementNoWithContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.DmlStatementNoWithContext
            super().__init__(parser)
            self.target = None # MultipartIdentifierContext
            self.targetAlias = None # TableAliasContext
            self.source = None # MultipartIdentifierContext
            self.sourceQuery = None # QueryContext
            self.sourceAlias = None # TableAliasContext
            self.mergeCondition = None # BooleanExpressionContext
            self.copyFrom(ctx)

        def MERGE(self):
            return self.getToken(SparkSQLParser.MERGE, 0)
        def INTO(self):
            return self.getToken(SparkSQLParser.INTO, 0)
        def USING(self):
            return self.getToken(SparkSQLParser.USING, 0)
        def ON(self):
            return self.getToken(SparkSQLParser.ON, 0)
        def multipartIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultipartIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,i)

        def tableAlias(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TableAliasContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,i)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def matchedClause(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MatchedClauseContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MatchedClauseContext,i)

        def notMatchedClause(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.NotMatchedClauseContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.NotMatchedClauseContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMergeIntoTable" ):
                listener.enterMergeIntoTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMergeIntoTable" ):
                listener.exitMergeIntoTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMergeIntoTable" ):
                return visitor.visitMergeIntoTable(self)
            else:
                return visitor.visitChildren(self)



    def dmlStatementNoWith(self):

        localctx = SparkSQLParser.DmlStatementNoWithContext(self, self._ctx, self.state)
        self.enterRule(localctx, 76, self.RULE_dmlStatementNoWith)
        self._la = 0 # Token type
        try:
            self.state = 1540
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.INSERT]:
                localctx = SparkSQLParser.SingleInsertQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1489
                self.insertInto()
                self.state = 1490
                self.queryTerm(0)
                self.state = 1491
                self.queryOrganization()
                pass
            elif token in [SparkSQLParser.FROM]:
                localctx = SparkSQLParser.MultiInsertQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1493
                self.fromClause()
                self.state = 1495 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 1494
                    self.multiInsertQueryBody()
                    self.state = 1497 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSQLParser.INSERT):
                        break

                pass
            elif token in [SparkSQLParser.DELETE]:
                localctx = SparkSQLParser.DeleteFromTableContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 1499
                self.match(SparkSQLParser.DELETE)
                self.state = 1500
                self.match(SparkSQLParser.FROM)
                self.state = 1501
                self.multipartIdentifier()
                self.state = 1502
                self.tableAlias()
                self.state = 1504
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.WHERE:
                    self.state = 1503
                    self.whereClause()


                pass
            elif token in [SparkSQLParser.UPDATE]:
                localctx = SparkSQLParser.UpdateTableContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 1506
                self.match(SparkSQLParser.UPDATE)
                self.state = 1507
                self.multipartIdentifier()
                self.state = 1508
                self.tableAlias()
                self.state = 1509
                self.setClause()
                self.state = 1511
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.WHERE:
                    self.state = 1510
                    self.whereClause()


                pass
            elif token in [SparkSQLParser.MERGE]:
                localctx = SparkSQLParser.MergeIntoTableContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 1513
                self.match(SparkSQLParser.MERGE)
                self.state = 1514
                self.match(SparkSQLParser.INTO)
                self.state = 1515
                localctx.target = self.multipartIdentifier()
                self.state = 1516
                localctx.targetAlias = self.tableAlias()
                self.state = 1517
                self.match(SparkSQLParser.USING)
                self.state = 1523
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSQLParser.ADD, SparkSQLParser.AFTER, SparkSQLParser.ALL, SparkSQLParser.ALTER, SparkSQLParser.ANALYZE, SparkSQLParser.AND, SparkSQLParser.ANTI, SparkSQLParser.ANY, SparkSQLParser.ARCHIVE, SparkSQLParser.ARRAY, SparkSQLParser.AS, SparkSQLParser.ASC, SparkSQLParser.AT, SparkSQLParser.AUTHORIZATION, SparkSQLParser.BETWEEN, SparkSQLParser.BOTH, SparkSQLParser.BUCKET, SparkSQLParser.BUCKETS, SparkSQLParser.BY, SparkSQLParser.CACHE, SparkSQLParser.CASCADE, SparkSQLParser.CASE, SparkSQLParser.CAST, SparkSQLParser.CHANGE, SparkSQLParser.CHECK, SparkSQLParser.CLEAR, SparkSQLParser.CLUSTER, SparkSQLParser.CLUSTERED, SparkSQLParser.CODEGEN, SparkSQLParser.COLLATE, SparkSQLParser.COLLECTION, SparkSQLParser.COLUMN, SparkSQLParser.COLUMNS, SparkSQLParser.COMMENT, SparkSQLParser.COMMIT, SparkSQLParser.COMPACT, SparkSQLParser.COMPACTIONS, SparkSQLParser.COMPUTE, SparkSQLParser.CONCATENATE, SparkSQLParser.CONSTRAINT, SparkSQLParser.COST, SparkSQLParser.CREATE, SparkSQLParser.CROSS, SparkSQLParser.CUBE, SparkSQLParser.CURRENT, SparkSQLParser.CURRENT_DATE, SparkSQLParser.CURRENT_TIME, SparkSQLParser.CURRENT_TIMESTAMP, SparkSQLParser.CURRENT_USER, SparkSQLParser.DATA, SparkSQLParser.DATABASE, SparkSQLParser.DATABASES, SparkSQLParser.DBPROPERTIES, SparkSQLParser.DEFINED, SparkSQLParser.DELETE, SparkSQLParser.DELIMITED, SparkSQLParser.DESC, SparkSQLParser.DESCRIBE, SparkSQLParser.DFS, SparkSQLParser.DIRECTORIES, SparkSQLParser.DIRECTORY, SparkSQLParser.DISTINCT, SparkSQLParser.DISTRIBUTE, SparkSQLParser.DIV, SparkSQLParser.DROP, SparkSQLParser.ELSE, SparkSQLParser.END, SparkSQLParser.ESCAPE, SparkSQLParser.ESCAPED, SparkSQLParser.EXCEPT, SparkSQLParser.EXCHANGE, SparkSQLParser.EXISTS, SparkSQLParser.EXPLAIN, SparkSQLParser.EXPORT, SparkSQLParser.EXTENDED, SparkSQLParser.EXTERNAL, SparkSQLParser.EXTRACT, SparkSQLParser.FALSE, SparkSQLParser.FETCH, SparkSQLParser.FIELDS, SparkSQLParser.FILTER, SparkSQLParser.FILEFORMAT, SparkSQLParser.FIRST, SparkSQLParser.FOLLOWING, SparkSQLParser.FOR, SparkSQLParser.FOREIGN, SparkSQLParser.FORMAT, SparkSQLParser.FORMATTED, SparkSQLParser.FROM, SparkSQLParser.FULL, SparkSQLParser.FUNCTION, SparkSQLParser.FUNCTIONS, SparkSQLParser.GLOBAL, SparkSQLParser.GRANT, SparkSQLParser.GROUP, SparkSQLParser.GROUPING, SparkSQLParser.HAVING, SparkSQLParser.IF, SparkSQLParser.IGNORE, SparkSQLParser.IMPORT, SparkSQLParser.IN, SparkSQLParser.INDEX, SparkSQLParser.INDEXES, SparkSQLParser.INNER, SparkSQLParser.INPATH, SparkSQLParser.INPUTFORMAT, SparkSQLParser.INSERT, SparkSQLParser.INTERSECT, SparkSQLParser.INTERVAL, SparkSQLParser.INTO, SparkSQLParser.IS, SparkSQLParser.ITEMS, SparkSQLParser.JOIN, SparkSQLParser.KEYS, SparkSQLParser.LAST, SparkSQLParser.LATERAL, SparkSQLParser.LAZY, SparkSQLParser.LEADING, SparkSQLParser.LEFT, SparkSQLParser.LIKE, SparkSQLParser.LIMIT, SparkSQLParser.LINES, SparkSQLParser.LIST, SparkSQLParser.LOAD, SparkSQLParser.LOCAL, SparkSQLParser.LOCATION, SparkSQLParser.LOCK, SparkSQLParser.LOCKS, SparkSQLParser.LOGICAL, SparkSQLParser.MACRO, SparkSQLParser.MAP, SparkSQLParser.MATCHED, SparkSQLParser.MERGE, SparkSQLParser.MSCK, SparkSQLParser.NAMESPACE, SparkSQLParser.NAMESPACES, SparkSQLParser.NATURAL, SparkSQLParser.NO, SparkSQLParser.NOT, SparkSQLParser.NULL, SparkSQLParser.NULLS, SparkSQLParser.OF, SparkSQLParser.ON, SparkSQLParser.ONLY, SparkSQLParser.OPTION, SparkSQLParser.OPTIONS, SparkSQLParser.OR, SparkSQLParser.ORDER, SparkSQLParser.OUT, SparkSQLParser.OUTER, SparkSQLParser.OUTPUTFORMAT, SparkSQLParser.OVER, SparkSQLParser.OVERLAPS, SparkSQLParser.OVERLAY, SparkSQLParser.OVERWRITE, SparkSQLParser.PARTITION, SparkSQLParser.PARTITIONED, SparkSQLParser.PARTITIONS, SparkSQLParser.PERCENTLIT, SparkSQLParser.PIVOT, SparkSQLParser.PLACING, SparkSQLParser.POSITION, SparkSQLParser.PRECEDING, SparkSQLParser.PRIMARY, SparkSQLParser.PRINCIPALS, SparkSQLParser.PROPERTIES, SparkSQLParser.PURGE, SparkSQLParser.QUERY, SparkSQLParser.RANGE, SparkSQLParser.RECORDREADER, SparkSQLParser.RECORDWRITER, SparkSQLParser.RECOVER, SparkSQLParser.REDUCE, SparkSQLParser.REFERENCES, SparkSQLParser.REFRESH, SparkSQLParser.RENAME, SparkSQLParser.REPAIR, SparkSQLParser.REPLACE, SparkSQLParser.RESET, SparkSQLParser.RESTRICT, SparkSQLParser.REVOKE, SparkSQLParser.RIGHT, SparkSQLParser.RLIKE, SparkSQLParser.ROLE, SparkSQLParser.ROLES, SparkSQLParser.ROLLBACK, SparkSQLParser.ROLLUP, SparkSQLParser.ROW, SparkSQLParser.ROWS, SparkSQLParser.SCHEMA, SparkSQLParser.SELECT, SparkSQLParser.SEMI, SparkSQLParser.SEPARATED, SparkSQLParser.SERDE, SparkSQLParser.SERDEPROPERTIES, SparkSQLParser.SESSION_USER, SparkSQLParser.SET, SparkSQLParser.SETMINUS, SparkSQLParser.SETS, SparkSQLParser.SHOW, SparkSQLParser.SKEWED, SparkSQLParser.SOME, SparkSQLParser.SORT, SparkSQLParser.SORTED, SparkSQLParser.START, SparkSQLParser.STATISTICS, SparkSQLParser.STORED, SparkSQLParser.STRATIFY, SparkSQLParser.STRUCT, SparkSQLParser.SUBSTR, SparkSQLParser.SUBSTRING, SparkSQLParser.TABLE, SparkSQLParser.TABLES, SparkSQLParser.TABLESAMPLE, SparkSQLParser.TBLPROPERTIES, SparkSQLParser.TEMPORARY, SparkSQLParser.TERMINATED, SparkSQLParser.THEN, SparkSQLParser.TIME, SparkSQLParser.TO, SparkSQLParser.TOUCH, SparkSQLParser.TRAILING, SparkSQLParser.TRANSACTION, SparkSQLParser.TRANSACTIONS, SparkSQLParser.TRANSFORM, SparkSQLParser.TRIM, SparkSQLParser.TRUE, SparkSQLParser.TRUNCATE, SparkSQLParser.TYPE, SparkSQLParser.UNARCHIVE, SparkSQLParser.UNBOUNDED, SparkSQLParser.UNCACHE, SparkSQLParser.UNION, SparkSQLParser.UNIQUE, SparkSQLParser.UNKNOWN, SparkSQLParser.UNLOCK, SparkSQLParser.UNSET, SparkSQLParser.UPDATE, SparkSQLParser.USE, SparkSQLParser.USER, SparkSQLParser.USING, SparkSQLParser.VALUES, SparkSQLParser.VIEW, SparkSQLParser.VIEWS, SparkSQLParser.WHEN, SparkSQLParser.WHERE, SparkSQLParser.WINDOW, SparkSQLParser.WITH, SparkSQLParser.ZONE, SparkSQLParser.IDENTIFIER, SparkSQLParser.BACKQUOTED_IDENTIFIER]:
                    self.state = 1518
                    localctx.source = self.multipartIdentifier()
                    pass
                elif token in [SparkSQLParser.T__1]:
                    self.state = 1519
                    self.match(SparkSQLParser.T__1)
                    self.state = 1520
                    localctx.sourceQuery = self.query()
                    self.state = 1521
                    self.match(SparkSQLParser.T__2)
                    pass
                else:
                    raise NoViableAltException(self)

                self.state = 1525
                localctx.sourceAlias = self.tableAlias()
                self.state = 1526
                self.match(SparkSQLParser.ON)
                self.state = 1527
                localctx.mergeCondition = self.booleanExpression(0)
                self.state = 1531
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,165,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1528
                        self.matchedClause() 
                    self.state = 1533
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,165,self._ctx)

                self.state = 1537
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.WHEN:
                    self.state = 1534
                    self.notMatchedClause()
                    self.state = 1539
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QueryOrganizationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._sortItem = None # SortItemContext
            self.order = list() # of SortItemContexts
            self._expression = None # ExpressionContext
            self.clusterBy = list() # of ExpressionContexts
            self.distributeBy = list() # of ExpressionContexts
            self.sort = list() # of SortItemContexts
            self.limit = None # ExpressionContext

        def ORDER(self):
            return self.getToken(SparkSQLParser.ORDER, 0)

        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.BY)
            else:
                return self.getToken(SparkSQLParser.BY, i)

        def CLUSTER(self):
            return self.getToken(SparkSQLParser.CLUSTER, 0)

        def DISTRIBUTE(self):
            return self.getToken(SparkSQLParser.DISTRIBUTE, 0)

        def SORT(self):
            return self.getToken(SparkSQLParser.SORT, 0)

        def windowClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WindowClauseContext,0)


        def LIMIT(self):
            return self.getToken(SparkSQLParser.LIMIT, 0)

        def sortItem(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.SortItemContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.SortItemContext,i)


        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def ALL(self):
            return self.getToken(SparkSQLParser.ALL, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_queryOrganization

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQueryOrganization" ):
                listener.enterQueryOrganization(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQueryOrganization" ):
                listener.exitQueryOrganization(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQueryOrganization" ):
                return visitor.visitQueryOrganization(self)
            else:
                return visitor.visitChildren(self)




    def queryOrganization(self):

        localctx = SparkSQLParser.QueryOrganizationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 78, self.RULE_queryOrganization)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1552
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,169,self._ctx)
            if la_ == 1:
                self.state = 1542
                self.match(SparkSQLParser.ORDER)
                self.state = 1543
                self.match(SparkSQLParser.BY)
                self.state = 1544
                localctx._sortItem = self.sortItem()
                localctx.order.append(localctx._sortItem)
                self.state = 1549
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,168,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1545
                        self.match(SparkSQLParser.T__3)
                        self.state = 1546
                        localctx._sortItem = self.sortItem()
                        localctx.order.append(localctx._sortItem) 
                    self.state = 1551
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,168,self._ctx)



            self.state = 1564
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,171,self._ctx)
            if la_ == 1:
                self.state = 1554
                self.match(SparkSQLParser.CLUSTER)
                self.state = 1555
                self.match(SparkSQLParser.BY)
                self.state = 1556
                localctx._expression = self.expression()
                localctx.clusterBy.append(localctx._expression)
                self.state = 1561
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,170,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1557
                        self.match(SparkSQLParser.T__3)
                        self.state = 1558
                        localctx._expression = self.expression()
                        localctx.clusterBy.append(localctx._expression) 
                    self.state = 1563
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,170,self._ctx)



            self.state = 1576
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,173,self._ctx)
            if la_ == 1:
                self.state = 1566
                self.match(SparkSQLParser.DISTRIBUTE)
                self.state = 1567
                self.match(SparkSQLParser.BY)
                self.state = 1568
                localctx._expression = self.expression()
                localctx.distributeBy.append(localctx._expression)
                self.state = 1573
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,172,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1569
                        self.match(SparkSQLParser.T__3)
                        self.state = 1570
                        localctx._expression = self.expression()
                        localctx.distributeBy.append(localctx._expression) 
                    self.state = 1575
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,172,self._ctx)



            self.state = 1588
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,175,self._ctx)
            if la_ == 1:
                self.state = 1578
                self.match(SparkSQLParser.SORT)
                self.state = 1579
                self.match(SparkSQLParser.BY)
                self.state = 1580
                localctx._sortItem = self.sortItem()
                localctx.sort.append(localctx._sortItem)
                self.state = 1585
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,174,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1581
                        self.match(SparkSQLParser.T__3)
                        self.state = 1582
                        localctx._sortItem = self.sortItem()
                        localctx.sort.append(localctx._sortItem) 
                    self.state = 1587
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,174,self._ctx)



            self.state = 1591
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,176,self._ctx)
            if la_ == 1:
                self.state = 1590
                self.windowClause()


            self.state = 1598
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,178,self._ctx)
            if la_ == 1:
                self.state = 1593
                self.match(SparkSQLParser.LIMIT)
                self.state = 1596
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,177,self._ctx)
                if la_ == 1:
                    self.state = 1594
                    self.match(SparkSQLParser.ALL)
                    pass

                elif la_ == 2:
                    self.state = 1595
                    localctx.limit = self.expression()
                    pass




        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class MultiInsertQueryBodyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def insertInto(self):
            return self.getTypedRuleContext(SparkSQLParser.InsertIntoContext,0)


        def fromStatementBody(self):
            return self.getTypedRuleContext(SparkSQLParser.FromStatementBodyContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_multiInsertQueryBody

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMultiInsertQueryBody" ):
                listener.enterMultiInsertQueryBody(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMultiInsertQueryBody" ):
                listener.exitMultiInsertQueryBody(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMultiInsertQueryBody" ):
                return visitor.visitMultiInsertQueryBody(self)
            else:
                return visitor.visitChildren(self)




    def multiInsertQueryBody(self):

        localctx = SparkSQLParser.MultiInsertQueryBodyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 80, self.RULE_multiInsertQueryBody)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1600
            self.insertInto()
            self.state = 1601
            self.fromStatementBody()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QueryTermContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_queryTerm

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class QueryTermDefaultContext(QueryTermContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QueryTermContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def queryPrimary(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryPrimaryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQueryTermDefault" ):
                listener.enterQueryTermDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQueryTermDefault" ):
                listener.exitQueryTermDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQueryTermDefault" ):
                return visitor.visitQueryTermDefault(self)
            else:
                return visitor.visitChildren(self)


    class SetOperationContext(QueryTermContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QueryTermContext
            super().__init__(parser)
            self.left = None # QueryTermContext
            self.operator = None # Token
            self.right = None # QueryTermContext
            self.copyFrom(ctx)

        def queryTerm(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.QueryTermContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.QueryTermContext,i)

        def INTERSECT(self):
            return self.getToken(SparkSQLParser.INTERSECT, 0)
        def setQuantifier(self):
            return self.getTypedRuleContext(SparkSQLParser.SetQuantifierContext,0)

        def UNION(self):
            return self.getToken(SparkSQLParser.UNION, 0)
        def EXCEPT(self):
            return self.getToken(SparkSQLParser.EXCEPT, 0)
        def SETMINUS(self):
            return self.getToken(SparkSQLParser.SETMINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetOperation" ):
                listener.enterSetOperation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetOperation" ):
                listener.exitSetOperation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetOperation" ):
                return visitor.visitSetOperation(self)
            else:
                return visitor.visitChildren(self)



    def queryTerm(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSQLParser.QueryTermContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 82
        self.enterRecursionRule(localctx, 82, self.RULE_queryTerm, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            localctx = SparkSQLParser.QueryTermDefaultContext(self, localctx)
            self._ctx = localctx
            _prevctx = localctx

            self.state = 1604
            self.queryPrimary()
            self._ctx.stop = self._input.LT(-1)
            self.state = 1620
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,182,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 1618
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,181,self._ctx)
                    if la_ == 1:
                        localctx = SparkSQLParser.SetOperationContext(self, SparkSQLParser.QueryTermContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_queryTerm)
                        self.state = 1606
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 1607
                        localctx.operator = self.match(SparkSQLParser.INTERSECT)
                        self.state = 1609
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        if _la==SparkSQLParser.ALL or _la==SparkSQLParser.DISTINCT:
                            self.state = 1608
                            self.setQuantifier()


                        self.state = 1611
                        localctx.right = self.queryTerm(3)
                        pass

                    elif la_ == 2:
                        localctx = SparkSQLParser.SetOperationContext(self, SparkSQLParser.QueryTermContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_queryTerm)
                        self.state = 1612
                        if not self.precpred(self._ctx, 1):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
                        self.state = 1613
                        localctx.operator = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(_la==SparkSQLParser.EXCEPT or _la==SparkSQLParser.SETMINUS or _la==SparkSQLParser.UNION):
                            localctx.operator = self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 1615
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        if _la==SparkSQLParser.ALL or _la==SparkSQLParser.DISTINCT:
                            self.state = 1614
                            self.setQuantifier()


                        self.state = 1617
                        localctx.right = self.queryTerm(2)
                        pass

             
                self.state = 1622
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,182,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx


    class QueryPrimaryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_queryPrimary

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class SubqueryContext(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubquery" ):
                listener.enterSubquery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubquery" ):
                listener.exitSubquery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubquery" ):
                return visitor.visitSubquery(self)
            else:
                return visitor.visitChildren(self)


    class QueryPrimaryDefaultContext(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def querySpecification(self):
            return self.getTypedRuleContext(SparkSQLParser.QuerySpecificationContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQueryPrimaryDefault" ):
                listener.enterQueryPrimaryDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQueryPrimaryDefault" ):
                listener.exitQueryPrimaryDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQueryPrimaryDefault" ):
                return visitor.visitQueryPrimaryDefault(self)
            else:
                return visitor.visitChildren(self)


    class InlineTableDefault1Context(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def inlineTable(self):
            return self.getTypedRuleContext(SparkSQLParser.InlineTableContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInlineTableDefault1" ):
                listener.enterInlineTableDefault1(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInlineTableDefault1" ):
                listener.exitInlineTableDefault1(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInlineTableDefault1" ):
                return visitor.visitInlineTableDefault1(self)
            else:
                return visitor.visitChildren(self)


    class FromStmtContext(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def fromStatement(self):
            return self.getTypedRuleContext(SparkSQLParser.FromStatementContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFromStmt" ):
                listener.enterFromStmt(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFromStmt" ):
                listener.exitFromStmt(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFromStmt" ):
                return visitor.visitFromStmt(self)
            else:
                return visitor.visitChildren(self)


    class TableContext(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)
        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable" ):
                listener.enterTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable" ):
                listener.exitTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable" ):
                return visitor.visitTable(self)
            else:
                return visitor.visitChildren(self)



    def queryPrimary(self):

        localctx = SparkSQLParser.QueryPrimaryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 84, self.RULE_queryPrimary)
        try:
            self.state = 1632
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.MAP, SparkSQLParser.REDUCE, SparkSQLParser.SELECT]:
                localctx = SparkSQLParser.QueryPrimaryDefaultContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1623
                self.querySpecification()
                pass
            elif token in [SparkSQLParser.FROM]:
                localctx = SparkSQLParser.FromStmtContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1624
                self.fromStatement()
                pass
            elif token in [SparkSQLParser.TABLE]:
                localctx = SparkSQLParser.TableContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 1625
                self.match(SparkSQLParser.TABLE)
                self.state = 1626
                self.multipartIdentifier()
                pass
            elif token in [SparkSQLParser.VALUES]:
                localctx = SparkSQLParser.InlineTableDefault1Context(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 1627
                self.inlineTable()
                pass
            elif token in [SparkSQLParser.T__1]:
                localctx = SparkSQLParser.SubqueryContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 1628
                self.match(SparkSQLParser.T__1)
                self.state = 1629
                self.query()
                self.state = 1630
                self.match(SparkSQLParser.T__2)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SortItemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.ordering = None # Token
            self.nullOrder = None # Token

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def NULLS(self):
            return self.getToken(SparkSQLParser.NULLS, 0)

        def ASC(self):
            return self.getToken(SparkSQLParser.ASC, 0)

        def DESC(self):
            return self.getToken(SparkSQLParser.DESC, 0)

        def LAST(self):
            return self.getToken(SparkSQLParser.LAST, 0)

        def FIRST(self):
            return self.getToken(SparkSQLParser.FIRST, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_sortItem

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSortItem" ):
                listener.enterSortItem(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSortItem" ):
                listener.exitSortItem(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSortItem" ):
                return visitor.visitSortItem(self)
            else:
                return visitor.visitChildren(self)




    def sortItem(self):

        localctx = SparkSQLParser.SortItemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 86, self.RULE_sortItem)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1634
            self.expression()
            self.state = 1636
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,184,self._ctx)
            if la_ == 1:
                self.state = 1635
                localctx.ordering = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.ASC or _la==SparkSQLParser.DESC):
                    localctx.ordering = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()


            self.state = 1640
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,185,self._ctx)
            if la_ == 1:
                self.state = 1638
                self.match(SparkSQLParser.NULLS)
                self.state = 1639
                localctx.nullOrder = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.FIRST or _la==SparkSQLParser.LAST):
                    localctx.nullOrder = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FromStatementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def fromClause(self):
            return self.getTypedRuleContext(SparkSQLParser.FromClauseContext,0)


        def fromStatementBody(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.FromStatementBodyContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.FromStatementBodyContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_fromStatement

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFromStatement" ):
                listener.enterFromStatement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFromStatement" ):
                listener.exitFromStatement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFromStatement" ):
                return visitor.visitFromStatement(self)
            else:
                return visitor.visitChildren(self)




    def fromStatement(self):

        localctx = SparkSQLParser.FromStatementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 88, self.RULE_fromStatement)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1642
            self.fromClause()
            self.state = 1644 
            self._errHandler.sync(self)
            _alt = 1
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt == 1:
                    self.state = 1643
                    self.fromStatementBody()

                else:
                    raise NoViableAltException(self)
                self.state = 1646 
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,186,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FromStatementBodyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def transformClause(self):
            return self.getTypedRuleContext(SparkSQLParser.TransformClauseContext,0)


        def queryOrganization(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryOrganizationContext,0)


        def whereClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WhereClauseContext,0)


        def selectClause(self):
            return self.getTypedRuleContext(SparkSQLParser.SelectClauseContext,0)


        def lateralView(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.LateralViewContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.LateralViewContext,i)


        def aggregationClause(self):
            return self.getTypedRuleContext(SparkSQLParser.AggregationClauseContext,0)


        def havingClause(self):
            return self.getTypedRuleContext(SparkSQLParser.HavingClauseContext,0)


        def windowClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WindowClauseContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_fromStatementBody

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFromStatementBody" ):
                listener.enterFromStatementBody(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFromStatementBody" ):
                listener.exitFromStatementBody(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFromStatementBody" ):
                return visitor.visitFromStatementBody(self)
            else:
                return visitor.visitChildren(self)




    def fromStatementBody(self):

        localctx = SparkSQLParser.FromStatementBodyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 90, self.RULE_fromStatementBody)
        try:
            self.state = 1675
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,193,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1648
                self.transformClause()
                self.state = 1650
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,187,self._ctx)
                if la_ == 1:
                    self.state = 1649
                    self.whereClause()


                self.state = 1652
                self.queryOrganization()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1654
                self.selectClause()
                self.state = 1658
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,188,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1655
                        self.lateralView() 
                    self.state = 1660
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,188,self._ctx)

                self.state = 1662
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,189,self._ctx)
                if la_ == 1:
                    self.state = 1661
                    self.whereClause()


                self.state = 1665
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,190,self._ctx)
                if la_ == 1:
                    self.state = 1664
                    self.aggregationClause()


                self.state = 1668
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,191,self._ctx)
                if la_ == 1:
                    self.state = 1667
                    self.havingClause()


                self.state = 1671
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,192,self._ctx)
                if la_ == 1:
                    self.state = 1670
                    self.windowClause()


                self.state = 1673
                self.queryOrganization()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QuerySpecificationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_querySpecification

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class RegularQuerySpecificationContext(QuerySpecificationContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QuerySpecificationContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def selectClause(self):
            return self.getTypedRuleContext(SparkSQLParser.SelectClauseContext,0)

        def fromClause(self):
            return self.getTypedRuleContext(SparkSQLParser.FromClauseContext,0)

        def lateralView(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.LateralViewContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.LateralViewContext,i)

        def whereClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WhereClauseContext,0)

        def aggregationClause(self):
            return self.getTypedRuleContext(SparkSQLParser.AggregationClauseContext,0)

        def havingClause(self):
            return self.getTypedRuleContext(SparkSQLParser.HavingClauseContext,0)

        def windowClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WindowClauseContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRegularQuerySpecification" ):
                listener.enterRegularQuerySpecification(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRegularQuerySpecification" ):
                listener.exitRegularQuerySpecification(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRegularQuerySpecification" ):
                return visitor.visitRegularQuerySpecification(self)
            else:
                return visitor.visitChildren(self)


    class TransformQuerySpecificationContext(QuerySpecificationContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.QuerySpecificationContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def transformClause(self):
            return self.getTypedRuleContext(SparkSQLParser.TransformClauseContext,0)

        def fromClause(self):
            return self.getTypedRuleContext(SparkSQLParser.FromClauseContext,0)

        def whereClause(self):
            return self.getTypedRuleContext(SparkSQLParser.WhereClauseContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTransformQuerySpecification" ):
                listener.enterTransformQuerySpecification(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTransformQuerySpecification" ):
                listener.exitTransformQuerySpecification(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTransformQuerySpecification" ):
                return visitor.visitTransformQuerySpecification(self)
            else:
                return visitor.visitChildren(self)



    def querySpecification(self):

        localctx = SparkSQLParser.QuerySpecificationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 92, self.RULE_querySpecification)
        try:
            self.state = 1706
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,202,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.TransformQuerySpecificationContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1677
                self.transformClause()
                self.state = 1679
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,194,self._ctx)
                if la_ == 1:
                    self.state = 1678
                    self.fromClause()


                self.state = 1682
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,195,self._ctx)
                if la_ == 1:
                    self.state = 1681
                    self.whereClause()


                pass

            elif la_ == 2:
                localctx = SparkSQLParser.RegularQuerySpecificationContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1684
                self.selectClause()
                self.state = 1686
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,196,self._ctx)
                if la_ == 1:
                    self.state = 1685
                    self.fromClause()


                self.state = 1691
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,197,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1688
                        self.lateralView() 
                    self.state = 1693
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,197,self._ctx)

                self.state = 1695
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,198,self._ctx)
                if la_ == 1:
                    self.state = 1694
                    self.whereClause()


                self.state = 1698
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,199,self._ctx)
                if la_ == 1:
                    self.state = 1697
                    self.aggregationClause()


                self.state = 1701
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,200,self._ctx)
                if la_ == 1:
                    self.state = 1700
                    self.havingClause()


                self.state = 1704
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,201,self._ctx)
                if la_ == 1:
                    self.state = 1703
                    self.windowClause()


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TransformClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.kind = None # Token
            self.inRowFormat = None # RowFormatContext
            self.recordWriter = None # Token
            self.script = None # Token
            self.outRowFormat = None # RowFormatContext
            self.recordReader = None # Token

        def USING(self):
            return self.getToken(SparkSQLParser.USING, 0)

        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.STRING)
            else:
                return self.getToken(SparkSQLParser.STRING, i)

        def SELECT(self):
            return self.getToken(SparkSQLParser.SELECT, 0)

        def namedExpressionSeq(self):
            return self.getTypedRuleContext(SparkSQLParser.NamedExpressionSeqContext,0)


        def TRANSFORM(self):
            return self.getToken(SparkSQLParser.TRANSFORM, 0)

        def MAP(self):
            return self.getToken(SparkSQLParser.MAP, 0)

        def REDUCE(self):
            return self.getToken(SparkSQLParser.REDUCE, 0)

        def RECORDWRITER(self):
            return self.getToken(SparkSQLParser.RECORDWRITER, 0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def RECORDREADER(self):
            return self.getToken(SparkSQLParser.RECORDREADER, 0)

        def rowFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.RowFormatContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.RowFormatContext,i)


        def identifierSeq(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierSeqContext,0)


        def colTypeList(self):
            return self.getTypedRuleContext(SparkSQLParser.ColTypeListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_transformClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTransformClause" ):
                listener.enterTransformClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTransformClause" ):
                listener.exitTransformClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTransformClause" ):
                return visitor.visitTransformClause(self)
            else:
                return visitor.visitChildren(self)




    def transformClause(self):

        localctx = SparkSQLParser.TransformClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 94, self.RULE_transformClause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1718
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.SELECT]:
                self.state = 1708
                self.match(SparkSQLParser.SELECT)
                self.state = 1709
                localctx.kind = self.match(SparkSQLParser.TRANSFORM)
                self.state = 1710
                self.match(SparkSQLParser.T__1)
                self.state = 1711
                self.namedExpressionSeq()
                self.state = 1712
                self.match(SparkSQLParser.T__2)
                pass
            elif token in [SparkSQLParser.MAP]:
                self.state = 1714
                localctx.kind = self.match(SparkSQLParser.MAP)
                self.state = 1715
                self.namedExpressionSeq()
                pass
            elif token in [SparkSQLParser.REDUCE]:
                self.state = 1716
                localctx.kind = self.match(SparkSQLParser.REDUCE)
                self.state = 1717
                self.namedExpressionSeq()
                pass
            else:
                raise NoViableAltException(self)

            self.state = 1721
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.ROW:
                self.state = 1720
                localctx.inRowFormat = self.rowFormat()


            self.state = 1725
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.RECORDWRITER:
                self.state = 1723
                self.match(SparkSQLParser.RECORDWRITER)
                self.state = 1724
                localctx.recordWriter = self.match(SparkSQLParser.STRING)


            self.state = 1727
            self.match(SparkSQLParser.USING)
            self.state = 1728
            localctx.script = self.match(SparkSQLParser.STRING)
            self.state = 1741
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,208,self._ctx)
            if la_ == 1:
                self.state = 1729
                self.match(SparkSQLParser.AS)
                self.state = 1739
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,207,self._ctx)
                if la_ == 1:
                    self.state = 1730
                    self.identifierSeq()
                    pass

                elif la_ == 2:
                    self.state = 1731
                    self.colTypeList()
                    pass

                elif la_ == 3:
                    self.state = 1732
                    self.match(SparkSQLParser.T__1)
                    self.state = 1735
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,206,self._ctx)
                    if la_ == 1:
                        self.state = 1733
                        self.identifierSeq()
                        pass

                    elif la_ == 2:
                        self.state = 1734
                        self.colTypeList()
                        pass


                    self.state = 1737
                    self.match(SparkSQLParser.T__2)
                    pass




            self.state = 1744
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,209,self._ctx)
            if la_ == 1:
                self.state = 1743
                localctx.outRowFormat = self.rowFormat()


            self.state = 1748
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,210,self._ctx)
            if la_ == 1:
                self.state = 1746
                self.match(SparkSQLParser.RECORDREADER)
                self.state = 1747
                localctx.recordReader = self.match(SparkSQLParser.STRING)


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SelectClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._hint = None # HintContext
            self.hints = list() # of HintContexts

        def SELECT(self):
            return self.getToken(SparkSQLParser.SELECT, 0)

        def namedExpressionSeq(self):
            return self.getTypedRuleContext(SparkSQLParser.NamedExpressionSeqContext,0)


        def setQuantifier(self):
            return self.getTypedRuleContext(SparkSQLParser.SetQuantifierContext,0)


        def hint(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.HintContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.HintContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_selectClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSelectClause" ):
                listener.enterSelectClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSelectClause" ):
                listener.exitSelectClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSelectClause" ):
                return visitor.visitSelectClause(self)
            else:
                return visitor.visitChildren(self)




    def selectClause(self):

        localctx = SparkSQLParser.SelectClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 96, self.RULE_selectClause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1750
            self.match(SparkSQLParser.SELECT)
            self.state = 1754
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__5:
                self.state = 1751
                localctx._hint = self.hint()
                localctx.hints.append(localctx._hint)
                self.state = 1756
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1758
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,212,self._ctx)
            if la_ == 1:
                self.state = 1757
                self.setQuantifier()


            self.state = 1760
            self.namedExpressionSeq()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SetClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)

        def assignmentList(self):
            return self.getTypedRuleContext(SparkSQLParser.AssignmentListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_setClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetClause" ):
                listener.enterSetClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetClause" ):
                listener.exitSetClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetClause" ):
                return visitor.visitSetClause(self)
            else:
                return visitor.visitChildren(self)




    def setClause(self):

        localctx = SparkSQLParser.SetClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 98, self.RULE_setClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1762
            self.match(SparkSQLParser.SET)
            self.state = 1763
            self.assignmentList()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class MatchedClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.matchedCond = None # BooleanExpressionContext

        def WHEN(self):
            return self.getToken(SparkSQLParser.WHEN, 0)

        def MATCHED(self):
            return self.getToken(SparkSQLParser.MATCHED, 0)

        def THEN(self):
            return self.getToken(SparkSQLParser.THEN, 0)

        def matchedAction(self):
            return self.getTypedRuleContext(SparkSQLParser.MatchedActionContext,0)


        def AND(self):
            return self.getToken(SparkSQLParser.AND, 0)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_matchedClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMatchedClause" ):
                listener.enterMatchedClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMatchedClause" ):
                listener.exitMatchedClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMatchedClause" ):
                return visitor.visitMatchedClause(self)
            else:
                return visitor.visitChildren(self)




    def matchedClause(self):

        localctx = SparkSQLParser.MatchedClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 100, self.RULE_matchedClause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1765
            self.match(SparkSQLParser.WHEN)
            self.state = 1766
            self.match(SparkSQLParser.MATCHED)
            self.state = 1769
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.AND:
                self.state = 1767
                self.match(SparkSQLParser.AND)
                self.state = 1768
                localctx.matchedCond = self.booleanExpression(0)


            self.state = 1771
            self.match(SparkSQLParser.THEN)
            self.state = 1772
            self.matchedAction()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NotMatchedClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.notMatchedCond = None # BooleanExpressionContext

        def WHEN(self):
            return self.getToken(SparkSQLParser.WHEN, 0)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def MATCHED(self):
            return self.getToken(SparkSQLParser.MATCHED, 0)

        def THEN(self):
            return self.getToken(SparkSQLParser.THEN, 0)

        def notMatchedAction(self):
            return self.getTypedRuleContext(SparkSQLParser.NotMatchedActionContext,0)


        def AND(self):
            return self.getToken(SparkSQLParser.AND, 0)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_notMatchedClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNotMatchedClause" ):
                listener.enterNotMatchedClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNotMatchedClause" ):
                listener.exitNotMatchedClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNotMatchedClause" ):
                return visitor.visitNotMatchedClause(self)
            else:
                return visitor.visitChildren(self)




    def notMatchedClause(self):

        localctx = SparkSQLParser.NotMatchedClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 102, self.RULE_notMatchedClause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1774
            self.match(SparkSQLParser.WHEN)
            self.state = 1775
            self.match(SparkSQLParser.NOT)
            self.state = 1776
            self.match(SparkSQLParser.MATCHED)
            self.state = 1779
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.AND:
                self.state = 1777
                self.match(SparkSQLParser.AND)
                self.state = 1778
                localctx.notMatchedCond = self.booleanExpression(0)


            self.state = 1781
            self.match(SparkSQLParser.THEN)
            self.state = 1782
            self.notMatchedAction()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class MatchedActionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def DELETE(self):
            return self.getToken(SparkSQLParser.DELETE, 0)

        def UPDATE(self):
            return self.getToken(SparkSQLParser.UPDATE, 0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)

        def ASTERISK(self):
            return self.getToken(SparkSQLParser.ASTERISK, 0)

        def assignmentList(self):
            return self.getTypedRuleContext(SparkSQLParser.AssignmentListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_matchedAction

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMatchedAction" ):
                listener.enterMatchedAction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMatchedAction" ):
                listener.exitMatchedAction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMatchedAction" ):
                return visitor.visitMatchedAction(self)
            else:
                return visitor.visitChildren(self)




    def matchedAction(self):

        localctx = SparkSQLParser.MatchedActionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 104, self.RULE_matchedAction)
        try:
            self.state = 1791
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,215,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1784
                self.match(SparkSQLParser.DELETE)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1785
                self.match(SparkSQLParser.UPDATE)
                self.state = 1786
                self.match(SparkSQLParser.SET)
                self.state = 1787
                self.match(SparkSQLParser.ASTERISK)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 1788
                self.match(SparkSQLParser.UPDATE)
                self.state = 1789
                self.match(SparkSQLParser.SET)
                self.state = 1790
                self.assignmentList()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NotMatchedActionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.columns = None # MultipartIdentifierListContext

        def INSERT(self):
            return self.getToken(SparkSQLParser.INSERT, 0)

        def ASTERISK(self):
            return self.getToken(SparkSQLParser.ASTERISK, 0)

        def VALUES(self):
            return self.getToken(SparkSQLParser.VALUES, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def multipartIdentifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_notMatchedAction

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNotMatchedAction" ):
                listener.enterNotMatchedAction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNotMatchedAction" ):
                listener.exitNotMatchedAction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNotMatchedAction" ):
                return visitor.visitNotMatchedAction(self)
            else:
                return visitor.visitChildren(self)




    def notMatchedAction(self):

        localctx = SparkSQLParser.NotMatchedActionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 106, self.RULE_notMatchedAction)
        self._la = 0 # Token type
        try:
            self.state = 1811
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,217,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1793
                self.match(SparkSQLParser.INSERT)
                self.state = 1794
                self.match(SparkSQLParser.ASTERISK)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1795
                self.match(SparkSQLParser.INSERT)
                self.state = 1796
                self.match(SparkSQLParser.T__1)
                self.state = 1797
                localctx.columns = self.multipartIdentifierList()
                self.state = 1798
                self.match(SparkSQLParser.T__2)
                self.state = 1799
                self.match(SparkSQLParser.VALUES)
                self.state = 1800
                self.match(SparkSQLParser.T__1)
                self.state = 1801
                self.expression()
                self.state = 1806
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 1802
                    self.match(SparkSQLParser.T__3)
                    self.state = 1803
                    self.expression()
                    self.state = 1808
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1809
                self.match(SparkSQLParser.T__2)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class AssignmentListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def assignment(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.AssignmentContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.AssignmentContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_assignmentList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAssignmentList" ):
                listener.enterAssignmentList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAssignmentList" ):
                listener.exitAssignmentList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAssignmentList" ):
                return visitor.visitAssignmentList(self)
            else:
                return visitor.visitChildren(self)




    def assignmentList(self):

        localctx = SparkSQLParser.AssignmentListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 108, self.RULE_assignmentList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1813
            self.assignment()
            self.state = 1818
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 1814
                self.match(SparkSQLParser.T__3)
                self.state = 1815
                self.assignment()
                self.state = 1820
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class AssignmentContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.key = None # MultipartIdentifierContext
            self.value = None # ExpressionContext

        def EQ(self):
            return self.getToken(SparkSQLParser.EQ, 0)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_assignment

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAssignment" ):
                listener.enterAssignment(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAssignment" ):
                listener.exitAssignment(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAssignment" ):
                return visitor.visitAssignment(self)
            else:
                return visitor.visitChildren(self)




    def assignment(self):

        localctx = SparkSQLParser.AssignmentContext(self, self._ctx, self.state)
        self.enterRule(localctx, 110, self.RULE_assignment)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1821
            localctx.key = self.multipartIdentifier()
            self.state = 1822
            self.match(SparkSQLParser.EQ)
            self.state = 1823
            localctx.value = self.expression()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class WhereClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def WHERE(self):
            return self.getToken(SparkSQLParser.WHERE, 0)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_whereClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWhereClause" ):
                listener.enterWhereClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWhereClause" ):
                listener.exitWhereClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWhereClause" ):
                return visitor.visitWhereClause(self)
            else:
                return visitor.visitChildren(self)




    def whereClause(self):

        localctx = SparkSQLParser.WhereClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 112, self.RULE_whereClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1825
            self.match(SparkSQLParser.WHERE)
            self.state = 1826
            self.booleanExpression(0)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class HavingClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def HAVING(self):
            return self.getToken(SparkSQLParser.HAVING, 0)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_havingClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterHavingClause" ):
                listener.enterHavingClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitHavingClause" ):
                listener.exitHavingClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitHavingClause" ):
                return visitor.visitHavingClause(self)
            else:
                return visitor.visitChildren(self)




    def havingClause(self):

        localctx = SparkSQLParser.HavingClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 114, self.RULE_havingClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1828
            self.match(SparkSQLParser.HAVING)
            self.state = 1829
            self.booleanExpression(0)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class HintContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._hintStatement = None # HintStatementContext
            self.hintStatements = list() # of HintStatementContexts

        def hintStatement(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.HintStatementContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.HintStatementContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_hint

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterHint" ):
                listener.enterHint(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitHint" ):
                listener.exitHint(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitHint" ):
                return visitor.visitHint(self)
            else:
                return visitor.visitChildren(self)




    def hint(self):

        localctx = SparkSQLParser.HintContext(self, self._ctx, self.state)
        self.enterRule(localctx, 116, self.RULE_hint)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1831
            self.match(SparkSQLParser.T__5)
            self.state = 1832
            localctx._hintStatement = self.hintStatement()
            localctx.hintStatements.append(localctx._hintStatement)
            self.state = 1839
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.T__3) | (1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                self.state = 1834
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__3:
                    self.state = 1833
                    self.match(SparkSQLParser.T__3)


                self.state = 1836
                localctx._hintStatement = self.hintStatement()
                localctx.hintStatements.append(localctx._hintStatement)
                self.state = 1841
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1842
            self.match(SparkSQLParser.T__6)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class HintStatementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.hintName = None # IdentifierContext
            self._primaryExpression = None # PrimaryExpressionContext
            self.parameters = list() # of PrimaryExpressionContexts

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def primaryExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.PrimaryExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.PrimaryExpressionContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_hintStatement

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterHintStatement" ):
                listener.enterHintStatement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitHintStatement" ):
                listener.exitHintStatement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitHintStatement" ):
                return visitor.visitHintStatement(self)
            else:
                return visitor.visitChildren(self)




    def hintStatement(self):

        localctx = SparkSQLParser.HintStatementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 118, self.RULE_hintStatement)
        self._la = 0 # Token type
        try:
            self.state = 1857
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,222,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1844
                localctx.hintName = self.identifier()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1845
                localctx.hintName = self.identifier()
                self.state = 1846
                self.match(SparkSQLParser.T__1)
                self.state = 1847
                localctx._primaryExpression = self.primaryExpression(0)
                localctx.parameters.append(localctx._primaryExpression)
                self.state = 1852
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 1848
                    self.match(SparkSQLParser.T__3)
                    self.state = 1849
                    localctx._primaryExpression = self.primaryExpression(0)
                    localctx.parameters.append(localctx._primaryExpression)
                    self.state = 1854
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1855
                self.match(SparkSQLParser.T__2)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FromClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)

        def relation(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.RelationContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.RelationContext,i)


        def lateralView(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.LateralViewContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.LateralViewContext,i)


        def pivotClause(self):
            return self.getTypedRuleContext(SparkSQLParser.PivotClauseContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_fromClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFromClause" ):
                listener.enterFromClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFromClause" ):
                listener.exitFromClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFromClause" ):
                return visitor.visitFromClause(self)
            else:
                return visitor.visitChildren(self)




    def fromClause(self):

        localctx = SparkSQLParser.FromClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 120, self.RULE_fromClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1859
            self.match(SparkSQLParser.FROM)
            self.state = 1860
            self.relation()
            self.state = 1865
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,223,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1861
                    self.match(SparkSQLParser.T__3)
                    self.state = 1862
                    self.relation() 
                self.state = 1867
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,223,self._ctx)

            self.state = 1871
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,224,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1868
                    self.lateralView() 
                self.state = 1873
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,224,self._ctx)

            self.state = 1875
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,225,self._ctx)
            if la_ == 1:
                self.state = 1874
                self.pivotClause()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class AggregationClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._expression = None # ExpressionContext
            self.groupingExpressions = list() # of ExpressionContexts
            self.kind = None # Token

        def GROUP(self):
            return self.getToken(SparkSQLParser.GROUP, 0)

        def BY(self):
            return self.getToken(SparkSQLParser.BY, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def WITH(self):
            return self.getToken(SparkSQLParser.WITH, 0)

        def SETS(self):
            return self.getToken(SparkSQLParser.SETS, 0)

        def groupingSet(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.GroupingSetContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.GroupingSetContext,i)


        def ROLLUP(self):
            return self.getToken(SparkSQLParser.ROLLUP, 0)

        def CUBE(self):
            return self.getToken(SparkSQLParser.CUBE, 0)

        def GROUPING(self):
            return self.getToken(SparkSQLParser.GROUPING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_aggregationClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAggregationClause" ):
                listener.enterAggregationClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAggregationClause" ):
                listener.exitAggregationClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAggregationClause" ):
                return visitor.visitAggregationClause(self)
            else:
                return visitor.visitChildren(self)




    def aggregationClause(self):

        localctx = SparkSQLParser.AggregationClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 122, self.RULE_aggregationClause)
        self._la = 0 # Token type
        try:
            self.state = 1921
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,230,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1877
                self.match(SparkSQLParser.GROUP)
                self.state = 1878
                self.match(SparkSQLParser.BY)
                self.state = 1879
                localctx._expression = self.expression()
                localctx.groupingExpressions.append(localctx._expression)
                self.state = 1884
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,226,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1880
                        self.match(SparkSQLParser.T__3)
                        self.state = 1881
                        localctx._expression = self.expression()
                        localctx.groupingExpressions.append(localctx._expression) 
                    self.state = 1886
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,226,self._ctx)

                self.state = 1904
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,228,self._ctx)
                if la_ == 1:
                    self.state = 1887
                    self.match(SparkSQLParser.WITH)
                    self.state = 1888
                    localctx.kind = self.match(SparkSQLParser.ROLLUP)

                elif la_ == 2:
                    self.state = 1889
                    self.match(SparkSQLParser.WITH)
                    self.state = 1890
                    localctx.kind = self.match(SparkSQLParser.CUBE)

                elif la_ == 3:
                    self.state = 1891
                    localctx.kind = self.match(SparkSQLParser.GROUPING)
                    self.state = 1892
                    self.match(SparkSQLParser.SETS)
                    self.state = 1893
                    self.match(SparkSQLParser.T__1)
                    self.state = 1894
                    self.groupingSet()
                    self.state = 1899
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 1895
                        self.match(SparkSQLParser.T__3)
                        self.state = 1896
                        self.groupingSet()
                        self.state = 1901
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    self.state = 1902
                    self.match(SparkSQLParser.T__2)


                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1906
                self.match(SparkSQLParser.GROUP)
                self.state = 1907
                self.match(SparkSQLParser.BY)
                self.state = 1908
                localctx.kind = self.match(SparkSQLParser.GROUPING)
                self.state = 1909
                self.match(SparkSQLParser.SETS)
                self.state = 1910
                self.match(SparkSQLParser.T__1)
                self.state = 1911
                self.groupingSet()
                self.state = 1916
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 1912
                    self.match(SparkSQLParser.T__3)
                    self.state = 1913
                    self.groupingSet()
                    self.state = 1918
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1919
                self.match(SparkSQLParser.T__2)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class GroupingSetContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_groupingSet

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterGroupingSet" ):
                listener.enterGroupingSet(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitGroupingSet" ):
                listener.exitGroupingSet(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitGroupingSet" ):
                return visitor.visitGroupingSet(self)
            else:
                return visitor.visitChildren(self)




    def groupingSet(self):

        localctx = SparkSQLParser.GroupingSetContext(self, self._ctx, self.state)
        self.enterRule(localctx, 124, self.RULE_groupingSet)
        self._la = 0 # Token type
        try:
            self.state = 1936
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,233,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1923
                self.match(SparkSQLParser.T__1)
                self.state = 1932
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.T__1) | (1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.PLUS - 256)) | (1 << (SparkSQLParser.MINUS - 256)) | (1 << (SparkSQLParser.ASTERISK - 256)) | (1 << (SparkSQLParser.TILDE - 256)) | (1 << (SparkSQLParser.STRING - 256)) | (1 << (SparkSQLParser.BIGINT_LITERAL - 256)) | (1 << (SparkSQLParser.SMALLINT_LITERAL - 256)) | (1 << (SparkSQLParser.TINYINT_LITERAL - 256)) | (1 << (SparkSQLParser.INTEGER_VALUE - 256)) | (1 << (SparkSQLParser.EXPONENT_VALUE - 256)) | (1 << (SparkSQLParser.DECIMAL_VALUE - 256)) | (1 << (SparkSQLParser.FLOAT_LITERAL - 256)) | (1 << (SparkSQLParser.DOUBLE_LITERAL - 256)) | (1 << (SparkSQLParser.BIGDECIMAL_LITERAL - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                    self.state = 1924
                    self.expression()
                    self.state = 1929
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 1925
                        self.match(SparkSQLParser.T__3)
                        self.state = 1926
                        self.expression()
                        self.state = 1931
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                self.state = 1934
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1935
                self.expression()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class PivotClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.aggregates = None # NamedExpressionSeqContext
            self._pivotValue = None # PivotValueContext
            self.pivotValues = list() # of PivotValueContexts

        def PIVOT(self):
            return self.getToken(SparkSQLParser.PIVOT, 0)

        def FOR(self):
            return self.getToken(SparkSQLParser.FOR, 0)

        def pivotColumn(self):
            return self.getTypedRuleContext(SparkSQLParser.PivotColumnContext,0)


        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)

        def namedExpressionSeq(self):
            return self.getTypedRuleContext(SparkSQLParser.NamedExpressionSeqContext,0)


        def pivotValue(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.PivotValueContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.PivotValueContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_pivotClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPivotClause" ):
                listener.enterPivotClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPivotClause" ):
                listener.exitPivotClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPivotClause" ):
                return visitor.visitPivotClause(self)
            else:
                return visitor.visitChildren(self)




    def pivotClause(self):

        localctx = SparkSQLParser.PivotClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 126, self.RULE_pivotClause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1938
            self.match(SparkSQLParser.PIVOT)
            self.state = 1939
            self.match(SparkSQLParser.T__1)
            self.state = 1940
            localctx.aggregates = self.namedExpressionSeq()
            self.state = 1941
            self.match(SparkSQLParser.FOR)
            self.state = 1942
            self.pivotColumn()
            self.state = 1943
            self.match(SparkSQLParser.IN)
            self.state = 1944
            self.match(SparkSQLParser.T__1)
            self.state = 1945
            localctx._pivotValue = self.pivotValue()
            localctx.pivotValues.append(localctx._pivotValue)
            self.state = 1950
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 1946
                self.match(SparkSQLParser.T__3)
                self.state = 1947
                localctx._pivotValue = self.pivotValue()
                localctx.pivotValues.append(localctx._pivotValue)
                self.state = 1952
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1953
            self.match(SparkSQLParser.T__2)
            self.state = 1954
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class PivotColumnContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._identifier = None # IdentifierContext
            self.identifiers = list() # of IdentifierContexts

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_pivotColumn

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPivotColumn" ):
                listener.enterPivotColumn(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPivotColumn" ):
                listener.exitPivotColumn(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPivotColumn" ):
                return visitor.visitPivotColumn(self)
            else:
                return visitor.visitChildren(self)




    def pivotColumn(self):

        localctx = SparkSQLParser.PivotColumnContext(self, self._ctx, self.state)
        self.enterRule(localctx, 128, self.RULE_pivotColumn)
        self._la = 0 # Token type
        try:
            self.state = 1968
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.ADD, SparkSQLParser.AFTER, SparkSQLParser.ALL, SparkSQLParser.ALTER, SparkSQLParser.ANALYZE, SparkSQLParser.AND, SparkSQLParser.ANTI, SparkSQLParser.ANY, SparkSQLParser.ARCHIVE, SparkSQLParser.ARRAY, SparkSQLParser.AS, SparkSQLParser.ASC, SparkSQLParser.AT, SparkSQLParser.AUTHORIZATION, SparkSQLParser.BETWEEN, SparkSQLParser.BOTH, SparkSQLParser.BUCKET, SparkSQLParser.BUCKETS, SparkSQLParser.BY, SparkSQLParser.CACHE, SparkSQLParser.CASCADE, SparkSQLParser.CASE, SparkSQLParser.CAST, SparkSQLParser.CHANGE, SparkSQLParser.CHECK, SparkSQLParser.CLEAR, SparkSQLParser.CLUSTER, SparkSQLParser.CLUSTERED, SparkSQLParser.CODEGEN, SparkSQLParser.COLLATE, SparkSQLParser.COLLECTION, SparkSQLParser.COLUMN, SparkSQLParser.COLUMNS, SparkSQLParser.COMMENT, SparkSQLParser.COMMIT, SparkSQLParser.COMPACT, SparkSQLParser.COMPACTIONS, SparkSQLParser.COMPUTE, SparkSQLParser.CONCATENATE, SparkSQLParser.CONSTRAINT, SparkSQLParser.COST, SparkSQLParser.CREATE, SparkSQLParser.CROSS, SparkSQLParser.CUBE, SparkSQLParser.CURRENT, SparkSQLParser.CURRENT_DATE, SparkSQLParser.CURRENT_TIME, SparkSQLParser.CURRENT_TIMESTAMP, SparkSQLParser.CURRENT_USER, SparkSQLParser.DATA, SparkSQLParser.DATABASE, SparkSQLParser.DATABASES, SparkSQLParser.DBPROPERTIES, SparkSQLParser.DEFINED, SparkSQLParser.DELETE, SparkSQLParser.DELIMITED, SparkSQLParser.DESC, SparkSQLParser.DESCRIBE, SparkSQLParser.DFS, SparkSQLParser.DIRECTORIES, SparkSQLParser.DIRECTORY, SparkSQLParser.DISTINCT, SparkSQLParser.DISTRIBUTE, SparkSQLParser.DIV, SparkSQLParser.DROP, SparkSQLParser.ELSE, SparkSQLParser.END, SparkSQLParser.ESCAPE, SparkSQLParser.ESCAPED, SparkSQLParser.EXCEPT, SparkSQLParser.EXCHANGE, SparkSQLParser.EXISTS, SparkSQLParser.EXPLAIN, SparkSQLParser.EXPORT, SparkSQLParser.EXTENDED, SparkSQLParser.EXTERNAL, SparkSQLParser.EXTRACT, SparkSQLParser.FALSE, SparkSQLParser.FETCH, SparkSQLParser.FIELDS, SparkSQLParser.FILTER, SparkSQLParser.FILEFORMAT, SparkSQLParser.FIRST, SparkSQLParser.FOLLOWING, SparkSQLParser.FOR, SparkSQLParser.FOREIGN, SparkSQLParser.FORMAT, SparkSQLParser.FORMATTED, SparkSQLParser.FROM, SparkSQLParser.FULL, SparkSQLParser.FUNCTION, SparkSQLParser.FUNCTIONS, SparkSQLParser.GLOBAL, SparkSQLParser.GRANT, SparkSQLParser.GROUP, SparkSQLParser.GROUPING, SparkSQLParser.HAVING, SparkSQLParser.IF, SparkSQLParser.IGNORE, SparkSQLParser.IMPORT, SparkSQLParser.IN, SparkSQLParser.INDEX, SparkSQLParser.INDEXES, SparkSQLParser.INNER, SparkSQLParser.INPATH, SparkSQLParser.INPUTFORMAT, SparkSQLParser.INSERT, SparkSQLParser.INTERSECT, SparkSQLParser.INTERVAL, SparkSQLParser.INTO, SparkSQLParser.IS, SparkSQLParser.ITEMS, SparkSQLParser.JOIN, SparkSQLParser.KEYS, SparkSQLParser.LAST, SparkSQLParser.LATERAL, SparkSQLParser.LAZY, SparkSQLParser.LEADING, SparkSQLParser.LEFT, SparkSQLParser.LIKE, SparkSQLParser.LIMIT, SparkSQLParser.LINES, SparkSQLParser.LIST, SparkSQLParser.LOAD, SparkSQLParser.LOCAL, SparkSQLParser.LOCATION, SparkSQLParser.LOCK, SparkSQLParser.LOCKS, SparkSQLParser.LOGICAL, SparkSQLParser.MACRO, SparkSQLParser.MAP, SparkSQLParser.MATCHED, SparkSQLParser.MERGE, SparkSQLParser.MSCK, SparkSQLParser.NAMESPACE, SparkSQLParser.NAMESPACES, SparkSQLParser.NATURAL, SparkSQLParser.NO, SparkSQLParser.NOT, SparkSQLParser.NULL, SparkSQLParser.NULLS, SparkSQLParser.OF, SparkSQLParser.ON, SparkSQLParser.ONLY, SparkSQLParser.OPTION, SparkSQLParser.OPTIONS, SparkSQLParser.OR, SparkSQLParser.ORDER, SparkSQLParser.OUT, SparkSQLParser.OUTER, SparkSQLParser.OUTPUTFORMAT, SparkSQLParser.OVER, SparkSQLParser.OVERLAPS, SparkSQLParser.OVERLAY, SparkSQLParser.OVERWRITE, SparkSQLParser.PARTITION, SparkSQLParser.PARTITIONED, SparkSQLParser.PARTITIONS, SparkSQLParser.PERCENTLIT, SparkSQLParser.PIVOT, SparkSQLParser.PLACING, SparkSQLParser.POSITION, SparkSQLParser.PRECEDING, SparkSQLParser.PRIMARY, SparkSQLParser.PRINCIPALS, SparkSQLParser.PROPERTIES, SparkSQLParser.PURGE, SparkSQLParser.QUERY, SparkSQLParser.RANGE, SparkSQLParser.RECORDREADER, SparkSQLParser.RECORDWRITER, SparkSQLParser.RECOVER, SparkSQLParser.REDUCE, SparkSQLParser.REFERENCES, SparkSQLParser.REFRESH, SparkSQLParser.RENAME, SparkSQLParser.REPAIR, SparkSQLParser.REPLACE, SparkSQLParser.RESET, SparkSQLParser.RESTRICT, SparkSQLParser.REVOKE, SparkSQLParser.RIGHT, SparkSQLParser.RLIKE, SparkSQLParser.ROLE, SparkSQLParser.ROLES, SparkSQLParser.ROLLBACK, SparkSQLParser.ROLLUP, SparkSQLParser.ROW, SparkSQLParser.ROWS, SparkSQLParser.SCHEMA, SparkSQLParser.SELECT, SparkSQLParser.SEMI, SparkSQLParser.SEPARATED, SparkSQLParser.SERDE, SparkSQLParser.SERDEPROPERTIES, SparkSQLParser.SESSION_USER, SparkSQLParser.SET, SparkSQLParser.SETMINUS, SparkSQLParser.SETS, SparkSQLParser.SHOW, SparkSQLParser.SKEWED, SparkSQLParser.SOME, SparkSQLParser.SORT, SparkSQLParser.SORTED, SparkSQLParser.START, SparkSQLParser.STATISTICS, SparkSQLParser.STORED, SparkSQLParser.STRATIFY, SparkSQLParser.STRUCT, SparkSQLParser.SUBSTR, SparkSQLParser.SUBSTRING, SparkSQLParser.TABLE, SparkSQLParser.TABLES, SparkSQLParser.TABLESAMPLE, SparkSQLParser.TBLPROPERTIES, SparkSQLParser.TEMPORARY, SparkSQLParser.TERMINATED, SparkSQLParser.THEN, SparkSQLParser.TIME, SparkSQLParser.TO, SparkSQLParser.TOUCH, SparkSQLParser.TRAILING, SparkSQLParser.TRANSACTION, SparkSQLParser.TRANSACTIONS, SparkSQLParser.TRANSFORM, SparkSQLParser.TRIM, SparkSQLParser.TRUE, SparkSQLParser.TRUNCATE, SparkSQLParser.TYPE, SparkSQLParser.UNARCHIVE, SparkSQLParser.UNBOUNDED, SparkSQLParser.UNCACHE, SparkSQLParser.UNION, SparkSQLParser.UNIQUE, SparkSQLParser.UNKNOWN, SparkSQLParser.UNLOCK, SparkSQLParser.UNSET, SparkSQLParser.UPDATE, SparkSQLParser.USE, SparkSQLParser.USER, SparkSQLParser.USING, SparkSQLParser.VALUES, SparkSQLParser.VIEW, SparkSQLParser.VIEWS, SparkSQLParser.WHEN, SparkSQLParser.WHERE, SparkSQLParser.WINDOW, SparkSQLParser.WITH, SparkSQLParser.ZONE, SparkSQLParser.IDENTIFIER, SparkSQLParser.BACKQUOTED_IDENTIFIER]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1956
                localctx._identifier = self.identifier()
                localctx.identifiers.append(localctx._identifier)
                pass
            elif token in [SparkSQLParser.T__1]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1957
                self.match(SparkSQLParser.T__1)
                self.state = 1958
                localctx._identifier = self.identifier()
                localctx.identifiers.append(localctx._identifier)
                self.state = 1963
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 1959
                    self.match(SparkSQLParser.T__3)
                    self.state = 1960
                    localctx._identifier = self.identifier()
                    localctx.identifiers.append(localctx._identifier)
                    self.state = 1965
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1966
                self.match(SparkSQLParser.T__2)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class PivotValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_pivotValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPivotValue" ):
                listener.enterPivotValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPivotValue" ):
                listener.exitPivotValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPivotValue" ):
                return visitor.visitPivotValue(self)
            else:
                return visitor.visitChildren(self)




    def pivotValue(self):

        localctx = SparkSQLParser.PivotValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 130, self.RULE_pivotValue)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1970
            self.expression()
            self.state = 1975
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                self.state = 1972
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,237,self._ctx)
                if la_ == 1:
                    self.state = 1971
                    self.match(SparkSQLParser.AS)


                self.state = 1974
                self.identifier()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class LateralViewContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.tblName = None # IdentifierContext
            self._identifier = None # IdentifierContext
            self.colName = list() # of IdentifierContexts

        def LATERAL(self):
            return self.getToken(SparkSQLParser.LATERAL, 0)

        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,0)


        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)


        def OUTER(self):
            return self.getToken(SparkSQLParser.OUTER, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_lateralView

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLateralView" ):
                listener.enterLateralView(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLateralView" ):
                listener.exitLateralView(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLateralView" ):
                return visitor.visitLateralView(self)
            else:
                return visitor.visitChildren(self)




    def lateralView(self):

        localctx = SparkSQLParser.LateralViewContext(self, self._ctx, self.state)
        self.enterRule(localctx, 132, self.RULE_lateralView)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1977
            self.match(SparkSQLParser.LATERAL)
            self.state = 1978
            self.match(SparkSQLParser.VIEW)
            self.state = 1980
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,239,self._ctx)
            if la_ == 1:
                self.state = 1979
                self.match(SparkSQLParser.OUTER)


            self.state = 1982
            self.qualifiedName()
            self.state = 1983
            self.match(SparkSQLParser.T__1)
            self.state = 1992
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.T__1) | (1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.PLUS - 256)) | (1 << (SparkSQLParser.MINUS - 256)) | (1 << (SparkSQLParser.ASTERISK - 256)) | (1 << (SparkSQLParser.TILDE - 256)) | (1 << (SparkSQLParser.STRING - 256)) | (1 << (SparkSQLParser.BIGINT_LITERAL - 256)) | (1 << (SparkSQLParser.SMALLINT_LITERAL - 256)) | (1 << (SparkSQLParser.TINYINT_LITERAL - 256)) | (1 << (SparkSQLParser.INTEGER_VALUE - 256)) | (1 << (SparkSQLParser.EXPONENT_VALUE - 256)) | (1 << (SparkSQLParser.DECIMAL_VALUE - 256)) | (1 << (SparkSQLParser.FLOAT_LITERAL - 256)) | (1 << (SparkSQLParser.DOUBLE_LITERAL - 256)) | (1 << (SparkSQLParser.BIGDECIMAL_LITERAL - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                self.state = 1984
                self.expression()
                self.state = 1989
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 1985
                    self.match(SparkSQLParser.T__3)
                    self.state = 1986
                    self.expression()
                    self.state = 1991
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 1994
            self.match(SparkSQLParser.T__2)
            self.state = 1995
            localctx.tblName = self.identifier()
            self.state = 2007
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,244,self._ctx)
            if la_ == 1:
                self.state = 1997
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,242,self._ctx)
                if la_ == 1:
                    self.state = 1996
                    self.match(SparkSQLParser.AS)


                self.state = 1999
                localctx._identifier = self.identifier()
                localctx.colName.append(localctx._identifier)
                self.state = 2004
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,243,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 2000
                        self.match(SparkSQLParser.T__3)
                        self.state = 2001
                        localctx._identifier = self.identifier()
                        localctx.colName.append(localctx._identifier) 
                    self.state = 2006
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,243,self._ctx)



        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SetQuantifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def DISTINCT(self):
            return self.getToken(SparkSQLParser.DISTINCT, 0)

        def ALL(self):
            return self.getToken(SparkSQLParser.ALL, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_setQuantifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetQuantifier" ):
                listener.enterSetQuantifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetQuantifier" ):
                listener.exitSetQuantifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetQuantifier" ):
                return visitor.visitSetQuantifier(self)
            else:
                return visitor.visitChildren(self)




    def setQuantifier(self):

        localctx = SparkSQLParser.SetQuantifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 134, self.RULE_setQuantifier)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2009
            _la = self._input.LA(1)
            if not(_la==SparkSQLParser.ALL or _la==SparkSQLParser.DISTINCT):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class RelationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def relationPrimary(self):
            return self.getTypedRuleContext(SparkSQLParser.RelationPrimaryContext,0)


        def joinRelation(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.JoinRelationContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.JoinRelationContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_relation

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRelation" ):
                listener.enterRelation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRelation" ):
                listener.exitRelation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRelation" ):
                return visitor.visitRelation(self)
            else:
                return visitor.visitChildren(self)




    def relation(self):

        localctx = SparkSQLParser.RelationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 136, self.RULE_relation)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2011
            self.relationPrimary()
            self.state = 2015
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,245,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2012
                    self.joinRelation() 
                self.state = 2017
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,245,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class JoinRelationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.right = None # RelationPrimaryContext

        def JOIN(self):
            return self.getToken(SparkSQLParser.JOIN, 0)

        def relationPrimary(self):
            return self.getTypedRuleContext(SparkSQLParser.RelationPrimaryContext,0)


        def joinType(self):
            return self.getTypedRuleContext(SparkSQLParser.JoinTypeContext,0)


        def joinCriteria(self):
            return self.getTypedRuleContext(SparkSQLParser.JoinCriteriaContext,0)


        def NATURAL(self):
            return self.getToken(SparkSQLParser.NATURAL, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_joinRelation

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoinRelation" ):
                listener.enterJoinRelation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoinRelation" ):
                listener.exitJoinRelation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoinRelation" ):
                return visitor.visitJoinRelation(self)
            else:
                return visitor.visitChildren(self)




    def joinRelation(self):

        localctx = SparkSQLParser.JoinRelationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 138, self.RULE_joinRelation)
        try:
            self.state = 2029
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.ANTI, SparkSQLParser.CROSS, SparkSQLParser.FULL, SparkSQLParser.INNER, SparkSQLParser.JOIN, SparkSQLParser.LEFT, SparkSQLParser.RIGHT, SparkSQLParser.SEMI]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2018
                self.joinType()
                self.state = 2019
                self.match(SparkSQLParser.JOIN)
                self.state = 2020
                localctx.right = self.relationPrimary()
                self.state = 2022
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,246,self._ctx)
                if la_ == 1:
                    self.state = 2021
                    self.joinCriteria()


                pass
            elif token in [SparkSQLParser.NATURAL]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2024
                self.match(SparkSQLParser.NATURAL)
                self.state = 2025
                self.joinType()
                self.state = 2026
                self.match(SparkSQLParser.JOIN)
                self.state = 2027
                localctx.right = self.relationPrimary()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class JoinTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INNER(self):
            return self.getToken(SparkSQLParser.INNER, 0)

        def CROSS(self):
            return self.getToken(SparkSQLParser.CROSS, 0)

        def LEFT(self):
            return self.getToken(SparkSQLParser.LEFT, 0)

        def OUTER(self):
            return self.getToken(SparkSQLParser.OUTER, 0)

        def SEMI(self):
            return self.getToken(SparkSQLParser.SEMI, 0)

        def RIGHT(self):
            return self.getToken(SparkSQLParser.RIGHT, 0)

        def FULL(self):
            return self.getToken(SparkSQLParser.FULL, 0)

        def ANTI(self):
            return self.getToken(SparkSQLParser.ANTI, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_joinType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoinType" ):
                listener.enterJoinType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoinType" ):
                listener.exitJoinType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoinType" ):
                return visitor.visitJoinType(self)
            else:
                return visitor.visitChildren(self)




    def joinType(self):

        localctx = SparkSQLParser.JoinTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 140, self.RULE_joinType)
        self._la = 0 # Token type
        try:
            self.state = 2055
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,254,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 2032
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.INNER:
                    self.state = 2031
                    self.match(SparkSQLParser.INNER)


                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 2034
                self.match(SparkSQLParser.CROSS)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 2035
                self.match(SparkSQLParser.LEFT)
                self.state = 2037
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OUTER:
                    self.state = 2036
                    self.match(SparkSQLParser.OUTER)


                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 2040
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LEFT:
                    self.state = 2039
                    self.match(SparkSQLParser.LEFT)


                self.state = 2042
                self.match(SparkSQLParser.SEMI)
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 2043
                self.match(SparkSQLParser.RIGHT)
                self.state = 2045
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OUTER:
                    self.state = 2044
                    self.match(SparkSQLParser.OUTER)


                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 2047
                self.match(SparkSQLParser.FULL)
                self.state = 2049
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.OUTER:
                    self.state = 2048
                    self.match(SparkSQLParser.OUTER)


                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 2052
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.LEFT:
                    self.state = 2051
                    self.match(SparkSQLParser.LEFT)


                self.state = 2054
                self.match(SparkSQLParser.ANTI)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class JoinCriteriaContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ON(self):
            return self.getToken(SparkSQLParser.ON, 0)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)


        def USING(self):
            return self.getToken(SparkSQLParser.USING, 0)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_joinCriteria

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoinCriteria" ):
                listener.enterJoinCriteria(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoinCriteria" ):
                listener.exitJoinCriteria(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoinCriteria" ):
                return visitor.visitJoinCriteria(self)
            else:
                return visitor.visitChildren(self)




    def joinCriteria(self):

        localctx = SparkSQLParser.JoinCriteriaContext(self, self._ctx, self.state)
        self.enterRule(localctx, 142, self.RULE_joinCriteria)
        try:
            self.state = 2061
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.ON]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2057
                self.match(SparkSQLParser.ON)
                self.state = 2058
                self.booleanExpression(0)
                pass
            elif token in [SparkSQLParser.USING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2059
                self.match(SparkSQLParser.USING)
                self.state = 2060
                self.identifierList()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SampleContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def TABLESAMPLE(self):
            return self.getToken(SparkSQLParser.TABLESAMPLE, 0)

        def sampleMethod(self):
            return self.getTypedRuleContext(SparkSQLParser.SampleMethodContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_sample

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSample" ):
                listener.enterSample(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSample" ):
                listener.exitSample(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSample" ):
                return visitor.visitSample(self)
            else:
                return visitor.visitChildren(self)




    def sample(self):

        localctx = SparkSQLParser.SampleContext(self, self._ctx, self.state)
        self.enterRule(localctx, 144, self.RULE_sample)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2063
            self.match(SparkSQLParser.TABLESAMPLE)
            self.state = 2064
            self.match(SparkSQLParser.T__1)
            self.state = 2066
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.T__1) | (1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.PLUS - 256)) | (1 << (SparkSQLParser.MINUS - 256)) | (1 << (SparkSQLParser.ASTERISK - 256)) | (1 << (SparkSQLParser.TILDE - 256)) | (1 << (SparkSQLParser.STRING - 256)) | (1 << (SparkSQLParser.BIGINT_LITERAL - 256)) | (1 << (SparkSQLParser.SMALLINT_LITERAL - 256)) | (1 << (SparkSQLParser.TINYINT_LITERAL - 256)) | (1 << (SparkSQLParser.INTEGER_VALUE - 256)) | (1 << (SparkSQLParser.EXPONENT_VALUE - 256)) | (1 << (SparkSQLParser.DECIMAL_VALUE - 256)) | (1 << (SparkSQLParser.FLOAT_LITERAL - 256)) | (1 << (SparkSQLParser.DOUBLE_LITERAL - 256)) | (1 << (SparkSQLParser.BIGDECIMAL_LITERAL - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                self.state = 2065
                self.sampleMethod()


            self.state = 2068
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class SampleMethodContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_sampleMethod

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class SampleByRowsContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.SampleMethodContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)

        def ROWS(self):
            return self.getToken(SparkSQLParser.ROWS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByRows" ):
                listener.enterSampleByRows(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByRows" ):
                listener.exitSampleByRows(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByRows" ):
                return visitor.visitSampleByRows(self)
            else:
                return visitor.visitChildren(self)


    class SampleByPercentileContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.SampleMethodContext
            super().__init__(parser)
            self.negativeSign = None # Token
            self.percentage = None # Token
            self.copyFrom(ctx)

        def PERCENTLIT(self):
            return self.getToken(SparkSQLParser.PERCENTLIT, 0)
        def INTEGER_VALUE(self):
            return self.getToken(SparkSQLParser.INTEGER_VALUE, 0)
        def DECIMAL_VALUE(self):
            return self.getToken(SparkSQLParser.DECIMAL_VALUE, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByPercentile" ):
                listener.enterSampleByPercentile(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByPercentile" ):
                listener.exitSampleByPercentile(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByPercentile" ):
                return visitor.visitSampleByPercentile(self)
            else:
                return visitor.visitChildren(self)


    class SampleByBucketContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.SampleMethodContext
            super().__init__(parser)
            self.sampleType = None # Token
            self.numerator = None # Token
            self.denominator = None # Token
            self.copyFrom(ctx)

        def OUT(self):
            return self.getToken(SparkSQLParser.OUT, 0)
        def OF(self):
            return self.getToken(SparkSQLParser.OF, 0)
        def BUCKET(self):
            return self.getToken(SparkSQLParser.BUCKET, 0)
        def INTEGER_VALUE(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.INTEGER_VALUE)
            else:
                return self.getToken(SparkSQLParser.INTEGER_VALUE, i)
        def ON(self):
            return self.getToken(SparkSQLParser.ON, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByBucket" ):
                listener.enterSampleByBucket(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByBucket" ):
                listener.exitSampleByBucket(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByBucket" ):
                return visitor.visitSampleByBucket(self)
            else:
                return visitor.visitChildren(self)


    class SampleByBytesContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.SampleMethodContext
            super().__init__(parser)
            self.bytes = None # ExpressionContext
            self.copyFrom(ctx)

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByBytes" ):
                listener.enterSampleByBytes(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByBytes" ):
                listener.exitSampleByBytes(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByBytes" ):
                return visitor.visitSampleByBytes(self)
            else:
                return visitor.visitChildren(self)



    def sampleMethod(self):

        localctx = SparkSQLParser.SampleMethodContext(self, self._ctx, self.state)
        self.enterRule(localctx, 146, self.RULE_sampleMethod)
        self._la = 0 # Token type
        try:
            self.state = 2094
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,260,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.SampleByPercentileContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2071
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2070
                    localctx.negativeSign = self.match(SparkSQLParser.MINUS)


                self.state = 2073
                localctx.percentage = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.INTEGER_VALUE or _la==SparkSQLParser.DECIMAL_VALUE):
                    localctx.percentage = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 2074
                self.match(SparkSQLParser.PERCENTLIT)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.SampleByRowsContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2075
                self.expression()
                self.state = 2076
                self.match(SparkSQLParser.ROWS)
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.SampleByBucketContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2078
                localctx.sampleType = self.match(SparkSQLParser.BUCKET)
                self.state = 2079
                localctx.numerator = self.match(SparkSQLParser.INTEGER_VALUE)
                self.state = 2080
                self.match(SparkSQLParser.OUT)
                self.state = 2081
                self.match(SparkSQLParser.OF)
                self.state = 2082
                localctx.denominator = self.match(SparkSQLParser.INTEGER_VALUE)
                self.state = 2091
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.ON:
                    self.state = 2083
                    self.match(SparkSQLParser.ON)
                    self.state = 2089
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,258,self._ctx)
                    if la_ == 1:
                        self.state = 2084
                        self.identifier()
                        pass

                    elif la_ == 2:
                        self.state = 2085
                        self.qualifiedName()
                        self.state = 2086
                        self.match(SparkSQLParser.T__1)
                        self.state = 2087
                        self.match(SparkSQLParser.T__2)
                        pass




                pass

            elif la_ == 4:
                localctx = SparkSQLParser.SampleByBytesContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2093
                localctx.bytes = self.expression()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class IdentifierListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifierSeq(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierSeqContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_identifierList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierList" ):
                listener.enterIdentifierList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierList" ):
                listener.exitIdentifierList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierList" ):
                return visitor.visitIdentifierList(self)
            else:
                return visitor.visitChildren(self)




    def identifierList(self):

        localctx = SparkSQLParser.IdentifierListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 148, self.RULE_identifierList)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2096
            self.match(SparkSQLParser.T__1)
            self.state = 2097
            self.identifierSeq()
            self.state = 2098
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class IdentifierSeqContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._errorCapturingIdentifier = None # ErrorCapturingIdentifierContext
            self.ident = list() # of ErrorCapturingIdentifierContexts

        def errorCapturingIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ErrorCapturingIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_identifierSeq

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierSeq" ):
                listener.enterIdentifierSeq(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierSeq" ):
                listener.exitIdentifierSeq(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierSeq" ):
                return visitor.visitIdentifierSeq(self)
            else:
                return visitor.visitChildren(self)




    def identifierSeq(self):

        localctx = SparkSQLParser.IdentifierSeqContext(self, self._ctx, self.state)
        self.enterRule(localctx, 150, self.RULE_identifierSeq)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2100
            localctx._errorCapturingIdentifier = self.errorCapturingIdentifier()
            localctx.ident.append(localctx._errorCapturingIdentifier)
            self.state = 2105
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,261,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2101
                    self.match(SparkSQLParser.T__3)
                    self.state = 2102
                    localctx._errorCapturingIdentifier = self.errorCapturingIdentifier()
                    localctx.ident.append(localctx._errorCapturingIdentifier) 
                self.state = 2107
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,261,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class OrderedIdentifierListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def orderedIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.OrderedIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.OrderedIdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_orderedIdentifierList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrderedIdentifierList" ):
                listener.enterOrderedIdentifierList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrderedIdentifierList" ):
                listener.exitOrderedIdentifierList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrderedIdentifierList" ):
                return visitor.visitOrderedIdentifierList(self)
            else:
                return visitor.visitChildren(self)




    def orderedIdentifierList(self):

        localctx = SparkSQLParser.OrderedIdentifierListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 152, self.RULE_orderedIdentifierList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2108
            self.match(SparkSQLParser.T__1)
            self.state = 2109
            self.orderedIdentifier()
            self.state = 2114
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 2110
                self.match(SparkSQLParser.T__3)
                self.state = 2111
                self.orderedIdentifier()
                self.state = 2116
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 2117
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class OrderedIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.ident = None # ErrorCapturingIdentifierContext
            self.ordering = None # Token

        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def ASC(self):
            return self.getToken(SparkSQLParser.ASC, 0)

        def DESC(self):
            return self.getToken(SparkSQLParser.DESC, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_orderedIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrderedIdentifier" ):
                listener.enterOrderedIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrderedIdentifier" ):
                listener.exitOrderedIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrderedIdentifier" ):
                return visitor.visitOrderedIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def orderedIdentifier(self):

        localctx = SparkSQLParser.OrderedIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 154, self.RULE_orderedIdentifier)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2119
            localctx.ident = self.errorCapturingIdentifier()
            self.state = 2121
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.ASC or _la==SparkSQLParser.DESC:
                self.state = 2120
                localctx.ordering = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.ASC or _la==SparkSQLParser.DESC):
                    localctx.ordering = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class IdentifierCommentListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifierComment(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierCommentContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierCommentContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_identifierCommentList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierCommentList" ):
                listener.enterIdentifierCommentList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierCommentList" ):
                listener.exitIdentifierCommentList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierCommentList" ):
                return visitor.visitIdentifierCommentList(self)
            else:
                return visitor.visitChildren(self)




    def identifierCommentList(self):

        localctx = SparkSQLParser.IdentifierCommentListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 156, self.RULE_identifierCommentList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2123
            self.match(SparkSQLParser.T__1)
            self.state = 2124
            self.identifierComment()
            self.state = 2129
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 2125
                self.match(SparkSQLParser.T__3)
                self.state = 2126
                self.identifierComment()
                self.state = 2131
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 2132
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class IdentifierCommentContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def commentSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_identifierComment

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierComment" ):
                listener.enterIdentifierComment(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierComment" ):
                listener.exitIdentifierComment(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierComment" ):
                return visitor.visitIdentifierComment(self)
            else:
                return visitor.visitChildren(self)




    def identifierComment(self):

        localctx = SparkSQLParser.IdentifierCommentContext(self, self._ctx, self.state)
        self.enterRule(localctx, 158, self.RULE_identifierComment)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2134
            self.identifier()
            self.state = 2136
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.COMMENT:
                self.state = 2135
                self.commentSpec()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class RelationPrimaryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_relationPrimary

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class TableValuedFunctionContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def functionTable(self):
            return self.getTypedRuleContext(SparkSQLParser.FunctionTableContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableValuedFunction" ):
                listener.enterTableValuedFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableValuedFunction" ):
                listener.exitTableValuedFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableValuedFunction" ):
                return visitor.visitTableValuedFunction(self)
            else:
                return visitor.visitChildren(self)


    class InlineTableDefault2Context(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def inlineTable(self):
            return self.getTypedRuleContext(SparkSQLParser.InlineTableContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInlineTableDefault2" ):
                listener.enterInlineTableDefault2(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInlineTableDefault2" ):
                listener.exitInlineTableDefault2(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInlineTableDefault2" ):
                return visitor.visitInlineTableDefault2(self)
            else:
                return visitor.visitChildren(self)


    class AliasedRelationContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def relation(self):
            return self.getTypedRuleContext(SparkSQLParser.RelationContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,0)

        def sample(self):
            return self.getTypedRuleContext(SparkSQLParser.SampleContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAliasedRelation" ):
                listener.enterAliasedRelation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAliasedRelation" ):
                listener.exitAliasedRelation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAliasedRelation" ):
                return visitor.visitAliasedRelation(self)
            else:
                return visitor.visitChildren(self)


    class AliasedQueryContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,0)

        def sample(self):
            return self.getTypedRuleContext(SparkSQLParser.SampleContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAliasedQuery" ):
                listener.enterAliasedQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAliasedQuery" ):
                listener.exitAliasedQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAliasedQuery" ):
                return visitor.visitAliasedQuery(self)
            else:
                return visitor.visitChildren(self)


    class TableNameContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,0)

        def sample(self):
            return self.getTypedRuleContext(SparkSQLParser.SampleContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableName" ):
                listener.enterTableName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableName" ):
                listener.exitTableName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableName" ):
                return visitor.visitTableName(self)
            else:
                return visitor.visitChildren(self)



    def relationPrimary(self):

        localctx = SparkSQLParser.RelationPrimaryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 160, self.RULE_relationPrimary)
        try:
            self.state = 2162
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,269,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.TableNameContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2138
                self.multipartIdentifier()
                self.state = 2140
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,266,self._ctx)
                if la_ == 1:
                    self.state = 2139
                    self.sample()


                self.state = 2142
                self.tableAlias()
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.AliasedQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2144
                self.match(SparkSQLParser.T__1)
                self.state = 2145
                self.query()
                self.state = 2146
                self.match(SparkSQLParser.T__2)
                self.state = 2148
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,267,self._ctx)
                if la_ == 1:
                    self.state = 2147
                    self.sample()


                self.state = 2150
                self.tableAlias()
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.AliasedRelationContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2152
                self.match(SparkSQLParser.T__1)
                self.state = 2153
                self.relation()
                self.state = 2154
                self.match(SparkSQLParser.T__2)
                self.state = 2156
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,268,self._ctx)
                if la_ == 1:
                    self.state = 2155
                    self.sample()


                self.state = 2158
                self.tableAlias()
                pass

            elif la_ == 4:
                localctx = SparkSQLParser.InlineTableDefault2Context(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2160
                self.inlineTable()
                pass

            elif la_ == 5:
                localctx = SparkSQLParser.TableValuedFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 2161
                self.functionTable()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class InlineTableContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def VALUES(self):
            return self.getToken(SparkSQLParser.VALUES, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def tableAlias(self):
            return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_inlineTable

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInlineTable" ):
                listener.enterInlineTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInlineTable" ):
                listener.exitInlineTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInlineTable" ):
                return visitor.visitInlineTable(self)
            else:
                return visitor.visitChildren(self)




    def inlineTable(self):

        localctx = SparkSQLParser.InlineTableContext(self, self._ctx, self.state)
        self.enterRule(localctx, 162, self.RULE_inlineTable)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2164
            self.match(SparkSQLParser.VALUES)
            self.state = 2165
            self.expression()
            self.state = 2170
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,270,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2166
                    self.match(SparkSQLParser.T__3)
                    self.state = 2167
                    self.expression() 
                self.state = 2172
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,270,self._ctx)

            self.state = 2173
            self.tableAlias()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FunctionTableContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.funcName = None # ErrorCapturingIdentifierContext

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSQLParser.TableAliasContext,0)


        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_functionTable

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFunctionTable" ):
                listener.enterFunctionTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFunctionTable" ):
                listener.exitFunctionTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFunctionTable" ):
                return visitor.visitFunctionTable(self)
            else:
                return visitor.visitChildren(self)




    def functionTable(self):

        localctx = SparkSQLParser.FunctionTableContext(self, self._ctx, self.state)
        self.enterRule(localctx, 164, self.RULE_functionTable)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2175
            localctx.funcName = self.errorCapturingIdentifier()
            self.state = 2176
            self.match(SparkSQLParser.T__1)
            self.state = 2185
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.T__1) | (1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.PLUS - 256)) | (1 << (SparkSQLParser.MINUS - 256)) | (1 << (SparkSQLParser.ASTERISK - 256)) | (1 << (SparkSQLParser.TILDE - 256)) | (1 << (SparkSQLParser.STRING - 256)) | (1 << (SparkSQLParser.BIGINT_LITERAL - 256)) | (1 << (SparkSQLParser.SMALLINT_LITERAL - 256)) | (1 << (SparkSQLParser.TINYINT_LITERAL - 256)) | (1 << (SparkSQLParser.INTEGER_VALUE - 256)) | (1 << (SparkSQLParser.EXPONENT_VALUE - 256)) | (1 << (SparkSQLParser.DECIMAL_VALUE - 256)) | (1 << (SparkSQLParser.FLOAT_LITERAL - 256)) | (1 << (SparkSQLParser.DOUBLE_LITERAL - 256)) | (1 << (SparkSQLParser.BIGDECIMAL_LITERAL - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                self.state = 2177
                self.expression()
                self.state = 2182
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 2178
                    self.match(SparkSQLParser.T__3)
                    self.state = 2179
                    self.expression()
                    self.state = 2184
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 2187
            self.match(SparkSQLParser.T__2)
            self.state = 2188
            self.tableAlias()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TableAliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def strictIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.StrictIdentifierContext,0)


        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_tableAlias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableAlias" ):
                listener.enterTableAlias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableAlias" ):
                listener.exitTableAlias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableAlias" ):
                return visitor.visitTableAlias(self)
            else:
                return visitor.visitChildren(self)




    def tableAlias(self):

        localctx = SparkSQLParser.TableAliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 166, self.RULE_tableAlias)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2197
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,275,self._ctx)
            if la_ == 1:
                self.state = 2191
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,273,self._ctx)
                if la_ == 1:
                    self.state = 2190
                    self.match(SparkSQLParser.AS)


                self.state = 2193
                self.strictIdentifier()
                self.state = 2195
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,274,self._ctx)
                if la_ == 1:
                    self.state = 2194
                    self.identifierList()




        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class RowFormatContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_rowFormat

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class RowFormatSerdeContext(RowFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.RowFormatContext
            super().__init__(parser)
            self.name = None # Token
            self.props = None # TablePropertyListContext
            self.copyFrom(ctx)

        def ROW(self):
            return self.getToken(SparkSQLParser.ROW, 0)
        def FORMAT(self):
            return self.getToken(SparkSQLParser.FORMAT, 0)
        def SERDE(self):
            return self.getToken(SparkSQLParser.SERDE, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)
        def WITH(self):
            return self.getToken(SparkSQLParser.WITH, 0)
        def SERDEPROPERTIES(self):
            return self.getToken(SparkSQLParser.SERDEPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSQLParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRowFormatSerde" ):
                listener.enterRowFormatSerde(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRowFormatSerde" ):
                listener.exitRowFormatSerde(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRowFormatSerde" ):
                return visitor.visitRowFormatSerde(self)
            else:
                return visitor.visitChildren(self)


    class RowFormatDelimitedContext(RowFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.RowFormatContext
            super().__init__(parser)
            self.fieldsTerminatedBy = None # Token
            self.escapedBy = None # Token
            self.collectionItemsTerminatedBy = None # Token
            self.keysTerminatedBy = None # Token
            self.linesSeparatedBy = None # Token
            self.nullDefinedAs = None # Token
            self.copyFrom(ctx)

        def ROW(self):
            return self.getToken(SparkSQLParser.ROW, 0)
        def FORMAT(self):
            return self.getToken(SparkSQLParser.FORMAT, 0)
        def DELIMITED(self):
            return self.getToken(SparkSQLParser.DELIMITED, 0)
        def FIELDS(self):
            return self.getToken(SparkSQLParser.FIELDS, 0)
        def TERMINATED(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.TERMINATED)
            else:
                return self.getToken(SparkSQLParser.TERMINATED, i)
        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.BY)
            else:
                return self.getToken(SparkSQLParser.BY, i)
        def COLLECTION(self):
            return self.getToken(SparkSQLParser.COLLECTION, 0)
        def ITEMS(self):
            return self.getToken(SparkSQLParser.ITEMS, 0)
        def MAP(self):
            return self.getToken(SparkSQLParser.MAP, 0)
        def KEYS(self):
            return self.getToken(SparkSQLParser.KEYS, 0)
        def LINES(self):
            return self.getToken(SparkSQLParser.LINES, 0)
        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)
        def DEFINED(self):
            return self.getToken(SparkSQLParser.DEFINED, 0)
        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)
        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.STRING)
            else:
                return self.getToken(SparkSQLParser.STRING, i)
        def ESCAPED(self):
            return self.getToken(SparkSQLParser.ESCAPED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRowFormatDelimited" ):
                listener.enterRowFormatDelimited(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRowFormatDelimited" ):
                listener.exitRowFormatDelimited(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRowFormatDelimited" ):
                return visitor.visitRowFormatDelimited(self)
            else:
                return visitor.visitChildren(self)



    def rowFormat(self):

        localctx = SparkSQLParser.RowFormatContext(self, self._ctx, self.state)
        self.enterRule(localctx, 168, self.RULE_rowFormat)
        try:
            self.state = 2248
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,283,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.RowFormatSerdeContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2199
                self.match(SparkSQLParser.ROW)
                self.state = 2200
                self.match(SparkSQLParser.FORMAT)
                self.state = 2201
                self.match(SparkSQLParser.SERDE)
                self.state = 2202
                localctx.name = self.match(SparkSQLParser.STRING)
                self.state = 2206
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,276,self._ctx)
                if la_ == 1:
                    self.state = 2203
                    self.match(SparkSQLParser.WITH)
                    self.state = 2204
                    self.match(SparkSQLParser.SERDEPROPERTIES)
                    self.state = 2205
                    localctx.props = self.tablePropertyList()


                pass

            elif la_ == 2:
                localctx = SparkSQLParser.RowFormatDelimitedContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2208
                self.match(SparkSQLParser.ROW)
                self.state = 2209
                self.match(SparkSQLParser.FORMAT)
                self.state = 2210
                self.match(SparkSQLParser.DELIMITED)
                self.state = 2220
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,278,self._ctx)
                if la_ == 1:
                    self.state = 2211
                    self.match(SparkSQLParser.FIELDS)
                    self.state = 2212
                    self.match(SparkSQLParser.TERMINATED)
                    self.state = 2213
                    self.match(SparkSQLParser.BY)
                    self.state = 2214
                    localctx.fieldsTerminatedBy = self.match(SparkSQLParser.STRING)
                    self.state = 2218
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,277,self._ctx)
                    if la_ == 1:
                        self.state = 2215
                        self.match(SparkSQLParser.ESCAPED)
                        self.state = 2216
                        self.match(SparkSQLParser.BY)
                        self.state = 2217
                        localctx.escapedBy = self.match(SparkSQLParser.STRING)




                self.state = 2227
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,279,self._ctx)
                if la_ == 1:
                    self.state = 2222
                    self.match(SparkSQLParser.COLLECTION)
                    self.state = 2223
                    self.match(SparkSQLParser.ITEMS)
                    self.state = 2224
                    self.match(SparkSQLParser.TERMINATED)
                    self.state = 2225
                    self.match(SparkSQLParser.BY)
                    self.state = 2226
                    localctx.collectionItemsTerminatedBy = self.match(SparkSQLParser.STRING)


                self.state = 2234
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,280,self._ctx)
                if la_ == 1:
                    self.state = 2229
                    self.match(SparkSQLParser.MAP)
                    self.state = 2230
                    self.match(SparkSQLParser.KEYS)
                    self.state = 2231
                    self.match(SparkSQLParser.TERMINATED)
                    self.state = 2232
                    self.match(SparkSQLParser.BY)
                    self.state = 2233
                    localctx.keysTerminatedBy = self.match(SparkSQLParser.STRING)


                self.state = 2240
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,281,self._ctx)
                if la_ == 1:
                    self.state = 2236
                    self.match(SparkSQLParser.LINES)
                    self.state = 2237
                    self.match(SparkSQLParser.TERMINATED)
                    self.state = 2238
                    self.match(SparkSQLParser.BY)
                    self.state = 2239
                    localctx.linesSeparatedBy = self.match(SparkSQLParser.STRING)


                self.state = 2246
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,282,self._ctx)
                if la_ == 1:
                    self.state = 2242
                    self.match(SparkSQLParser.NULL)
                    self.state = 2243
                    self.match(SparkSQLParser.DEFINED)
                    self.state = 2244
                    self.match(SparkSQLParser.AS)
                    self.state = 2245
                    localctx.nullDefinedAs = self.match(SparkSQLParser.STRING)


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class MultipartIdentifierListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def multipartIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.MultipartIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_multipartIdentifierList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMultipartIdentifierList" ):
                listener.enterMultipartIdentifierList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMultipartIdentifierList" ):
                listener.exitMultipartIdentifierList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMultipartIdentifierList" ):
                return visitor.visitMultipartIdentifierList(self)
            else:
                return visitor.visitChildren(self)




    def multipartIdentifierList(self):

        localctx = SparkSQLParser.MultipartIdentifierListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 170, self.RULE_multipartIdentifierList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2250
            self.multipartIdentifier()
            self.state = 2255
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 2251
                self.match(SparkSQLParser.T__3)
                self.state = 2252
                self.multipartIdentifier()
                self.state = 2257
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class MultipartIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._errorCapturingIdentifier = None # ErrorCapturingIdentifierContext
            self.parts = list() # of ErrorCapturingIdentifierContexts

        def errorCapturingIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ErrorCapturingIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_multipartIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMultipartIdentifier" ):
                listener.enterMultipartIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMultipartIdentifier" ):
                listener.exitMultipartIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMultipartIdentifier" ):
                return visitor.visitMultipartIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def multipartIdentifier(self):

        localctx = SparkSQLParser.MultipartIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 172, self.RULE_multipartIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2258
            localctx._errorCapturingIdentifier = self.errorCapturingIdentifier()
            localctx.parts.append(localctx._errorCapturingIdentifier)
            self.state = 2263
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,285,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2259
                    self.match(SparkSQLParser.T__4)
                    self.state = 2260
                    localctx._errorCapturingIdentifier = self.errorCapturingIdentifier()
                    localctx.parts.append(localctx._errorCapturingIdentifier) 
                self.state = 2265
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,285,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TableIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.db = None # ErrorCapturingIdentifierContext
            self.table = None # ErrorCapturingIdentifierContext

        def errorCapturingIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ErrorCapturingIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_tableIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableIdentifier" ):
                listener.enterTableIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableIdentifier" ):
                listener.exitTableIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableIdentifier" ):
                return visitor.visitTableIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def tableIdentifier(self):

        localctx = SparkSQLParser.TableIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 174, self.RULE_tableIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2269
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,286,self._ctx)
            if la_ == 1:
                self.state = 2266
                localctx.db = self.errorCapturingIdentifier()
                self.state = 2267
                self.match(SparkSQLParser.T__4)


            self.state = 2271
            localctx.table = self.errorCapturingIdentifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FunctionIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.db = None # ErrorCapturingIdentifierContext
            self.function = None # ErrorCapturingIdentifierContext

        def errorCapturingIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ErrorCapturingIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_functionIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFunctionIdentifier" ):
                listener.enterFunctionIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFunctionIdentifier" ):
                listener.exitFunctionIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFunctionIdentifier" ):
                return visitor.visitFunctionIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def functionIdentifier(self):

        localctx = SparkSQLParser.FunctionIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 176, self.RULE_functionIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2276
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,287,self._ctx)
            if la_ == 1:
                self.state = 2273
                localctx.db = self.errorCapturingIdentifier()
                self.state = 2274
                self.match(SparkSQLParser.T__4)


            self.state = 2278
            localctx.function = self.errorCapturingIdentifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NamedExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.name = None # ErrorCapturingIdentifierContext

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def identifierList(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierListContext,0)


        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_namedExpression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedExpression" ):
                listener.enterNamedExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedExpression" ):
                listener.exitNamedExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedExpression" ):
                return visitor.visitNamedExpression(self)
            else:
                return visitor.visitChildren(self)




    def namedExpression(self):

        localctx = SparkSQLParser.NamedExpressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 178, self.RULE_namedExpression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2280
            self.expression()
            self.state = 2288
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,290,self._ctx)
            if la_ == 1:
                self.state = 2282
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,288,self._ctx)
                if la_ == 1:
                    self.state = 2281
                    self.match(SparkSQLParser.AS)


                self.state = 2286
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSQLParser.ADD, SparkSQLParser.AFTER, SparkSQLParser.ALL, SparkSQLParser.ALTER, SparkSQLParser.ANALYZE, SparkSQLParser.AND, SparkSQLParser.ANTI, SparkSQLParser.ANY, SparkSQLParser.ARCHIVE, SparkSQLParser.ARRAY, SparkSQLParser.AS, SparkSQLParser.ASC, SparkSQLParser.AT, SparkSQLParser.AUTHORIZATION, SparkSQLParser.BETWEEN, SparkSQLParser.BOTH, SparkSQLParser.BUCKET, SparkSQLParser.BUCKETS, SparkSQLParser.BY, SparkSQLParser.CACHE, SparkSQLParser.CASCADE, SparkSQLParser.CASE, SparkSQLParser.CAST, SparkSQLParser.CHANGE, SparkSQLParser.CHECK, SparkSQLParser.CLEAR, SparkSQLParser.CLUSTER, SparkSQLParser.CLUSTERED, SparkSQLParser.CODEGEN, SparkSQLParser.COLLATE, SparkSQLParser.COLLECTION, SparkSQLParser.COLUMN, SparkSQLParser.COLUMNS, SparkSQLParser.COMMENT, SparkSQLParser.COMMIT, SparkSQLParser.COMPACT, SparkSQLParser.COMPACTIONS, SparkSQLParser.COMPUTE, SparkSQLParser.CONCATENATE, SparkSQLParser.CONSTRAINT, SparkSQLParser.COST, SparkSQLParser.CREATE, SparkSQLParser.CROSS, SparkSQLParser.CUBE, SparkSQLParser.CURRENT, SparkSQLParser.CURRENT_DATE, SparkSQLParser.CURRENT_TIME, SparkSQLParser.CURRENT_TIMESTAMP, SparkSQLParser.CURRENT_USER, SparkSQLParser.DATA, SparkSQLParser.DATABASE, SparkSQLParser.DATABASES, SparkSQLParser.DBPROPERTIES, SparkSQLParser.DEFINED, SparkSQLParser.DELETE, SparkSQLParser.DELIMITED, SparkSQLParser.DESC, SparkSQLParser.DESCRIBE, SparkSQLParser.DFS, SparkSQLParser.DIRECTORIES, SparkSQLParser.DIRECTORY, SparkSQLParser.DISTINCT, SparkSQLParser.DISTRIBUTE, SparkSQLParser.DIV, SparkSQLParser.DROP, SparkSQLParser.ELSE, SparkSQLParser.END, SparkSQLParser.ESCAPE, SparkSQLParser.ESCAPED, SparkSQLParser.EXCEPT, SparkSQLParser.EXCHANGE, SparkSQLParser.EXISTS, SparkSQLParser.EXPLAIN, SparkSQLParser.EXPORT, SparkSQLParser.EXTENDED, SparkSQLParser.EXTERNAL, SparkSQLParser.EXTRACT, SparkSQLParser.FALSE, SparkSQLParser.FETCH, SparkSQLParser.FIELDS, SparkSQLParser.FILTER, SparkSQLParser.FILEFORMAT, SparkSQLParser.FIRST, SparkSQLParser.FOLLOWING, SparkSQLParser.FOR, SparkSQLParser.FOREIGN, SparkSQLParser.FORMAT, SparkSQLParser.FORMATTED, SparkSQLParser.FROM, SparkSQLParser.FULL, SparkSQLParser.FUNCTION, SparkSQLParser.FUNCTIONS, SparkSQLParser.GLOBAL, SparkSQLParser.GRANT, SparkSQLParser.GROUP, SparkSQLParser.GROUPING, SparkSQLParser.HAVING, SparkSQLParser.IF, SparkSQLParser.IGNORE, SparkSQLParser.IMPORT, SparkSQLParser.IN, SparkSQLParser.INDEX, SparkSQLParser.INDEXES, SparkSQLParser.INNER, SparkSQLParser.INPATH, SparkSQLParser.INPUTFORMAT, SparkSQLParser.INSERT, SparkSQLParser.INTERSECT, SparkSQLParser.INTERVAL, SparkSQLParser.INTO, SparkSQLParser.IS, SparkSQLParser.ITEMS, SparkSQLParser.JOIN, SparkSQLParser.KEYS, SparkSQLParser.LAST, SparkSQLParser.LATERAL, SparkSQLParser.LAZY, SparkSQLParser.LEADING, SparkSQLParser.LEFT, SparkSQLParser.LIKE, SparkSQLParser.LIMIT, SparkSQLParser.LINES, SparkSQLParser.LIST, SparkSQLParser.LOAD, SparkSQLParser.LOCAL, SparkSQLParser.LOCATION, SparkSQLParser.LOCK, SparkSQLParser.LOCKS, SparkSQLParser.LOGICAL, SparkSQLParser.MACRO, SparkSQLParser.MAP, SparkSQLParser.MATCHED, SparkSQLParser.MERGE, SparkSQLParser.MSCK, SparkSQLParser.NAMESPACE, SparkSQLParser.NAMESPACES, SparkSQLParser.NATURAL, SparkSQLParser.NO, SparkSQLParser.NOT, SparkSQLParser.NULL, SparkSQLParser.NULLS, SparkSQLParser.OF, SparkSQLParser.ON, SparkSQLParser.ONLY, SparkSQLParser.OPTION, SparkSQLParser.OPTIONS, SparkSQLParser.OR, SparkSQLParser.ORDER, SparkSQLParser.OUT, SparkSQLParser.OUTER, SparkSQLParser.OUTPUTFORMAT, SparkSQLParser.OVER, SparkSQLParser.OVERLAPS, SparkSQLParser.OVERLAY, SparkSQLParser.OVERWRITE, SparkSQLParser.PARTITION, SparkSQLParser.PARTITIONED, SparkSQLParser.PARTITIONS, SparkSQLParser.PERCENTLIT, SparkSQLParser.PIVOT, SparkSQLParser.PLACING, SparkSQLParser.POSITION, SparkSQLParser.PRECEDING, SparkSQLParser.PRIMARY, SparkSQLParser.PRINCIPALS, SparkSQLParser.PROPERTIES, SparkSQLParser.PURGE, SparkSQLParser.QUERY, SparkSQLParser.RANGE, SparkSQLParser.RECORDREADER, SparkSQLParser.RECORDWRITER, SparkSQLParser.RECOVER, SparkSQLParser.REDUCE, SparkSQLParser.REFERENCES, SparkSQLParser.REFRESH, SparkSQLParser.RENAME, SparkSQLParser.REPAIR, SparkSQLParser.REPLACE, SparkSQLParser.RESET, SparkSQLParser.RESTRICT, SparkSQLParser.REVOKE, SparkSQLParser.RIGHT, SparkSQLParser.RLIKE, SparkSQLParser.ROLE, SparkSQLParser.ROLES, SparkSQLParser.ROLLBACK, SparkSQLParser.ROLLUP, SparkSQLParser.ROW, SparkSQLParser.ROWS, SparkSQLParser.SCHEMA, SparkSQLParser.SELECT, SparkSQLParser.SEMI, SparkSQLParser.SEPARATED, SparkSQLParser.SERDE, SparkSQLParser.SERDEPROPERTIES, SparkSQLParser.SESSION_USER, SparkSQLParser.SET, SparkSQLParser.SETMINUS, SparkSQLParser.SETS, SparkSQLParser.SHOW, SparkSQLParser.SKEWED, SparkSQLParser.SOME, SparkSQLParser.SORT, SparkSQLParser.SORTED, SparkSQLParser.START, SparkSQLParser.STATISTICS, SparkSQLParser.STORED, SparkSQLParser.STRATIFY, SparkSQLParser.STRUCT, SparkSQLParser.SUBSTR, SparkSQLParser.SUBSTRING, SparkSQLParser.TABLE, SparkSQLParser.TABLES, SparkSQLParser.TABLESAMPLE, SparkSQLParser.TBLPROPERTIES, SparkSQLParser.TEMPORARY, SparkSQLParser.TERMINATED, SparkSQLParser.THEN, SparkSQLParser.TIME, SparkSQLParser.TO, SparkSQLParser.TOUCH, SparkSQLParser.TRAILING, SparkSQLParser.TRANSACTION, SparkSQLParser.TRANSACTIONS, SparkSQLParser.TRANSFORM, SparkSQLParser.TRIM, SparkSQLParser.TRUE, SparkSQLParser.TRUNCATE, SparkSQLParser.TYPE, SparkSQLParser.UNARCHIVE, SparkSQLParser.UNBOUNDED, SparkSQLParser.UNCACHE, SparkSQLParser.UNION, SparkSQLParser.UNIQUE, SparkSQLParser.UNKNOWN, SparkSQLParser.UNLOCK, SparkSQLParser.UNSET, SparkSQLParser.UPDATE, SparkSQLParser.USE, SparkSQLParser.USER, SparkSQLParser.USING, SparkSQLParser.VALUES, SparkSQLParser.VIEW, SparkSQLParser.VIEWS, SparkSQLParser.WHEN, SparkSQLParser.WHERE, SparkSQLParser.WINDOW, SparkSQLParser.WITH, SparkSQLParser.ZONE, SparkSQLParser.IDENTIFIER, SparkSQLParser.BACKQUOTED_IDENTIFIER]:
                    self.state = 2284
                    localctx.name = self.errorCapturingIdentifier()
                    pass
                elif token in [SparkSQLParser.T__1]:
                    self.state = 2285
                    self.identifierList()
                    pass
                else:
                    raise NoViableAltException(self)



        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NamedExpressionSeqContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def namedExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.NamedExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.NamedExpressionContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_namedExpressionSeq

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedExpressionSeq" ):
                listener.enterNamedExpressionSeq(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedExpressionSeq" ):
                listener.exitNamedExpressionSeq(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedExpressionSeq" ):
                return visitor.visitNamedExpressionSeq(self)
            else:
                return visitor.visitChildren(self)




    def namedExpressionSeq(self):

        localctx = SparkSQLParser.NamedExpressionSeqContext(self, self._ctx, self.state)
        self.enterRule(localctx, 180, self.RULE_namedExpressionSeq)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2290
            self.namedExpression()
            self.state = 2295
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,291,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2291
                    self.match(SparkSQLParser.T__3)
                    self.state = 2292
                    self.namedExpression() 
                self.state = 2297
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,291,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TransformListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._transform = None # TransformContext
            self.transforms = list() # of TransformContexts

        def transform(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TransformContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TransformContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_transformList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTransformList" ):
                listener.enterTransformList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTransformList" ):
                listener.exitTransformList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTransformList" ):
                return visitor.visitTransformList(self)
            else:
                return visitor.visitChildren(self)




    def transformList(self):

        localctx = SparkSQLParser.TransformListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 182, self.RULE_transformList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2298
            self.match(SparkSQLParser.T__1)
            self.state = 2299
            localctx._transform = self.transform()
            localctx.transforms.append(localctx._transform)
            self.state = 2304
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 2300
                self.match(SparkSQLParser.T__3)
                self.state = 2301
                localctx._transform = self.transform()
                localctx.transforms.append(localctx._transform)
                self.state = 2306
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 2307
            self.match(SparkSQLParser.T__2)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TransformContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_transform

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class IdentityTransformContext(TransformContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.TransformContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentityTransform" ):
                listener.enterIdentityTransform(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentityTransform" ):
                listener.exitIdentityTransform(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentityTransform" ):
                return visitor.visitIdentityTransform(self)
            else:
                return visitor.visitChildren(self)


    class ApplyTransformContext(TransformContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.TransformContext
            super().__init__(parser)
            self.transformName = None # IdentifierContext
            self._transformArgument = None # TransformArgumentContext
            self.argument = list() # of TransformArgumentContexts
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def transformArgument(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.TransformArgumentContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.TransformArgumentContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterApplyTransform" ):
                listener.enterApplyTransform(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitApplyTransform" ):
                listener.exitApplyTransform(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitApplyTransform" ):
                return visitor.visitApplyTransform(self)
            else:
                return visitor.visitChildren(self)



    def transform(self):

        localctx = SparkSQLParser.TransformContext(self, self._ctx, self.state)
        self.enterRule(localctx, 184, self.RULE_transform)
        self._la = 0 # Token type
        try:
            self.state = 2322
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,294,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.IdentityTransformContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2309
                self.qualifiedName()
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.ApplyTransformContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2310
                localctx.transformName = self.identifier()
                self.state = 2311
                self.match(SparkSQLParser.T__1)
                self.state = 2312
                localctx._transformArgument = self.transformArgument()
                localctx.argument.append(localctx._transformArgument)
                self.state = 2317
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 2313
                    self.match(SparkSQLParser.T__3)
                    self.state = 2314
                    localctx._transformArgument = self.transformArgument()
                    localctx.argument.append(localctx._transformArgument)
                    self.state = 2319
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 2320
                self.match(SparkSQLParser.T__2)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class TransformArgumentContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,0)


        def constant(self):
            return self.getTypedRuleContext(SparkSQLParser.ConstantContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_transformArgument

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTransformArgument" ):
                listener.enterTransformArgument(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTransformArgument" ):
                listener.exitTransformArgument(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTransformArgument" ):
                return visitor.visitTransformArgument(self)
            else:
                return visitor.visitChildren(self)




    def transformArgument(self):

        localctx = SparkSQLParser.TransformArgumentContext(self, self._ctx, self.state)
        self.enterRule(localctx, 186, self.RULE_transformArgument)
        try:
            self.state = 2326
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,295,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 2324
                self.qualifiedName()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 2325
                self.constant()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExpression" ):
                listener.enterExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExpression" ):
                listener.exitExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExpression" ):
                return visitor.visitExpression(self)
            else:
                return visitor.visitChildren(self)




    def expression(self):

        localctx = SparkSQLParser.ExpressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 188, self.RULE_expression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2328
            self.booleanExpression(0)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class BooleanExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_booleanExpression

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class LogicalNotContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.BooleanExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLogicalNot" ):
                listener.enterLogicalNot(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLogicalNot" ):
                listener.exitLogicalNot(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLogicalNot" ):
                return visitor.visitLogicalNot(self)
            else:
                return visitor.visitChildren(self)


    class PredicatedContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.BooleanExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,0)

        def predicate(self):
            return self.getTypedRuleContext(SparkSQLParser.PredicateContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPredicated" ):
                listener.enterPredicated(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPredicated" ):
                listener.exitPredicated(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPredicated" ):
                return visitor.visitPredicated(self)
            else:
                return visitor.visitChildren(self)


    class ExistsContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.BooleanExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)
        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExists" ):
                listener.enterExists(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExists" ):
                listener.exitExists(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExists" ):
                return visitor.visitExists(self)
            else:
                return visitor.visitChildren(self)


    class LogicalBinaryContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.BooleanExpressionContext
            super().__init__(parser)
            self.left = None # BooleanExpressionContext
            self.operator = None # Token
            self.right = None # BooleanExpressionContext
            self.copyFrom(ctx)

        def booleanExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.BooleanExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,i)

        def AND(self):
            return self.getToken(SparkSQLParser.AND, 0)
        def OR(self):
            return self.getToken(SparkSQLParser.OR, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLogicalBinary" ):
                listener.enterLogicalBinary(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLogicalBinary" ):
                listener.exitLogicalBinary(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLogicalBinary" ):
                return visitor.visitLogicalBinary(self)
            else:
                return visitor.visitChildren(self)



    def booleanExpression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSQLParser.BooleanExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 190
        self.enterRecursionRule(localctx, 190, self.RULE_booleanExpression, _p)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2342
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,297,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.LogicalNotContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx

                self.state = 2331
                self.match(SparkSQLParser.NOT)
                self.state = 2332
                self.booleanExpression(5)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.ExistsContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2333
                self.match(SparkSQLParser.EXISTS)
                self.state = 2334
                self.match(SparkSQLParser.T__1)
                self.state = 2335
                self.query()
                self.state = 2336
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.PredicatedContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2338
                self.valueExpression(0)
                self.state = 2340
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,296,self._ctx)
                if la_ == 1:
                    self.state = 2339
                    self.predicate()


                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 2352
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,299,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 2350
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,298,self._ctx)
                    if la_ == 1:
                        localctx = SparkSQLParser.LogicalBinaryContext(self, SparkSQLParser.BooleanExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_booleanExpression)
                        self.state = 2344
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 2345
                        localctx.operator = self.match(SparkSQLParser.AND)
                        self.state = 2346
                        localctx.right = self.booleanExpression(3)
                        pass

                    elif la_ == 2:
                        localctx = SparkSQLParser.LogicalBinaryContext(self, SparkSQLParser.BooleanExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_booleanExpression)
                        self.state = 2347
                        if not self.precpred(self._ctx, 1):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
                        self.state = 2348
                        localctx.operator = self.match(SparkSQLParser.OR)
                        self.state = 2349
                        localctx.right = self.booleanExpression(2)
                        pass

             
                self.state = 2354
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,299,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx


    class PredicateContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_predicate

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class RlikeContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.pattern = None # ValueExpressionContext
            self.copyFrom(ctx)

        def RLIKE(self):
            return self.getToken(SparkSQLParser.RLIKE, 0)
        def valueExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,0)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRlike" ):
                listener.enterRlike(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRlike" ):
                listener.exitRlike(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRlike" ):
                return visitor.visitRlike(self)
            else:
                return visitor.visitChildren(self)


    class BewteenContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.lower = None # ValueExpressionContext
            self.upper = None # ValueExpressionContext
            self.copyFrom(ctx)

        def AND(self):
            return self.getToken(SparkSQLParser.AND, 0)
        def BETWEEN(self):
            return self.getToken(SparkSQLParser.BETWEEN, 0)
        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,i)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBewteen" ):
                listener.enterBewteen(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBewteen" ):
                listener.exitBewteen(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBewteen" ):
                return visitor.visitBewteen(self)
            else:
                return visitor.visitChildren(self)


    class LikeListContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.quantifier = None # Token
            self.copyFrom(ctx)

        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)
        def ANY(self):
            return self.getToken(SparkSQLParser.ANY, 0)
        def SOME(self):
            return self.getToken(SparkSQLParser.SOME, 0)
        def ALL(self):
            return self.getToken(SparkSQLParser.ALL, 0)
        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLikeList" ):
                listener.enterLikeList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLikeList" ):
                listener.exitLikeList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLikeList" ):
                return visitor.visitLikeList(self)
            else:
                return visitor.visitChildren(self)


    class LikeValueContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.pattern = None # ValueExpressionContext
            self.escapeChar = None # Token
            self.copyFrom(ctx)

        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)
        def valueExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,0)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)
        def ESCAPE(self):
            return self.getToken(SparkSQLParser.ESCAPE, 0)
        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLikeValue" ):
                listener.enterLikeValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLikeValue" ):
                listener.exitLikeValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLikeValue" ):
                return visitor.visitLikeValue(self)
            else:
                return visitor.visitChildren(self)


    class InSubqueryContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)

        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInSubquery" ):
                listener.enterInSubquery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInSubquery" ):
                listener.exitInSubquery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInSubquery" ):
                return visitor.visitInSubquery(self)
            else:
                return visitor.visitChildren(self)


    class DistinctFromContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.right = None # ValueExpressionContext
            self.copyFrom(ctx)

        def IS(self):
            return self.getToken(SparkSQLParser.IS, 0)
        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def DISTINCT(self):
            return self.getToken(SparkSQLParser.DISTINCT, 0)
        def valueExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,0)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDistinctFrom" ):
                listener.enterDistinctFrom(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDistinctFrom" ):
                listener.exitDistinctFrom(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDistinctFrom" ):
                return visitor.visitDistinctFrom(self)
            else:
                return visitor.visitChildren(self)


    class InListContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.copyFrom(ctx)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)

        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInList" ):
                listener.enterInList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInList" ):
                listener.exitInList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInList" ):
                return visitor.visitInList(self)
            else:
                return visitor.visitChildren(self)


    class NullPredicateContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.copyFrom(ctx)

        def IS(self):
            return self.getToken(SparkSQLParser.IS, 0)
        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNullPredicate" ):
                listener.enterNullPredicate(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNullPredicate" ):
                listener.exitNullPredicate(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNullPredicate" ):
                return visitor.visitNullPredicate(self)
            else:
                return visitor.visitChildren(self)


    class BooleanPredicateContext(PredicateContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PredicateContext
            super().__init__(parser)
            self.kind = None # Token
            self.copyFrom(ctx)

        def IS(self):
            return self.getToken(SparkSQLParser.IS, 0)
        def TRUE(self):
            return self.getToken(SparkSQLParser.TRUE, 0)
        def FALSE(self):
            return self.getToken(SparkSQLParser.FALSE, 0)
        def UNKNOWN(self):
            return self.getToken(SparkSQLParser.UNKNOWN, 0)
        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBooleanPredicate" ):
                listener.enterBooleanPredicate(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBooleanPredicate" ):
                listener.exitBooleanPredicate(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBooleanPredicate" ):
                return visitor.visitBooleanPredicate(self)
            else:
                return visitor.visitChildren(self)



    def predicate(self):

        localctx = SparkSQLParser.PredicateContext(self, self._ctx, self.state)
        self.enterRule(localctx, 192, self.RULE_predicate)
        self._la = 0 # Token type
        try:
            self.state = 2437
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,313,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.BewteenContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2356
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2355
                    self.match(SparkSQLParser.NOT)


                self.state = 2358
                localctx.kind = self.match(SparkSQLParser.BETWEEN)
                self.state = 2359
                localctx.lower = self.valueExpression(0)
                self.state = 2360
                self.match(SparkSQLParser.AND)
                self.state = 2361
                localctx.upper = self.valueExpression(0)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.InListContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2364
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2363
                    self.match(SparkSQLParser.NOT)


                self.state = 2366
                localctx.kind = self.match(SparkSQLParser.IN)
                self.state = 2367
                self.match(SparkSQLParser.T__1)
                self.state = 2368
                self.expression()
                self.state = 2373
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSQLParser.T__3:
                    self.state = 2369
                    self.match(SparkSQLParser.T__3)
                    self.state = 2370
                    self.expression()
                    self.state = 2375
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 2376
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.InSubqueryContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2379
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2378
                    self.match(SparkSQLParser.NOT)


                self.state = 2381
                localctx.kind = self.match(SparkSQLParser.IN)
                self.state = 2382
                self.match(SparkSQLParser.T__1)
                self.state = 2383
                self.query()
                self.state = 2384
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 4:
                localctx = SparkSQLParser.RlikeContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2387
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2386
                    self.match(SparkSQLParser.NOT)


                self.state = 2389
                localctx.kind = self.match(SparkSQLParser.RLIKE)
                self.state = 2390
                localctx.pattern = self.valueExpression(0)
                pass

            elif la_ == 5:
                localctx = SparkSQLParser.LikeListContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 2392
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2391
                    self.match(SparkSQLParser.NOT)


                self.state = 2394
                localctx.kind = self.match(SparkSQLParser.LIKE)
                self.state = 2395
                localctx.quantifier = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.ALL or _la==SparkSQLParser.ANY or _la==SparkSQLParser.SOME):
                    localctx.quantifier = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 2409
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,307,self._ctx)
                if la_ == 1:
                    self.state = 2396
                    self.match(SparkSQLParser.T__1)
                    self.state = 2397
                    self.match(SparkSQLParser.T__2)
                    pass

                elif la_ == 2:
                    self.state = 2398
                    self.match(SparkSQLParser.T__1)
                    self.state = 2399
                    self.expression()
                    self.state = 2404
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 2400
                        self.match(SparkSQLParser.T__3)
                        self.state = 2401
                        self.expression()
                        self.state = 2406
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    self.state = 2407
                    self.match(SparkSQLParser.T__2)
                    pass


                pass

            elif la_ == 6:
                localctx = SparkSQLParser.LikeValueContext(self, localctx)
                self.enterOuterAlt(localctx, 6)
                self.state = 2412
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2411
                    self.match(SparkSQLParser.NOT)


                self.state = 2414
                localctx.kind = self.match(SparkSQLParser.LIKE)
                self.state = 2415
                localctx.pattern = self.valueExpression(0)
                self.state = 2418
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,309,self._ctx)
                if la_ == 1:
                    self.state = 2416
                    self.match(SparkSQLParser.ESCAPE)
                    self.state = 2417
                    localctx.escapeChar = self.match(SparkSQLParser.STRING)


                pass

            elif la_ == 7:
                localctx = SparkSQLParser.NullPredicateContext(self, localctx)
                self.enterOuterAlt(localctx, 7)
                self.state = 2420
                self.match(SparkSQLParser.IS)
                self.state = 2422
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2421
                    self.match(SparkSQLParser.NOT)


                self.state = 2424
                localctx.kind = self.match(SparkSQLParser.NULL)
                pass

            elif la_ == 8:
                localctx = SparkSQLParser.BooleanPredicateContext(self, localctx)
                self.enterOuterAlt(localctx, 8)
                self.state = 2425
                self.match(SparkSQLParser.IS)
                self.state = 2427
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2426
                    self.match(SparkSQLParser.NOT)


                self.state = 2429
                localctx.kind = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.FALSE or _la==SparkSQLParser.TRUE or _la==SparkSQLParser.UNKNOWN):
                    localctx.kind = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass

            elif la_ == 9:
                localctx = SparkSQLParser.DistinctFromContext(self, localctx)
                self.enterOuterAlt(localctx, 9)
                self.state = 2430
                self.match(SparkSQLParser.IS)
                self.state = 2432
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.NOT:
                    self.state = 2431
                    self.match(SparkSQLParser.NOT)


                self.state = 2434
                localctx.kind = self.match(SparkSQLParser.DISTINCT)
                self.state = 2435
                self.match(SparkSQLParser.FROM)
                self.state = 2436
                localctx.right = self.valueExpression(0)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ValueExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_valueExpression

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class ValueExpressionDefaultContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ValueExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def primaryExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.PrimaryExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterValueExpressionDefault" ):
                listener.enterValueExpressionDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitValueExpressionDefault" ):
                listener.exitValueExpressionDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitValueExpressionDefault" ):
                return visitor.visitValueExpressionDefault(self)
            else:
                return visitor.visitChildren(self)


    class ComparisonContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ValueExpressionContext
            super().__init__(parser)
            self.left = None # ValueExpressionContext
            self.right = None # ValueExpressionContext
            self.copyFrom(ctx)

        def comparisonOperator(self):
            return self.getTypedRuleContext(SparkSQLParser.ComparisonOperatorContext,0)

        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComparison" ):
                listener.enterComparison(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComparison" ):
                listener.exitComparison(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComparison" ):
                return visitor.visitComparison(self)
            else:
                return visitor.visitChildren(self)


    class ArithmeticBinaryContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ValueExpressionContext
            super().__init__(parser)
            self.left = None # ValueExpressionContext
            self.operator = None # Token
            self.right = None # ValueExpressionContext
            self.copyFrom(ctx)

        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,i)

        def ASTERISK(self):
            return self.getToken(SparkSQLParser.ASTERISK, 0)
        def SLASH(self):
            return self.getToken(SparkSQLParser.SLASH, 0)
        def PERCENT(self):
            return self.getToken(SparkSQLParser.PERCENT, 0)
        def DIV(self):
            return self.getToken(SparkSQLParser.DIV, 0)
        def PLUS(self):
            return self.getToken(SparkSQLParser.PLUS, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)
        def CONCAT_PIPE(self):
            return self.getToken(SparkSQLParser.CONCAT_PIPE, 0)
        def AMPERSAND(self):
            return self.getToken(SparkSQLParser.AMPERSAND, 0)
        def HAT(self):
            return self.getToken(SparkSQLParser.HAT, 0)
        def PIPE(self):
            return self.getToken(SparkSQLParser.PIPE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterArithmeticBinary" ):
                listener.enterArithmeticBinary(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitArithmeticBinary" ):
                listener.exitArithmeticBinary(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitArithmeticBinary" ):
                return visitor.visitArithmeticBinary(self)
            else:
                return visitor.visitChildren(self)


    class ArithmeticUnaryContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ValueExpressionContext
            super().__init__(parser)
            self.operator = None # Token
            self.copyFrom(ctx)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,0)

        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)
        def PLUS(self):
            return self.getToken(SparkSQLParser.PLUS, 0)
        def TILDE(self):
            return self.getToken(SparkSQLParser.TILDE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterArithmeticUnary" ):
                listener.enterArithmeticUnary(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitArithmeticUnary" ):
                listener.exitArithmeticUnary(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitArithmeticUnary" ):
                return visitor.visitArithmeticUnary(self)
            else:
                return visitor.visitChildren(self)



    def valueExpression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSQLParser.ValueExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 194
        self.enterRecursionRule(localctx, 194, self.RULE_valueExpression, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2443
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,314,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.ValueExpressionDefaultContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx

                self.state = 2440
                self.primaryExpression(0)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.ArithmeticUnaryContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2441
                localctx.operator = self._input.LT(1)
                _la = self._input.LA(1)
                if not(((((_la - 269)) & ~0x3f) == 0 and ((1 << (_la - 269)) & ((1 << (SparkSQLParser.PLUS - 269)) | (1 << (SparkSQLParser.MINUS - 269)) | (1 << (SparkSQLParser.TILDE - 269)))) != 0)):
                    localctx.operator = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 2442
                self.valueExpression(7)
                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 2466
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,316,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 2464
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,315,self._ctx)
                    if la_ == 1:
                        localctx = SparkSQLParser.ArithmeticBinaryContext(self, SparkSQLParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2445
                        if not self.precpred(self._ctx, 6):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 6)")
                        self.state = 2446
                        localctx.operator = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(_la==SparkSQLParser.DIV or ((((_la - 271)) & ~0x3f) == 0 and ((1 << (_la - 271)) & ((1 << (SparkSQLParser.ASTERISK - 271)) | (1 << (SparkSQLParser.SLASH - 271)) | (1 << (SparkSQLParser.PERCENT - 271)))) != 0)):
                            localctx.operator = self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 2447
                        localctx.right = self.valueExpression(7)
                        pass

                    elif la_ == 2:
                        localctx = SparkSQLParser.ArithmeticBinaryContext(self, SparkSQLParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2448
                        if not self.precpred(self._ctx, 5):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 5)")
                        self.state = 2449
                        localctx.operator = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 269)) & ~0x3f) == 0 and ((1 << (_la - 269)) & ((1 << (SparkSQLParser.PLUS - 269)) | (1 << (SparkSQLParser.MINUS - 269)) | (1 << (SparkSQLParser.CONCAT_PIPE - 269)))) != 0)):
                            localctx.operator = self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 2450
                        localctx.right = self.valueExpression(6)
                        pass

                    elif la_ == 3:
                        localctx = SparkSQLParser.ArithmeticBinaryContext(self, SparkSQLParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2451
                        if not self.precpred(self._ctx, 4):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 4)")
                        self.state = 2452
                        localctx.operator = self.match(SparkSQLParser.AMPERSAND)
                        self.state = 2453
                        localctx.right = self.valueExpression(5)
                        pass

                    elif la_ == 4:
                        localctx = SparkSQLParser.ArithmeticBinaryContext(self, SparkSQLParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2454
                        if not self.precpred(self._ctx, 3):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 3)")
                        self.state = 2455
                        localctx.operator = self.match(SparkSQLParser.HAT)
                        self.state = 2456
                        localctx.right = self.valueExpression(4)
                        pass

                    elif la_ == 5:
                        localctx = SparkSQLParser.ArithmeticBinaryContext(self, SparkSQLParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2457
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 2458
                        localctx.operator = self.match(SparkSQLParser.PIPE)
                        self.state = 2459
                        localctx.right = self.valueExpression(3)
                        pass

                    elif la_ == 6:
                        localctx = SparkSQLParser.ComparisonContext(self, SparkSQLParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2460
                        if not self.precpred(self._ctx, 1):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
                        self.state = 2461
                        self.comparisonOperator()
                        self.state = 2462
                        localctx.right = self.valueExpression(2)
                        pass

             
                self.state = 2468
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,316,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx


    class PrimaryExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_primaryExpression

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class StructContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self._namedExpression = None # NamedExpressionContext
            self.argument = list() # of NamedExpressionContexts
            self.copyFrom(ctx)

        def STRUCT(self):
            return self.getToken(SparkSQLParser.STRUCT, 0)
        def namedExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.NamedExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.NamedExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStruct" ):
                listener.enterStruct(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStruct" ):
                listener.exitStruct(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStruct" ):
                return visitor.visitStruct(self)
            else:
                return visitor.visitChildren(self)


    class DereferenceContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.base = None # PrimaryExpressionContext
            self.fieldName = None # IdentifierContext
            self.copyFrom(ctx)

        def primaryExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.PrimaryExpressionContext,0)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDereference" ):
                listener.enterDereference(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDereference" ):
                listener.exitDereference(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDereference" ):
                return visitor.visitDereference(self)
            else:
                return visitor.visitChildren(self)


    class SimpleCaseContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.value = None # ExpressionContext
            self.elseExpression = None # ExpressionContext
            self.copyFrom(ctx)

        def CASE(self):
            return self.getToken(SparkSQLParser.CASE, 0)
        def END(self):
            return self.getToken(SparkSQLParser.END, 0)
        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)

        def whenClause(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.WhenClauseContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.WhenClauseContext,i)

        def ELSE(self):
            return self.getToken(SparkSQLParser.ELSE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSimpleCase" ):
                listener.enterSimpleCase(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSimpleCase" ):
                listener.exitSimpleCase(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSimpleCase" ):
                return visitor.visitSimpleCase(self)
            else:
                return visitor.visitChildren(self)


    class ColumnReferenceContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumnReference" ):
                listener.enterColumnReference(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumnReference" ):
                listener.exitColumnReference(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumnReference" ):
                return visitor.visitColumnReference(self)
            else:
                return visitor.visitChildren(self)


    class RowConstructorContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def namedExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.NamedExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.NamedExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRowConstructor" ):
                listener.enterRowConstructor(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRowConstructor" ):
                listener.exitRowConstructor(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRowConstructor" ):
                return visitor.visitRowConstructor(self)
            else:
                return visitor.visitChildren(self)


    class LastContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def LAST(self):
            return self.getToken(SparkSQLParser.LAST, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)

        def IGNORE(self):
            return self.getToken(SparkSQLParser.IGNORE, 0)
        def NULLS(self):
            return self.getToken(SparkSQLParser.NULLS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLast" ):
                listener.enterLast(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLast" ):
                listener.exitLast(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLast" ):
                return visitor.visitLast(self)
            else:
                return visitor.visitChildren(self)


    class StarContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ASTERISK(self):
            return self.getToken(SparkSQLParser.ASTERISK, 0)
        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStar" ):
                listener.enterStar(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStar" ):
                listener.exitStar(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStar" ):
                return visitor.visitStar(self)
            else:
                return visitor.visitChildren(self)


    class OverlayContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.r_input = None # ValueExpressionContext
            self.replace = None # ValueExpressionContext
            self.position = None # ValueExpressionContext
            self.length = None # ValueExpressionContext
            self.copyFrom(ctx)

        def OVERLAY(self):
            return self.getToken(SparkSQLParser.OVERLAY, 0)
        def PLACING(self):
            return self.getToken(SparkSQLParser.PLACING, 0)
        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,i)

        def FOR(self):
            return self.getToken(SparkSQLParser.FOR, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOverlay" ):
                listener.enterOverlay(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOverlay" ):
                listener.exitOverlay(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOverlay" ):
                return visitor.visitOverlay(self)
            else:
                return visitor.visitChildren(self)


    class SubscriptContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.value = None # PrimaryExpressionContext
            self.index = None # ValueExpressionContext
            self.copyFrom(ctx)

        def primaryExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.PrimaryExpressionContext,0)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubscript" ):
                listener.enterSubscript(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubscript" ):
                listener.exitSubscript(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubscript" ):
                return visitor.visitSubscript(self)
            else:
                return visitor.visitChildren(self)


    class SubqueryExpressionContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSQLParser.QueryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubqueryExpression" ):
                listener.enterSubqueryExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubqueryExpression" ):
                listener.exitSubqueryExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubqueryExpression" ):
                return visitor.visitSubqueryExpression(self)
            else:
                return visitor.visitChildren(self)


    class SubstringContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.r_str = None # ValueExpressionContext
            self.pos = None # ValueExpressionContext
            self.r_len = None # ValueExpressionContext
            self.copyFrom(ctx)

        def SUBSTR(self):
            return self.getToken(SparkSQLParser.SUBSTR, 0)
        def SUBSTRING(self):
            return self.getToken(SparkSQLParser.SUBSTRING, 0)
        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,i)

        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def FOR(self):
            return self.getToken(SparkSQLParser.FOR, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubstring" ):
                listener.enterSubstring(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubstring" ):
                listener.exitSubstring(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubstring" ):
                return visitor.visitSubstring(self)
            else:
                return visitor.visitChildren(self)


    class CurrentDatetimeContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.name = None # Token
            self.copyFrom(ctx)

        def CURRENT_DATE(self):
            return self.getToken(SparkSQLParser.CURRENT_DATE, 0)
        def CURRENT_TIMESTAMP(self):
            return self.getToken(SparkSQLParser.CURRENT_TIMESTAMP, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCurrentDatetime" ):
                listener.enterCurrentDatetime(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCurrentDatetime" ):
                listener.exitCurrentDatetime(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCurrentDatetime" ):
                return visitor.visitCurrentDatetime(self)
            else:
                return visitor.visitChildren(self)


    class CastContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CAST(self):
            return self.getToken(SparkSQLParser.CAST, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)
        def dataType(self):
            return self.getTypedRuleContext(SparkSQLParser.DataTypeContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCast" ):
                listener.enterCast(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCast" ):
                listener.exitCast(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCast" ):
                return visitor.visitCast(self)
            else:
                return visitor.visitChildren(self)


    class ConstantDefaultContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def constant(self):
            return self.getTypedRuleContext(SparkSQLParser.ConstantContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterConstantDefault" ):
                listener.enterConstantDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitConstantDefault" ):
                listener.exitConstantDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitConstantDefault" ):
                return visitor.visitConstantDefault(self)
            else:
                return visitor.visitChildren(self)


    class LambdaContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLambda" ):
                listener.enterLambda(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLambda" ):
                listener.exitLambda(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLambda" ):
                return visitor.visitLambda(self)
            else:
                return visitor.visitChildren(self)


    class ParenthesizedExpressionContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterParenthesizedExpression" ):
                listener.enterParenthesizedExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitParenthesizedExpression" ):
                listener.exitParenthesizedExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitParenthesizedExpression" ):
                return visitor.visitParenthesizedExpression(self)
            else:
                return visitor.visitChildren(self)


    class ExtractContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.field = None # IdentifierContext
            self.source = None # ValueExpressionContext
            self.copyFrom(ctx)

        def EXTRACT(self):
            return self.getToken(SparkSQLParser.EXTRACT, 0)
        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExtract" ):
                listener.enterExtract(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExtract" ):
                listener.exitExtract(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExtract" ):
                return visitor.visitExtract(self)
            else:
                return visitor.visitChildren(self)


    class TrimContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.trimOption = None # Token
            self.trimStr = None # ValueExpressionContext
            self.srcStr = None # ValueExpressionContext
            self.copyFrom(ctx)

        def TRIM(self):
            return self.getToken(SparkSQLParser.TRIM, 0)
        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)
        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,i)

        def BOTH(self):
            return self.getToken(SparkSQLParser.BOTH, 0)
        def LEADING(self):
            return self.getToken(SparkSQLParser.LEADING, 0)
        def TRAILING(self):
            return self.getToken(SparkSQLParser.TRAILING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTrim" ):
                listener.enterTrim(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTrim" ):
                listener.exitTrim(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTrim" ):
                return visitor.visitTrim(self)
            else:
                return visitor.visitChildren(self)


    class FunctionCallContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self._expression = None # ExpressionContext
            self.argument = list() # of ExpressionContexts
            self.where = None # BooleanExpressionContext
            self.copyFrom(ctx)

        def functionName(self):
            return self.getTypedRuleContext(SparkSQLParser.FunctionNameContext,0)

        def FILTER(self):
            return self.getToken(SparkSQLParser.FILTER, 0)
        def WHERE(self):
            return self.getToken(SparkSQLParser.WHERE, 0)
        def OVER(self):
            return self.getToken(SparkSQLParser.OVER, 0)
        def windowSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.WindowSpecContext,0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanExpressionContext,0)

        def setQuantifier(self):
            return self.getTypedRuleContext(SparkSQLParser.SetQuantifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFunctionCall" ):
                listener.enterFunctionCall(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFunctionCall" ):
                listener.exitFunctionCall(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFunctionCall" ):
                return visitor.visitFunctionCall(self)
            else:
                return visitor.visitChildren(self)


    class SearchedCaseContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.elseExpression = None # ExpressionContext
            self.copyFrom(ctx)

        def CASE(self):
            return self.getToken(SparkSQLParser.CASE, 0)
        def END(self):
            return self.getToken(SparkSQLParser.END, 0)
        def whenClause(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.WhenClauseContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.WhenClauseContext,i)

        def ELSE(self):
            return self.getToken(SparkSQLParser.ELSE, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearchedCase" ):
                listener.enterSearchedCase(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearchedCase" ):
                listener.exitSearchedCase(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearchedCase" ):
                return visitor.visitSearchedCase(self)
            else:
                return visitor.visitChildren(self)


    class PositionContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.substr = None # ValueExpressionContext
            self.r_str = None # ValueExpressionContext
            self.copyFrom(ctx)

        def POSITION(self):
            return self.getToken(SparkSQLParser.POSITION, 0)
        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)
        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ValueExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPosition" ):
                listener.enterPosition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPosition" ):
                listener.exitPosition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPosition" ):
                return visitor.visitPosition(self)
            else:
                return visitor.visitChildren(self)


    class FirstContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def FIRST(self):
            return self.getToken(SparkSQLParser.FIRST, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)

        def IGNORE(self):
            return self.getToken(SparkSQLParser.IGNORE, 0)
        def NULLS(self):
            return self.getToken(SparkSQLParser.NULLS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFirst" ):
                listener.enterFirst(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFirst" ):
                listener.exitFirst(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFirst" ):
                return visitor.visitFirst(self)
            else:
                return visitor.visitChildren(self)



    def primaryExpression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSQLParser.PrimaryExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 196
        self.enterRecursionRule(localctx, 196, self.RULE_primaryExpression, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2653
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,336,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.CurrentDatetimeContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx

                self.state = 2470
                localctx.name = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.CURRENT_DATE or _la==SparkSQLParser.CURRENT_TIMESTAMP):
                    localctx.name = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.SearchedCaseContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2471
                self.match(SparkSQLParser.CASE)
                self.state = 2473 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2472
                    self.whenClause()
                    self.state = 2475 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSQLParser.WHEN):
                        break

                self.state = 2479
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.ELSE:
                    self.state = 2477
                    self.match(SparkSQLParser.ELSE)
                    self.state = 2478
                    localctx.elseExpression = self.expression()


                self.state = 2481
                self.match(SparkSQLParser.END)
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.SimpleCaseContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2483
                self.match(SparkSQLParser.CASE)
                self.state = 2484
                localctx.value = self.expression()
                self.state = 2486 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2485
                    self.whenClause()
                    self.state = 2488 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSQLParser.WHEN):
                        break

                self.state = 2492
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.ELSE:
                    self.state = 2490
                    self.match(SparkSQLParser.ELSE)
                    self.state = 2491
                    localctx.elseExpression = self.expression()


                self.state = 2494
                self.match(SparkSQLParser.END)
                pass

            elif la_ == 4:
                localctx = SparkSQLParser.CastContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2496
                self.match(SparkSQLParser.CAST)
                self.state = 2497
                self.match(SparkSQLParser.T__1)
                self.state = 2498
                self.expression()
                self.state = 2499
                self.match(SparkSQLParser.AS)
                self.state = 2500
                self.dataType()
                self.state = 2501
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 5:
                localctx = SparkSQLParser.StructContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2503
                self.match(SparkSQLParser.STRUCT)
                self.state = 2504
                self.match(SparkSQLParser.T__1)
                self.state = 2513
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.T__1) | (1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.PLUS - 256)) | (1 << (SparkSQLParser.MINUS - 256)) | (1 << (SparkSQLParser.ASTERISK - 256)) | (1 << (SparkSQLParser.TILDE - 256)) | (1 << (SparkSQLParser.STRING - 256)) | (1 << (SparkSQLParser.BIGINT_LITERAL - 256)) | (1 << (SparkSQLParser.SMALLINT_LITERAL - 256)) | (1 << (SparkSQLParser.TINYINT_LITERAL - 256)) | (1 << (SparkSQLParser.INTEGER_VALUE - 256)) | (1 << (SparkSQLParser.EXPONENT_VALUE - 256)) | (1 << (SparkSQLParser.DECIMAL_VALUE - 256)) | (1 << (SparkSQLParser.FLOAT_LITERAL - 256)) | (1 << (SparkSQLParser.DOUBLE_LITERAL - 256)) | (1 << (SparkSQLParser.BIGDECIMAL_LITERAL - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                    self.state = 2505
                    localctx._namedExpression = self.namedExpression()
                    localctx.argument.append(localctx._namedExpression)
                    self.state = 2510
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 2506
                        self.match(SparkSQLParser.T__3)
                        self.state = 2507
                        localctx._namedExpression = self.namedExpression()
                        localctx.argument.append(localctx._namedExpression)
                        self.state = 2512
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                self.state = 2515
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 6:
                localctx = SparkSQLParser.FirstContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2516
                self.match(SparkSQLParser.FIRST)
                self.state = 2517
                self.match(SparkSQLParser.T__1)
                self.state = 2518
                self.expression()
                self.state = 2521
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.IGNORE:
                    self.state = 2519
                    self.match(SparkSQLParser.IGNORE)
                    self.state = 2520
                    self.match(SparkSQLParser.NULLS)


                self.state = 2523
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 7:
                localctx = SparkSQLParser.LastContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2525
                self.match(SparkSQLParser.LAST)
                self.state = 2526
                self.match(SparkSQLParser.T__1)
                self.state = 2527
                self.expression()
                self.state = 2530
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.IGNORE:
                    self.state = 2528
                    self.match(SparkSQLParser.IGNORE)
                    self.state = 2529
                    self.match(SparkSQLParser.NULLS)


                self.state = 2532
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 8:
                localctx = SparkSQLParser.PositionContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2534
                self.match(SparkSQLParser.POSITION)
                self.state = 2535
                self.match(SparkSQLParser.T__1)
                self.state = 2536
                localctx.substr = self.valueExpression(0)
                self.state = 2537
                self.match(SparkSQLParser.IN)
                self.state = 2538
                localctx.r_str = self.valueExpression(0)
                self.state = 2539
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 9:
                localctx = SparkSQLParser.ConstantDefaultContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2541
                self.constant()
                pass

            elif la_ == 10:
                localctx = SparkSQLParser.StarContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2542
                self.match(SparkSQLParser.ASTERISK)
                pass

            elif la_ == 11:
                localctx = SparkSQLParser.StarContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2543
                self.qualifiedName()
                self.state = 2544
                self.match(SparkSQLParser.T__4)
                self.state = 2545
                self.match(SparkSQLParser.ASTERISK)
                pass

            elif la_ == 12:
                localctx = SparkSQLParser.RowConstructorContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2547
                self.match(SparkSQLParser.T__1)
                self.state = 2548
                self.namedExpression()
                self.state = 2551 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2549
                    self.match(SparkSQLParser.T__3)
                    self.state = 2550
                    self.namedExpression()
                    self.state = 2553 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSQLParser.T__3):
                        break

                self.state = 2555
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 13:
                localctx = SparkSQLParser.SubqueryExpressionContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2557
                self.match(SparkSQLParser.T__1)
                self.state = 2558
                self.query()
                self.state = 2559
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 14:
                localctx = SparkSQLParser.FunctionCallContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2561
                self.functionName()
                self.state = 2562
                self.match(SparkSQLParser.T__1)
                self.state = 2574
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.T__1) | (1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.PLUS - 256)) | (1 << (SparkSQLParser.MINUS - 256)) | (1 << (SparkSQLParser.ASTERISK - 256)) | (1 << (SparkSQLParser.TILDE - 256)) | (1 << (SparkSQLParser.STRING - 256)) | (1 << (SparkSQLParser.BIGINT_LITERAL - 256)) | (1 << (SparkSQLParser.SMALLINT_LITERAL - 256)) | (1 << (SparkSQLParser.TINYINT_LITERAL - 256)) | (1 << (SparkSQLParser.INTEGER_VALUE - 256)) | (1 << (SparkSQLParser.EXPONENT_VALUE - 256)) | (1 << (SparkSQLParser.DECIMAL_VALUE - 256)) | (1 << (SparkSQLParser.FLOAT_LITERAL - 256)) | (1 << (SparkSQLParser.DOUBLE_LITERAL - 256)) | (1 << (SparkSQLParser.BIGDECIMAL_LITERAL - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                    self.state = 2564
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,326,self._ctx)
                    if la_ == 1:
                        self.state = 2563
                        self.setQuantifier()


                    self.state = 2566
                    localctx._expression = self.expression()
                    localctx.argument.append(localctx._expression)
                    self.state = 2571
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 2567
                        self.match(SparkSQLParser.T__3)
                        self.state = 2568
                        localctx._expression = self.expression()
                        localctx.argument.append(localctx._expression)
                        self.state = 2573
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                self.state = 2576
                self.match(SparkSQLParser.T__2)
                self.state = 2583
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,329,self._ctx)
                if la_ == 1:
                    self.state = 2577
                    self.match(SparkSQLParser.FILTER)
                    self.state = 2578
                    self.match(SparkSQLParser.T__1)
                    self.state = 2579
                    self.match(SparkSQLParser.WHERE)
                    self.state = 2580
                    localctx.where = self.booleanExpression(0)
                    self.state = 2581
                    self.match(SparkSQLParser.T__2)


                self.state = 2587
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,330,self._ctx)
                if la_ == 1:
                    self.state = 2585
                    self.match(SparkSQLParser.OVER)
                    self.state = 2586
                    self.windowSpec()


                pass

            elif la_ == 15:
                localctx = SparkSQLParser.LambdaContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2589
                self.identifier()
                self.state = 2590
                self.match(SparkSQLParser.T__7)
                self.state = 2591
                self.expression()
                pass

            elif la_ == 16:
                localctx = SparkSQLParser.LambdaContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2593
                self.match(SparkSQLParser.T__1)
                self.state = 2594
                self.identifier()
                self.state = 2597 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2595
                    self.match(SparkSQLParser.T__3)
                    self.state = 2596
                    self.identifier()
                    self.state = 2599 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSQLParser.T__3):
                        break

                self.state = 2601
                self.match(SparkSQLParser.T__2)
                self.state = 2602
                self.match(SparkSQLParser.T__7)
                self.state = 2603
                self.expression()
                pass

            elif la_ == 17:
                localctx = SparkSQLParser.ColumnReferenceContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2605
                self.identifier()
                pass

            elif la_ == 18:
                localctx = SparkSQLParser.ParenthesizedExpressionContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2606
                self.match(SparkSQLParser.T__1)
                self.state = 2607
                self.expression()
                self.state = 2608
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 19:
                localctx = SparkSQLParser.ExtractContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2610
                self.match(SparkSQLParser.EXTRACT)
                self.state = 2611
                self.match(SparkSQLParser.T__1)
                self.state = 2612
                localctx.field = self.identifier()
                self.state = 2613
                self.match(SparkSQLParser.FROM)
                self.state = 2614
                localctx.source = self.valueExpression(0)
                self.state = 2615
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 20:
                localctx = SparkSQLParser.SubstringContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2617
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.SUBSTR or _la==SparkSQLParser.SUBSTRING):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 2618
                self.match(SparkSQLParser.T__1)
                self.state = 2619
                localctx.r_str = self.valueExpression(0)
                self.state = 2620
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.T__3 or _la==SparkSQLParser.FROM):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 2621
                localctx.pos = self.valueExpression(0)
                self.state = 2624
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.T__3 or _la==SparkSQLParser.FOR:
                    self.state = 2622
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.T__3 or _la==SparkSQLParser.FOR):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 2623
                    localctx.r_len = self.valueExpression(0)


                self.state = 2626
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 21:
                localctx = SparkSQLParser.TrimContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2628
                self.match(SparkSQLParser.TRIM)
                self.state = 2629
                self.match(SparkSQLParser.T__1)
                self.state = 2631
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,333,self._ctx)
                if la_ == 1:
                    self.state = 2630
                    localctx.trimOption = self._input.LT(1)
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.BOTH or _la==SparkSQLParser.LEADING or _la==SparkSQLParser.TRAILING):
                        localctx.trimOption = self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                self.state = 2634
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,334,self._ctx)
                if la_ == 1:
                    self.state = 2633
                    localctx.trimStr = self.valueExpression(0)


                self.state = 2636
                self.match(SparkSQLParser.FROM)
                self.state = 2637
                localctx.srcStr = self.valueExpression(0)
                self.state = 2638
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 22:
                localctx = SparkSQLParser.OverlayContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2640
                self.match(SparkSQLParser.OVERLAY)
                self.state = 2641
                self.match(SparkSQLParser.T__1)
                self.state = 2642
                localctx.r_input = self.valueExpression(0)
                self.state = 2643
                self.match(SparkSQLParser.PLACING)
                self.state = 2644
                localctx.replace = self.valueExpression(0)
                self.state = 2645
                self.match(SparkSQLParser.FROM)
                self.state = 2646
                localctx.position = self.valueExpression(0)
                self.state = 2649
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.FOR:
                    self.state = 2647
                    self.match(SparkSQLParser.FOR)
                    self.state = 2648
                    localctx.length = self.valueExpression(0)


                self.state = 2651
                self.match(SparkSQLParser.T__2)
                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 2665
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,338,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 2663
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,337,self._ctx)
                    if la_ == 1:
                        localctx = SparkSQLParser.SubscriptContext(self, SparkSQLParser.PrimaryExpressionContext(self, _parentctx, _parentState))
                        localctx.value = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_primaryExpression)
                        self.state = 2655
                        if not self.precpred(self._ctx, 8):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 8)")
                        self.state = 2656
                        self.match(SparkSQLParser.T__8)
                        self.state = 2657
                        localctx.index = self.valueExpression(0)
                        self.state = 2658
                        self.match(SparkSQLParser.T__9)
                        pass

                    elif la_ == 2:
                        localctx = SparkSQLParser.DereferenceContext(self, SparkSQLParser.PrimaryExpressionContext(self, _parentctx, _parentState))
                        localctx.base = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_primaryExpression)
                        self.state = 2660
                        if not self.precpred(self._ctx, 6):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 6)")
                        self.state = 2661
                        self.match(SparkSQLParser.T__4)
                        self.state = 2662
                        localctx.fieldName = self.identifier()
                        pass

             
                self.state = 2667
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,338,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx


    class ConstantContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_constant

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class NullLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNullLiteral" ):
                listener.enterNullLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNullLiteral" ):
                listener.exitNullLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNullLiteral" ):
                return visitor.visitNullLiteral(self)
            else:
                return visitor.visitChildren(self)


    class StringLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.STRING)
            else:
                return self.getToken(SparkSQLParser.STRING, i)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStringLiteral" ):
                listener.enterStringLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStringLiteral" ):
                listener.exitStringLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStringLiteral" ):
                return visitor.visitStringLiteral(self)
            else:
                return visitor.visitChildren(self)


    class TypeConstructorContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTypeConstructor" ):
                listener.enterTypeConstructor(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTypeConstructor" ):
                listener.exitTypeConstructor(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTypeConstructor" ):
                return visitor.visitTypeConstructor(self)
            else:
                return visitor.visitChildren(self)


    class IntervalLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def interval(self):
            return self.getTypedRuleContext(SparkSQLParser.IntervalContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIntervalLiteral" ):
                listener.enterIntervalLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIntervalLiteral" ):
                listener.exitIntervalLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIntervalLiteral" ):
                return visitor.visitIntervalLiteral(self)
            else:
                return visitor.visitChildren(self)


    class NumericLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def number(self):
            return self.getTypedRuleContext(SparkSQLParser.NumberContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNumericLiteral" ):
                listener.enterNumericLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNumericLiteral" ):
                listener.exitNumericLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNumericLiteral" ):
                return visitor.visitNumericLiteral(self)
            else:
                return visitor.visitChildren(self)


    class BooleanLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def booleanValue(self):
            return self.getTypedRuleContext(SparkSQLParser.BooleanValueContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBooleanLiteral" ):
                listener.enterBooleanLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBooleanLiteral" ):
                listener.exitBooleanLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBooleanLiteral" ):
                return visitor.visitBooleanLiteral(self)
            else:
                return visitor.visitChildren(self)



    def constant(self):

        localctx = SparkSQLParser.ConstantContext(self, self._ctx, self.state)
        self.enterRule(localctx, 198, self.RULE_constant)
        try:
            self.state = 2680
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,340,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.NullLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2668
                self.match(SparkSQLParser.NULL)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.IntervalLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2669
                self.interval()
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.TypeConstructorContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2670
                self.identifier()
                self.state = 2671
                self.match(SparkSQLParser.STRING)
                pass

            elif la_ == 4:
                localctx = SparkSQLParser.NumericLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2673
                self.number()
                pass

            elif la_ == 5:
                localctx = SparkSQLParser.BooleanLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 2674
                self.booleanValue()
                pass

            elif la_ == 6:
                localctx = SparkSQLParser.StringLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 6)
                self.state = 2676 
                self._errHandler.sync(self)
                _alt = 1
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt == 1:
                        self.state = 2675
                        self.match(SparkSQLParser.STRING)

                    else:
                        raise NoViableAltException(self)
                    self.state = 2678 
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,339,self._ctx)

                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ComparisonOperatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def EQ(self):
            return self.getToken(SparkSQLParser.EQ, 0)

        def NEQ(self):
            return self.getToken(SparkSQLParser.NEQ, 0)

        def NEQJ(self):
            return self.getToken(SparkSQLParser.NEQJ, 0)

        def LT(self):
            return self.getToken(SparkSQLParser.LT, 0)

        def LTE(self):
            return self.getToken(SparkSQLParser.LTE, 0)

        def GT(self):
            return self.getToken(SparkSQLParser.GT, 0)

        def GTE(self):
            return self.getToken(SparkSQLParser.GTE, 0)

        def NSEQ(self):
            return self.getToken(SparkSQLParser.NSEQ, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_comparisonOperator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComparisonOperator" ):
                listener.enterComparisonOperator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComparisonOperator" ):
                listener.exitComparisonOperator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComparisonOperator" ):
                return visitor.visitComparisonOperator(self)
            else:
                return visitor.visitChildren(self)




    def comparisonOperator(self):

        localctx = SparkSQLParser.ComparisonOperatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 200, self.RULE_comparisonOperator)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2682
            _la = self._input.LA(1)
            if not(((((_la - 261)) & ~0x3f) == 0 and ((1 << (_la - 261)) & ((1 << (SparkSQLParser.EQ - 261)) | (1 << (SparkSQLParser.NSEQ - 261)) | (1 << (SparkSQLParser.NEQ - 261)) | (1 << (SparkSQLParser.NEQJ - 261)) | (1 << (SparkSQLParser.LT - 261)) | (1 << (SparkSQLParser.LTE - 261)) | (1 << (SparkSQLParser.GT - 261)) | (1 << (SparkSQLParser.GTE - 261)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ArithmeticOperatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def PLUS(self):
            return self.getToken(SparkSQLParser.PLUS, 0)

        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def ASTERISK(self):
            return self.getToken(SparkSQLParser.ASTERISK, 0)

        def SLASH(self):
            return self.getToken(SparkSQLParser.SLASH, 0)

        def PERCENT(self):
            return self.getToken(SparkSQLParser.PERCENT, 0)

        def DIV(self):
            return self.getToken(SparkSQLParser.DIV, 0)

        def TILDE(self):
            return self.getToken(SparkSQLParser.TILDE, 0)

        def AMPERSAND(self):
            return self.getToken(SparkSQLParser.AMPERSAND, 0)

        def PIPE(self):
            return self.getToken(SparkSQLParser.PIPE, 0)

        def CONCAT_PIPE(self):
            return self.getToken(SparkSQLParser.CONCAT_PIPE, 0)

        def HAT(self):
            return self.getToken(SparkSQLParser.HAT, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_arithmeticOperator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterArithmeticOperator" ):
                listener.enterArithmeticOperator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitArithmeticOperator" ):
                listener.exitArithmeticOperator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitArithmeticOperator" ):
                return visitor.visitArithmeticOperator(self)
            else:
                return visitor.visitChildren(self)




    def arithmeticOperator(self):

        localctx = SparkSQLParser.ArithmeticOperatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 202, self.RULE_arithmeticOperator)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2684
            _la = self._input.LA(1)
            if not(_la==SparkSQLParser.DIV or ((((_la - 269)) & ~0x3f) == 0 and ((1 << (_la - 269)) & ((1 << (SparkSQLParser.PLUS - 269)) | (1 << (SparkSQLParser.MINUS - 269)) | (1 << (SparkSQLParser.ASTERISK - 269)) | (1 << (SparkSQLParser.SLASH - 269)) | (1 << (SparkSQLParser.PERCENT - 269)) | (1 << (SparkSQLParser.TILDE - 269)) | (1 << (SparkSQLParser.AMPERSAND - 269)) | (1 << (SparkSQLParser.PIPE - 269)) | (1 << (SparkSQLParser.CONCAT_PIPE - 269)) | (1 << (SparkSQLParser.HAT - 269)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class PredicateOperatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def OR(self):
            return self.getToken(SparkSQLParser.OR, 0)

        def AND(self):
            return self.getToken(SparkSQLParser.AND, 0)

        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_predicateOperator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPredicateOperator" ):
                listener.enterPredicateOperator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPredicateOperator" ):
                listener.exitPredicateOperator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPredicateOperator" ):
                return visitor.visitPredicateOperator(self)
            else:
                return visitor.visitChildren(self)




    def predicateOperator(self):

        localctx = SparkSQLParser.PredicateOperatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 204, self.RULE_predicateOperator)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2686
            _la = self._input.LA(1)
            if not(_la==SparkSQLParser.AND or ((((_la - 112)) & ~0x3f) == 0 and ((1 << (_la - 112)) & ((1 << (SparkSQLParser.IN - 112)) | (1 << (SparkSQLParser.NOT - 112)) | (1 << (SparkSQLParser.OR - 112)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class BooleanValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def TRUE(self):
            return self.getToken(SparkSQLParser.TRUE, 0)

        def FALSE(self):
            return self.getToken(SparkSQLParser.FALSE, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_booleanValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBooleanValue" ):
                listener.enterBooleanValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBooleanValue" ):
                listener.exitBooleanValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBooleanValue" ):
                return visitor.visitBooleanValue(self)
            else:
                return visitor.visitChildren(self)




    def booleanValue(self):

        localctx = SparkSQLParser.BooleanValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 206, self.RULE_booleanValue)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2688
            _la = self._input.LA(1)
            if not(_la==SparkSQLParser.FALSE or _la==SparkSQLParser.TRUE):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class IntervalContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INTERVAL(self):
            return self.getToken(SparkSQLParser.INTERVAL, 0)

        def errorCapturingMultiUnitsInterval(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingMultiUnitsIntervalContext,0)


        def errorCapturingUnitToUnitInterval(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingUnitToUnitIntervalContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_interval

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInterval" ):
                listener.enterInterval(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInterval" ):
                listener.exitInterval(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInterval" ):
                return visitor.visitInterval(self)
            else:
                return visitor.visitChildren(self)




    def interval(self):

        localctx = SparkSQLParser.IntervalContext(self, self._ctx, self.state)
        self.enterRule(localctx, 208, self.RULE_interval)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2690
            self.match(SparkSQLParser.INTERVAL)
            self.state = 2693
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,341,self._ctx)
            if la_ == 1:
                self.state = 2691
                self.errorCapturingMultiUnitsInterval()

            elif la_ == 2:
                self.state = 2692
                self.errorCapturingUnitToUnitInterval()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ErrorCapturingMultiUnitsIntervalContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def multiUnitsInterval(self):
            return self.getTypedRuleContext(SparkSQLParser.MultiUnitsIntervalContext,0)


        def unitToUnitInterval(self):
            return self.getTypedRuleContext(SparkSQLParser.UnitToUnitIntervalContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_errorCapturingMultiUnitsInterval

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterErrorCapturingMultiUnitsInterval" ):
                listener.enterErrorCapturingMultiUnitsInterval(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitErrorCapturingMultiUnitsInterval" ):
                listener.exitErrorCapturingMultiUnitsInterval(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitErrorCapturingMultiUnitsInterval" ):
                return visitor.visitErrorCapturingMultiUnitsInterval(self)
            else:
                return visitor.visitChildren(self)




    def errorCapturingMultiUnitsInterval(self):

        localctx = SparkSQLParser.ErrorCapturingMultiUnitsIntervalContext(self, self._ctx, self.state)
        self.enterRule(localctx, 210, self.RULE_errorCapturingMultiUnitsInterval)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2695
            self.multiUnitsInterval()
            self.state = 2697
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,342,self._ctx)
            if la_ == 1:
                self.state = 2696
                self.unitToUnitInterval()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class MultiUnitsIntervalContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._identifier = None # IdentifierContext
            self.unit = list() # of IdentifierContexts

        def intervalValue(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IntervalValueContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IntervalValueContext,i)


        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_multiUnitsInterval

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMultiUnitsInterval" ):
                listener.enterMultiUnitsInterval(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMultiUnitsInterval" ):
                listener.exitMultiUnitsInterval(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMultiUnitsInterval" ):
                return visitor.visitMultiUnitsInterval(self)
            else:
                return visitor.visitChildren(self)




    def multiUnitsInterval(self):

        localctx = SparkSQLParser.MultiUnitsIntervalContext(self, self._ctx, self.state)
        self.enterRule(localctx, 212, self.RULE_multiUnitsInterval)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2702 
            self._errHandler.sync(self)
            _alt = 1
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt == 1:
                    self.state = 2699
                    self.intervalValue()
                    self.state = 2700
                    localctx._identifier = self.identifier()
                    localctx.unit.append(localctx._identifier)

                else:
                    raise NoViableAltException(self)
                self.state = 2704 
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,343,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ErrorCapturingUnitToUnitIntervalContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.body = None # UnitToUnitIntervalContext
            self.error1 = None # MultiUnitsIntervalContext
            self.error2 = None # UnitToUnitIntervalContext

        def unitToUnitInterval(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.UnitToUnitIntervalContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.UnitToUnitIntervalContext,i)


        def multiUnitsInterval(self):
            return self.getTypedRuleContext(SparkSQLParser.MultiUnitsIntervalContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_errorCapturingUnitToUnitInterval

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterErrorCapturingUnitToUnitInterval" ):
                listener.enterErrorCapturingUnitToUnitInterval(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitErrorCapturingUnitToUnitInterval" ):
                listener.exitErrorCapturingUnitToUnitInterval(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitErrorCapturingUnitToUnitInterval" ):
                return visitor.visitErrorCapturingUnitToUnitInterval(self)
            else:
                return visitor.visitChildren(self)




    def errorCapturingUnitToUnitInterval(self):

        localctx = SparkSQLParser.ErrorCapturingUnitToUnitIntervalContext(self, self._ctx, self.state)
        self.enterRule(localctx, 214, self.RULE_errorCapturingUnitToUnitInterval)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2706
            localctx.body = self.unitToUnitInterval()
            self.state = 2709
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,344,self._ctx)
            if la_ == 1:
                self.state = 2707
                localctx.error1 = self.multiUnitsInterval()

            elif la_ == 2:
                self.state = 2708
                localctx.error2 = self.unitToUnitInterval()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class UnitToUnitIntervalContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.value = None # IntervalValueContext
            self.r_from = None # IdentifierContext
            self.to = None # IdentifierContext

        def TO(self):
            return self.getToken(SparkSQLParser.TO, 0)

        def intervalValue(self):
            return self.getTypedRuleContext(SparkSQLParser.IntervalValueContext,0)


        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_unitToUnitInterval

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnitToUnitInterval" ):
                listener.enterUnitToUnitInterval(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnitToUnitInterval" ):
                listener.exitUnitToUnitInterval(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnitToUnitInterval" ):
                return visitor.visitUnitToUnitInterval(self)
            else:
                return visitor.visitChildren(self)




    def unitToUnitInterval(self):

        localctx = SparkSQLParser.UnitToUnitIntervalContext(self, self._ctx, self.state)
        self.enterRule(localctx, 216, self.RULE_unitToUnitInterval)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2711
            localctx.value = self.intervalValue()
            self.state = 2712
            localctx.r_from = self.identifier()
            self.state = 2713
            self.match(SparkSQLParser.TO)
            self.state = 2714
            localctx.to = self.identifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class IntervalValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INTEGER_VALUE(self):
            return self.getToken(SparkSQLParser.INTEGER_VALUE, 0)

        def DECIMAL_VALUE(self):
            return self.getToken(SparkSQLParser.DECIMAL_VALUE, 0)

        def PLUS(self):
            return self.getToken(SparkSQLParser.PLUS, 0)

        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def STRING(self):
            return self.getToken(SparkSQLParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_intervalValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIntervalValue" ):
                listener.enterIntervalValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIntervalValue" ):
                listener.exitIntervalValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIntervalValue" ):
                return visitor.visitIntervalValue(self)
            else:
                return visitor.visitChildren(self)




    def intervalValue(self):

        localctx = SparkSQLParser.IntervalValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 218, self.RULE_intervalValue)
        self._la = 0 # Token type
        try:
            self.state = 2721
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.PLUS, SparkSQLParser.MINUS, SparkSQLParser.INTEGER_VALUE, SparkSQLParser.DECIMAL_VALUE]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2717
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.PLUS or _la==SparkSQLParser.MINUS:
                    self.state = 2716
                    _la = self._input.LA(1)
                    if not(_la==SparkSQLParser.PLUS or _la==SparkSQLParser.MINUS):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                self.state = 2719
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.INTEGER_VALUE or _la==SparkSQLParser.DECIMAL_VALUE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass
            elif token in [SparkSQLParser.STRING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2720
                self.match(SparkSQLParser.STRING)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ColPositionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.position = None # Token
            self.afterCol = None # ErrorCapturingIdentifierContext

        def FIRST(self):
            return self.getToken(SparkSQLParser.FIRST, 0)

        def AFTER(self):
            return self.getToken(SparkSQLParser.AFTER, 0)

        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_colPosition

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColPosition" ):
                listener.enterColPosition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColPosition" ):
                listener.exitColPosition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColPosition" ):
                return visitor.visitColPosition(self)
            else:
                return visitor.visitChildren(self)




    def colPosition(self):

        localctx = SparkSQLParser.ColPositionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 220, self.RULE_colPosition)
        try:
            self.state = 2726
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.FIRST]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2723
                localctx.position = self.match(SparkSQLParser.FIRST)
                pass
            elif token in [SparkSQLParser.AFTER]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2724
                localctx.position = self.match(SparkSQLParser.AFTER)
                self.state = 2725
                localctx.afterCol = self.errorCapturingIdentifier()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class DataTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_dataType

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class ComplexDataTypeContext(DataTypeContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.DataTypeContext
            super().__init__(parser)
            self.r_complex = None # Token
            self.copyFrom(ctx)

        def LT(self):
            return self.getToken(SparkSQLParser.LT, 0)
        def dataType(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.DataTypeContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.DataTypeContext,i)

        def GT(self):
            return self.getToken(SparkSQLParser.GT, 0)
        def ARRAY(self):
            return self.getToken(SparkSQLParser.ARRAY, 0)
        def MAP(self):
            return self.getToken(SparkSQLParser.MAP, 0)
        def STRUCT(self):
            return self.getToken(SparkSQLParser.STRUCT, 0)
        def NEQ(self):
            return self.getToken(SparkSQLParser.NEQ, 0)
        def complexColTypeList(self):
            return self.getTypedRuleContext(SparkSQLParser.ComplexColTypeListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComplexDataType" ):
                listener.enterComplexDataType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComplexDataType" ):
                listener.exitComplexDataType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComplexDataType" ):
                return visitor.visitComplexDataType(self)
            else:
                return visitor.visitChildren(self)


    class PrimitiveDataTypeContext(DataTypeContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.DataTypeContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)

        def INTEGER_VALUE(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.INTEGER_VALUE)
            else:
                return self.getToken(SparkSQLParser.INTEGER_VALUE, i)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPrimitiveDataType" ):
                listener.enterPrimitiveDataType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPrimitiveDataType" ):
                listener.exitPrimitiveDataType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPrimitiveDataType" ):
                return visitor.visitPrimitiveDataType(self)
            else:
                return visitor.visitChildren(self)



    def dataType(self):

        localctx = SparkSQLParser.DataTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 222, self.RULE_dataType)
        self._la = 0 # Token type
        try:
            self.state = 2762
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,352,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.ComplexDataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2728
                localctx.r_complex = self.match(SparkSQLParser.ARRAY)
                self.state = 2729
                self.match(SparkSQLParser.LT)
                self.state = 2730
                self.dataType()
                self.state = 2731
                self.match(SparkSQLParser.GT)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.ComplexDataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2733
                localctx.r_complex = self.match(SparkSQLParser.MAP)
                self.state = 2734
                self.match(SparkSQLParser.LT)
                self.state = 2735
                self.dataType()
                self.state = 2736
                self.match(SparkSQLParser.T__3)
                self.state = 2737
                self.dataType()
                self.state = 2738
                self.match(SparkSQLParser.GT)
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.ComplexDataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2740
                localctx.r_complex = self.match(SparkSQLParser.STRUCT)
                self.state = 2747
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSQLParser.LT]:
                    self.state = 2741
                    self.match(SparkSQLParser.LT)
                    self.state = 2743
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSQLParser.ADD) | (1 << SparkSQLParser.AFTER) | (1 << SparkSQLParser.ALL) | (1 << SparkSQLParser.ALTER) | (1 << SparkSQLParser.ANALYZE) | (1 << SparkSQLParser.AND) | (1 << SparkSQLParser.ANTI) | (1 << SparkSQLParser.ANY) | (1 << SparkSQLParser.ARCHIVE) | (1 << SparkSQLParser.ARRAY) | (1 << SparkSQLParser.AS) | (1 << SparkSQLParser.ASC) | (1 << SparkSQLParser.AT) | (1 << SparkSQLParser.AUTHORIZATION) | (1 << SparkSQLParser.BETWEEN) | (1 << SparkSQLParser.BOTH) | (1 << SparkSQLParser.BUCKET) | (1 << SparkSQLParser.BUCKETS) | (1 << SparkSQLParser.BY) | (1 << SparkSQLParser.CACHE) | (1 << SparkSQLParser.CASCADE) | (1 << SparkSQLParser.CASE) | (1 << SparkSQLParser.CAST) | (1 << SparkSQLParser.CHANGE) | (1 << SparkSQLParser.CHECK) | (1 << SparkSQLParser.CLEAR) | (1 << SparkSQLParser.CLUSTER) | (1 << SparkSQLParser.CLUSTERED) | (1 << SparkSQLParser.CODEGEN) | (1 << SparkSQLParser.COLLATE) | (1 << SparkSQLParser.COLLECTION) | (1 << SparkSQLParser.COLUMN) | (1 << SparkSQLParser.COLUMNS) | (1 << SparkSQLParser.COMMENT) | (1 << SparkSQLParser.COMMIT) | (1 << SparkSQLParser.COMPACT) | (1 << SparkSQLParser.COMPACTIONS) | (1 << SparkSQLParser.COMPUTE) | (1 << SparkSQLParser.CONCATENATE) | (1 << SparkSQLParser.CONSTRAINT) | (1 << SparkSQLParser.COST) | (1 << SparkSQLParser.CREATE) | (1 << SparkSQLParser.CROSS) | (1 << SparkSQLParser.CUBE) | (1 << SparkSQLParser.CURRENT) | (1 << SparkSQLParser.CURRENT_DATE) | (1 << SparkSQLParser.CURRENT_TIME) | (1 << SparkSQLParser.CURRENT_TIMESTAMP) | (1 << SparkSQLParser.CURRENT_USER) | (1 << SparkSQLParser.DATA) | (1 << SparkSQLParser.DATABASE) | (1 << SparkSQLParser.DATABASES))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSQLParser.DBPROPERTIES - 64)) | (1 << (SparkSQLParser.DEFINED - 64)) | (1 << (SparkSQLParser.DELETE - 64)) | (1 << (SparkSQLParser.DELIMITED - 64)) | (1 << (SparkSQLParser.DESC - 64)) | (1 << (SparkSQLParser.DESCRIBE - 64)) | (1 << (SparkSQLParser.DFS - 64)) | (1 << (SparkSQLParser.DIRECTORIES - 64)) | (1 << (SparkSQLParser.DIRECTORY - 64)) | (1 << (SparkSQLParser.DISTINCT - 64)) | (1 << (SparkSQLParser.DISTRIBUTE - 64)) | (1 << (SparkSQLParser.DIV - 64)) | (1 << (SparkSQLParser.DROP - 64)) | (1 << (SparkSQLParser.ELSE - 64)) | (1 << (SparkSQLParser.END - 64)) | (1 << (SparkSQLParser.ESCAPE - 64)) | (1 << (SparkSQLParser.ESCAPED - 64)) | (1 << (SparkSQLParser.EXCEPT - 64)) | (1 << (SparkSQLParser.EXCHANGE - 64)) | (1 << (SparkSQLParser.EXISTS - 64)) | (1 << (SparkSQLParser.EXPLAIN - 64)) | (1 << (SparkSQLParser.EXPORT - 64)) | (1 << (SparkSQLParser.EXTENDED - 64)) | (1 << (SparkSQLParser.EXTERNAL - 64)) | (1 << (SparkSQLParser.EXTRACT - 64)) | (1 << (SparkSQLParser.FALSE - 64)) | (1 << (SparkSQLParser.FETCH - 64)) | (1 << (SparkSQLParser.FIELDS - 64)) | (1 << (SparkSQLParser.FILTER - 64)) | (1 << (SparkSQLParser.FILEFORMAT - 64)) | (1 << (SparkSQLParser.FIRST - 64)) | (1 << (SparkSQLParser.FOLLOWING - 64)) | (1 << (SparkSQLParser.FOR - 64)) | (1 << (SparkSQLParser.FOREIGN - 64)) | (1 << (SparkSQLParser.FORMAT - 64)) | (1 << (SparkSQLParser.FORMATTED - 64)) | (1 << (SparkSQLParser.FROM - 64)) | (1 << (SparkSQLParser.FULL - 64)) | (1 << (SparkSQLParser.FUNCTION - 64)) | (1 << (SparkSQLParser.FUNCTIONS - 64)) | (1 << (SparkSQLParser.GLOBAL - 64)) | (1 << (SparkSQLParser.GRANT - 64)) | (1 << (SparkSQLParser.GROUP - 64)) | (1 << (SparkSQLParser.GROUPING - 64)) | (1 << (SparkSQLParser.HAVING - 64)) | (1 << (SparkSQLParser.IF - 64)) | (1 << (SparkSQLParser.IGNORE - 64)) | (1 << (SparkSQLParser.IMPORT - 64)) | (1 << (SparkSQLParser.IN - 64)) | (1 << (SparkSQLParser.INDEX - 64)) | (1 << (SparkSQLParser.INDEXES - 64)) | (1 << (SparkSQLParser.INNER - 64)) | (1 << (SparkSQLParser.INPATH - 64)) | (1 << (SparkSQLParser.INPUTFORMAT - 64)) | (1 << (SparkSQLParser.INSERT - 64)) | (1 << (SparkSQLParser.INTERSECT - 64)) | (1 << (SparkSQLParser.INTERVAL - 64)) | (1 << (SparkSQLParser.INTO - 64)) | (1 << (SparkSQLParser.IS - 64)) | (1 << (SparkSQLParser.ITEMS - 64)) | (1 << (SparkSQLParser.JOIN - 64)) | (1 << (SparkSQLParser.KEYS - 64)) | (1 << (SparkSQLParser.LAST - 64)) | (1 << (SparkSQLParser.LATERAL - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSQLParser.LAZY - 128)) | (1 << (SparkSQLParser.LEADING - 128)) | (1 << (SparkSQLParser.LEFT - 128)) | (1 << (SparkSQLParser.LIKE - 128)) | (1 << (SparkSQLParser.LIMIT - 128)) | (1 << (SparkSQLParser.LINES - 128)) | (1 << (SparkSQLParser.LIST - 128)) | (1 << (SparkSQLParser.LOAD - 128)) | (1 << (SparkSQLParser.LOCAL - 128)) | (1 << (SparkSQLParser.LOCATION - 128)) | (1 << (SparkSQLParser.LOCK - 128)) | (1 << (SparkSQLParser.LOCKS - 128)) | (1 << (SparkSQLParser.LOGICAL - 128)) | (1 << (SparkSQLParser.MACRO - 128)) | (1 << (SparkSQLParser.MAP - 128)) | (1 << (SparkSQLParser.MATCHED - 128)) | (1 << (SparkSQLParser.MERGE - 128)) | (1 << (SparkSQLParser.MSCK - 128)) | (1 << (SparkSQLParser.NAMESPACE - 128)) | (1 << (SparkSQLParser.NAMESPACES - 128)) | (1 << (SparkSQLParser.NATURAL - 128)) | (1 << (SparkSQLParser.NO - 128)) | (1 << (SparkSQLParser.NOT - 128)) | (1 << (SparkSQLParser.NULL - 128)) | (1 << (SparkSQLParser.NULLS - 128)) | (1 << (SparkSQLParser.OF - 128)) | (1 << (SparkSQLParser.ON - 128)) | (1 << (SparkSQLParser.ONLY - 128)) | (1 << (SparkSQLParser.OPTION - 128)) | (1 << (SparkSQLParser.OPTIONS - 128)) | (1 << (SparkSQLParser.OR - 128)) | (1 << (SparkSQLParser.ORDER - 128)) | (1 << (SparkSQLParser.OUT - 128)) | (1 << (SparkSQLParser.OUTER - 128)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 128)) | (1 << (SparkSQLParser.OVER - 128)) | (1 << (SparkSQLParser.OVERLAPS - 128)) | (1 << (SparkSQLParser.OVERLAY - 128)) | (1 << (SparkSQLParser.OVERWRITE - 128)) | (1 << (SparkSQLParser.PARTITION - 128)) | (1 << (SparkSQLParser.PARTITIONED - 128)) | (1 << (SparkSQLParser.PARTITIONS - 128)) | (1 << (SparkSQLParser.PERCENTLIT - 128)) | (1 << (SparkSQLParser.PIVOT - 128)) | (1 << (SparkSQLParser.PLACING - 128)) | (1 << (SparkSQLParser.POSITION - 128)) | (1 << (SparkSQLParser.PRECEDING - 128)) | (1 << (SparkSQLParser.PRIMARY - 128)) | (1 << (SparkSQLParser.PRINCIPALS - 128)) | (1 << (SparkSQLParser.PROPERTIES - 128)) | (1 << (SparkSQLParser.PURGE - 128)) | (1 << (SparkSQLParser.QUERY - 128)) | (1 << (SparkSQLParser.RANGE - 128)) | (1 << (SparkSQLParser.RECORDREADER - 128)) | (1 << (SparkSQLParser.RECORDWRITER - 128)) | (1 << (SparkSQLParser.RECOVER - 128)) | (1 << (SparkSQLParser.REDUCE - 128)) | (1 << (SparkSQLParser.REFERENCES - 128)) | (1 << (SparkSQLParser.REFRESH - 128)) | (1 << (SparkSQLParser.RENAME - 128)) | (1 << (SparkSQLParser.REPAIR - 128)) | (1 << (SparkSQLParser.REPLACE - 128)) | (1 << (SparkSQLParser.RESET - 128)) | (1 << (SparkSQLParser.RESTRICT - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSQLParser.REVOKE - 192)) | (1 << (SparkSQLParser.RIGHT - 192)) | (1 << (SparkSQLParser.RLIKE - 192)) | (1 << (SparkSQLParser.ROLE - 192)) | (1 << (SparkSQLParser.ROLES - 192)) | (1 << (SparkSQLParser.ROLLBACK - 192)) | (1 << (SparkSQLParser.ROLLUP - 192)) | (1 << (SparkSQLParser.ROW - 192)) | (1 << (SparkSQLParser.ROWS - 192)) | (1 << (SparkSQLParser.SCHEMA - 192)) | (1 << (SparkSQLParser.SELECT - 192)) | (1 << (SparkSQLParser.SEMI - 192)) | (1 << (SparkSQLParser.SEPARATED - 192)) | (1 << (SparkSQLParser.SERDE - 192)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 192)) | (1 << (SparkSQLParser.SESSION_USER - 192)) | (1 << (SparkSQLParser.SET - 192)) | (1 << (SparkSQLParser.SETMINUS - 192)) | (1 << (SparkSQLParser.SETS - 192)) | (1 << (SparkSQLParser.SHOW - 192)) | (1 << (SparkSQLParser.SKEWED - 192)) | (1 << (SparkSQLParser.SOME - 192)) | (1 << (SparkSQLParser.SORT - 192)) | (1 << (SparkSQLParser.SORTED - 192)) | (1 << (SparkSQLParser.START - 192)) | (1 << (SparkSQLParser.STATISTICS - 192)) | (1 << (SparkSQLParser.STORED - 192)) | (1 << (SparkSQLParser.STRATIFY - 192)) | (1 << (SparkSQLParser.STRUCT - 192)) | (1 << (SparkSQLParser.SUBSTR - 192)) | (1 << (SparkSQLParser.SUBSTRING - 192)) | (1 << (SparkSQLParser.TABLE - 192)) | (1 << (SparkSQLParser.TABLES - 192)) | (1 << (SparkSQLParser.TABLESAMPLE - 192)) | (1 << (SparkSQLParser.TBLPROPERTIES - 192)) | (1 << (SparkSQLParser.TEMPORARY - 192)) | (1 << (SparkSQLParser.TERMINATED - 192)) | (1 << (SparkSQLParser.THEN - 192)) | (1 << (SparkSQLParser.TIME - 192)) | (1 << (SparkSQLParser.TO - 192)) | (1 << (SparkSQLParser.TOUCH - 192)) | (1 << (SparkSQLParser.TRAILING - 192)) | (1 << (SparkSQLParser.TRANSACTION - 192)) | (1 << (SparkSQLParser.TRANSACTIONS - 192)) | (1 << (SparkSQLParser.TRANSFORM - 192)) | (1 << (SparkSQLParser.TRIM - 192)) | (1 << (SparkSQLParser.TRUE - 192)) | (1 << (SparkSQLParser.TRUNCATE - 192)) | (1 << (SparkSQLParser.TYPE - 192)) | (1 << (SparkSQLParser.UNARCHIVE - 192)) | (1 << (SparkSQLParser.UNBOUNDED - 192)) | (1 << (SparkSQLParser.UNCACHE - 192)) | (1 << (SparkSQLParser.UNION - 192)) | (1 << (SparkSQLParser.UNIQUE - 192)) | (1 << (SparkSQLParser.UNKNOWN - 192)) | (1 << (SparkSQLParser.UNLOCK - 192)) | (1 << (SparkSQLParser.UNSET - 192)) | (1 << (SparkSQLParser.UPDATE - 192)) | (1 << (SparkSQLParser.USE - 192)) | (1 << (SparkSQLParser.USER - 192)) | (1 << (SparkSQLParser.USING - 192)) | (1 << (SparkSQLParser.VALUES - 192)) | (1 << (SparkSQLParser.VIEW - 192)) | (1 << (SparkSQLParser.VIEWS - 192)))) != 0) or ((((_la - 256)) & ~0x3f) == 0 and ((1 << (_la - 256)) & ((1 << (SparkSQLParser.WHEN - 256)) | (1 << (SparkSQLParser.WHERE - 256)) | (1 << (SparkSQLParser.WINDOW - 256)) | (1 << (SparkSQLParser.WITH - 256)) | (1 << (SparkSQLParser.ZONE - 256)) | (1 << (SparkSQLParser.IDENTIFIER - 256)) | (1 << (SparkSQLParser.BACKQUOTED_IDENTIFIER - 256)))) != 0):
                        self.state = 2742
                        self.complexColTypeList()


                    self.state = 2745
                    self.match(SparkSQLParser.GT)
                    pass
                elif token in [SparkSQLParser.NEQ]:
                    self.state = 2746
                    self.match(SparkSQLParser.NEQ)
                    pass
                else:
                    raise NoViableAltException(self)

                pass

            elif la_ == 4:
                localctx = SparkSQLParser.PrimitiveDataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2749
                self.identifier()
                self.state = 2760
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,351,self._ctx)
                if la_ == 1:
                    self.state = 2750
                    self.match(SparkSQLParser.T__1)
                    self.state = 2751
                    self.match(SparkSQLParser.INTEGER_VALUE)
                    self.state = 2756
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 2752
                        self.match(SparkSQLParser.T__3)
                        self.state = 2753
                        self.match(SparkSQLParser.INTEGER_VALUE)
                        self.state = 2758
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    self.state = 2759
                    self.match(SparkSQLParser.T__2)


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QualifiedColTypeWithPositionListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def qualifiedColTypeWithPosition(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.QualifiedColTypeWithPositionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.QualifiedColTypeWithPositionContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_qualifiedColTypeWithPositionList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQualifiedColTypeWithPositionList" ):
                listener.enterQualifiedColTypeWithPositionList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQualifiedColTypeWithPositionList" ):
                listener.exitQualifiedColTypeWithPositionList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQualifiedColTypeWithPositionList" ):
                return visitor.visitQualifiedColTypeWithPositionList(self)
            else:
                return visitor.visitChildren(self)




    def qualifiedColTypeWithPositionList(self):

        localctx = SparkSQLParser.QualifiedColTypeWithPositionListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 224, self.RULE_qualifiedColTypeWithPositionList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2764
            self.qualifiedColTypeWithPosition()
            self.state = 2769
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 2765
                self.match(SparkSQLParser.T__3)
                self.state = 2766
                self.qualifiedColTypeWithPosition()
                self.state = 2771
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QualifiedColTypeWithPositionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.name = None # MultipartIdentifierContext

        def dataType(self):
            return self.getTypedRuleContext(SparkSQLParser.DataTypeContext,0)


        def multipartIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.MultipartIdentifierContext,0)


        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def commentSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,0)


        def colPosition(self):
            return self.getTypedRuleContext(SparkSQLParser.ColPositionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_qualifiedColTypeWithPosition

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQualifiedColTypeWithPosition" ):
                listener.enterQualifiedColTypeWithPosition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQualifiedColTypeWithPosition" ):
                listener.exitQualifiedColTypeWithPosition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQualifiedColTypeWithPosition" ):
                return visitor.visitQualifiedColTypeWithPosition(self)
            else:
                return visitor.visitChildren(self)




    def qualifiedColTypeWithPosition(self):

        localctx = SparkSQLParser.QualifiedColTypeWithPositionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 226, self.RULE_qualifiedColTypeWithPosition)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2772
            localctx.name = self.multipartIdentifier()
            self.state = 2773
            self.dataType()
            self.state = 2776
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.NOT:
                self.state = 2774
                self.match(SparkSQLParser.NOT)
                self.state = 2775
                self.match(SparkSQLParser.NULL)


            self.state = 2779
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.COMMENT:
                self.state = 2778
                self.commentSpec()


            self.state = 2782
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.AFTER or _la==SparkSQLParser.FIRST:
                self.state = 2781
                self.colPosition()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ColTypeListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def colType(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ColTypeContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ColTypeContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_colTypeList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColTypeList" ):
                listener.enterColTypeList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColTypeList" ):
                listener.exitColTypeList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColTypeList" ):
                return visitor.visitColTypeList(self)
            else:
                return visitor.visitChildren(self)




    def colTypeList(self):

        localctx = SparkSQLParser.ColTypeListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 228, self.RULE_colTypeList)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2784
            self.colType()
            self.state = 2789
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,357,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2785
                    self.match(SparkSQLParser.T__3)
                    self.state = 2786
                    self.colType() 
                self.state = 2791
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,357,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ColTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.colName = None # ErrorCapturingIdentifierContext

        def dataType(self):
            return self.getTypedRuleContext(SparkSQLParser.DataTypeContext,0)


        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def commentSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_colType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColType" ):
                listener.enterColType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColType" ):
                listener.exitColType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColType" ):
                return visitor.visitColType(self)
            else:
                return visitor.visitChildren(self)




    def colType(self):

        localctx = SparkSQLParser.ColTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 230, self.RULE_colType)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2792
            localctx.colName = self.errorCapturingIdentifier()
            self.state = 2793
            self.dataType()
            self.state = 2796
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,358,self._ctx)
            if la_ == 1:
                self.state = 2794
                self.match(SparkSQLParser.NOT)
                self.state = 2795
                self.match(SparkSQLParser.NULL)


            self.state = 2799
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,359,self._ctx)
            if la_ == 1:
                self.state = 2798
                self.commentSpec()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ComplexColTypeListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def complexColType(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ComplexColTypeContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ComplexColTypeContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_complexColTypeList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComplexColTypeList" ):
                listener.enterComplexColTypeList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComplexColTypeList" ):
                listener.exitComplexColTypeList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComplexColTypeList" ):
                return visitor.visitComplexColTypeList(self)
            else:
                return visitor.visitChildren(self)




    def complexColTypeList(self):

        localctx = SparkSQLParser.ComplexColTypeListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 232, self.RULE_complexColTypeList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2801
            self.complexColType()
            self.state = 2806
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 2802
                self.match(SparkSQLParser.T__3)
                self.state = 2803
                self.complexColType()
                self.state = 2808
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ComplexColTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def dataType(self):
            return self.getTypedRuleContext(SparkSQLParser.DataTypeContext,0)


        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def commentSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_complexColType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComplexColType" ):
                listener.enterComplexColType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComplexColType" ):
                listener.exitComplexColType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComplexColType" ):
                return visitor.visitComplexColType(self)
            else:
                return visitor.visitChildren(self)




    def complexColType(self):

        localctx = SparkSQLParser.ComplexColTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 234, self.RULE_complexColType)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2809
            self.identifier()
            self.state = 2810
            self.match(SparkSQLParser.T__10)
            self.state = 2811
            self.dataType()
            self.state = 2814
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.NOT:
                self.state = 2812
                self.match(SparkSQLParser.NOT)
                self.state = 2813
                self.match(SparkSQLParser.NULL)


            self.state = 2817
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSQLParser.COMMENT:
                self.state = 2816
                self.commentSpec()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class WhenClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.condition = None # ExpressionContext
            self.result = None # ExpressionContext

        def WHEN(self):
            return self.getToken(SparkSQLParser.WHEN, 0)

        def THEN(self):
            return self.getToken(SparkSQLParser.THEN, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_whenClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWhenClause" ):
                listener.enterWhenClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWhenClause" ):
                listener.exitWhenClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWhenClause" ):
                return visitor.visitWhenClause(self)
            else:
                return visitor.visitChildren(self)




    def whenClause(self):

        localctx = SparkSQLParser.WhenClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 236, self.RULE_whenClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2819
            self.match(SparkSQLParser.WHEN)
            self.state = 2820
            localctx.condition = self.expression()
            self.state = 2821
            self.match(SparkSQLParser.THEN)
            self.state = 2822
            localctx.result = self.expression()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class WindowClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def WINDOW(self):
            return self.getToken(SparkSQLParser.WINDOW, 0)

        def namedWindow(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.NamedWindowContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.NamedWindowContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_windowClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindowClause" ):
                listener.enterWindowClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindowClause" ):
                listener.exitWindowClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindowClause" ):
                return visitor.visitWindowClause(self)
            else:
                return visitor.visitChildren(self)




    def windowClause(self):

        localctx = SparkSQLParser.WindowClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 238, self.RULE_windowClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2824
            self.match(SparkSQLParser.WINDOW)
            self.state = 2825
            self.namedWindow()
            self.state = 2830
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,363,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2826
                    self.match(SparkSQLParser.T__3)
                    self.state = 2827
                    self.namedWindow() 
                self.state = 2832
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,363,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NamedWindowContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.name = None # ErrorCapturingIdentifierContext

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def windowSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.WindowSpecContext,0)


        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_namedWindow

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedWindow" ):
                listener.enterNamedWindow(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedWindow" ):
                listener.exitNamedWindow(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedWindow" ):
                return visitor.visitNamedWindow(self)
            else:
                return visitor.visitChildren(self)




    def namedWindow(self):

        localctx = SparkSQLParser.NamedWindowContext(self, self._ctx, self.state)
        self.enterRule(localctx, 240, self.RULE_namedWindow)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2833
            localctx.name = self.errorCapturingIdentifier()
            self.state = 2834
            self.match(SparkSQLParser.AS)
            self.state = 2835
            self.windowSpec()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class WindowSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_windowSpec

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class WindowRefContext(WindowSpecContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.WindowSpecContext
            super().__init__(parser)
            self.name = None # ErrorCapturingIdentifierContext
            self.copyFrom(ctx)

        def errorCapturingIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.ErrorCapturingIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindowRef" ):
                listener.enterWindowRef(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindowRef" ):
                listener.exitWindowRef(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindowRef" ):
                return visitor.visitWindowRef(self)
            else:
                return visitor.visitChildren(self)


    class WindowDefContext(WindowSpecContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.WindowSpecContext
            super().__init__(parser)
            self._expression = None # ExpressionContext
            self.partition = list() # of ExpressionContexts
            self.copyFrom(ctx)

        def CLUSTER(self):
            return self.getToken(SparkSQLParser.CLUSTER, 0)
        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSQLParser.BY)
            else:
                return self.getToken(SparkSQLParser.BY, i)
        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,i)

        def windowFrame(self):
            return self.getTypedRuleContext(SparkSQLParser.WindowFrameContext,0)

        def sortItem(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.SortItemContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.SortItemContext,i)

        def PARTITION(self):
            return self.getToken(SparkSQLParser.PARTITION, 0)
        def DISTRIBUTE(self):
            return self.getToken(SparkSQLParser.DISTRIBUTE, 0)
        def ORDER(self):
            return self.getToken(SparkSQLParser.ORDER, 0)
        def SORT(self):
            return self.getToken(SparkSQLParser.SORT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindowDef" ):
                listener.enterWindowDef(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindowDef" ):
                listener.exitWindowDef(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindowDef" ):
                return visitor.visitWindowDef(self)
            else:
                return visitor.visitChildren(self)



    def windowSpec(self):

        localctx = SparkSQLParser.WindowSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 242, self.RULE_windowSpec)
        self._la = 0 # Token type
        try:
            self.state = 2883
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,371,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.WindowRefContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2837
                localctx.name = self.errorCapturingIdentifier()
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.WindowRefContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2838
                self.match(SparkSQLParser.T__1)
                self.state = 2839
                localctx.name = self.errorCapturingIdentifier()
                self.state = 2840
                self.match(SparkSQLParser.T__2)
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.WindowDefContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2842
                self.match(SparkSQLParser.T__1)
                self.state = 2877
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSQLParser.CLUSTER]:
                    self.state = 2843
                    self.match(SparkSQLParser.CLUSTER)
                    self.state = 2844
                    self.match(SparkSQLParser.BY)
                    self.state = 2845
                    localctx._expression = self.expression()
                    localctx.partition.append(localctx._expression)
                    self.state = 2850
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSQLParser.T__3:
                        self.state = 2846
                        self.match(SparkSQLParser.T__3)
                        self.state = 2847
                        localctx._expression = self.expression()
                        localctx.partition.append(localctx._expression)
                        self.state = 2852
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    pass
                elif token in [SparkSQLParser.T__2, SparkSQLParser.DISTRIBUTE, SparkSQLParser.ORDER, SparkSQLParser.PARTITION, SparkSQLParser.RANGE, SparkSQLParser.ROWS, SparkSQLParser.SORT]:
                    self.state = 2863
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.DISTRIBUTE or _la==SparkSQLParser.PARTITION:
                        self.state = 2853
                        _la = self._input.LA(1)
                        if not(_la==SparkSQLParser.DISTRIBUTE or _la==SparkSQLParser.PARTITION):
                            self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 2854
                        self.match(SparkSQLParser.BY)
                        self.state = 2855
                        localctx._expression = self.expression()
                        localctx.partition.append(localctx._expression)
                        self.state = 2860
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        while _la==SparkSQLParser.T__3:
                            self.state = 2856
                            self.match(SparkSQLParser.T__3)
                            self.state = 2857
                            localctx._expression = self.expression()
                            localctx.partition.append(localctx._expression)
                            self.state = 2862
                            self._errHandler.sync(self)
                            _la = self._input.LA(1)



                    self.state = 2875
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSQLParser.ORDER or _la==SparkSQLParser.SORT:
                        self.state = 2865
                        _la = self._input.LA(1)
                        if not(_la==SparkSQLParser.ORDER or _la==SparkSQLParser.SORT):
                            self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 2866
                        self.match(SparkSQLParser.BY)
                        self.state = 2867
                        self.sortItem()
                        self.state = 2872
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        while _la==SparkSQLParser.T__3:
                            self.state = 2868
                            self.match(SparkSQLParser.T__3)
                            self.state = 2869
                            self.sortItem()
                            self.state = 2874
                            self._errHandler.sync(self)
                            _la = self._input.LA(1)



                    pass
                else:
                    raise NoViableAltException(self)

                self.state = 2880
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.RANGE or _la==SparkSQLParser.ROWS:
                    self.state = 2879
                    self.windowFrame()


                self.state = 2882
                self.match(SparkSQLParser.T__2)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class WindowFrameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.frameType = None # Token
            self.start = None # FrameBoundContext
            self.end = None # FrameBoundContext

        def RANGE(self):
            return self.getToken(SparkSQLParser.RANGE, 0)

        def frameBound(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.FrameBoundContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.FrameBoundContext,i)


        def ROWS(self):
            return self.getToken(SparkSQLParser.ROWS, 0)

        def BETWEEN(self):
            return self.getToken(SparkSQLParser.BETWEEN, 0)

        def AND(self):
            return self.getToken(SparkSQLParser.AND, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_windowFrame

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindowFrame" ):
                listener.enterWindowFrame(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindowFrame" ):
                listener.exitWindowFrame(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindowFrame" ):
                return visitor.visitWindowFrame(self)
            else:
                return visitor.visitChildren(self)




    def windowFrame(self):

        localctx = SparkSQLParser.WindowFrameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 244, self.RULE_windowFrame)
        try:
            self.state = 2901
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,372,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 2885
                localctx.frameType = self.match(SparkSQLParser.RANGE)
                self.state = 2886
                localctx.start = self.frameBound()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 2887
                localctx.frameType = self.match(SparkSQLParser.ROWS)
                self.state = 2888
                localctx.start = self.frameBound()
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 2889
                localctx.frameType = self.match(SparkSQLParser.RANGE)
                self.state = 2890
                self.match(SparkSQLParser.BETWEEN)
                self.state = 2891
                localctx.start = self.frameBound()
                self.state = 2892
                self.match(SparkSQLParser.AND)
                self.state = 2893
                localctx.end = self.frameBound()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 2895
                localctx.frameType = self.match(SparkSQLParser.ROWS)
                self.state = 2896
                self.match(SparkSQLParser.BETWEEN)
                self.state = 2897
                localctx.start = self.frameBound()
                self.state = 2898
                self.match(SparkSQLParser.AND)
                self.state = 2899
                localctx.end = self.frameBound()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FrameBoundContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.boundType = None # Token

        def UNBOUNDED(self):
            return self.getToken(SparkSQLParser.UNBOUNDED, 0)

        def PRECEDING(self):
            return self.getToken(SparkSQLParser.PRECEDING, 0)

        def FOLLOWING(self):
            return self.getToken(SparkSQLParser.FOLLOWING, 0)

        def ROW(self):
            return self.getToken(SparkSQLParser.ROW, 0)

        def CURRENT(self):
            return self.getToken(SparkSQLParser.CURRENT, 0)

        def expression(self):
            return self.getTypedRuleContext(SparkSQLParser.ExpressionContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_frameBound

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFrameBound" ):
                listener.enterFrameBound(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFrameBound" ):
                listener.exitFrameBound(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFrameBound" ):
                return visitor.visitFrameBound(self)
            else:
                return visitor.visitChildren(self)




    def frameBound(self):

        localctx = SparkSQLParser.FrameBoundContext(self, self._ctx, self.state)
        self.enterRule(localctx, 246, self.RULE_frameBound)
        self._la = 0 # Token type
        try:
            self.state = 2910
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,373,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 2903
                self.match(SparkSQLParser.UNBOUNDED)
                self.state = 2904
                localctx.boundType = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.FOLLOWING or _la==SparkSQLParser.PRECEDING):
                    localctx.boundType = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 2905
                localctx.boundType = self.match(SparkSQLParser.CURRENT)
                self.state = 2906
                self.match(SparkSQLParser.ROW)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 2907
                self.expression()
                self.state = 2908
                localctx.boundType = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.FOLLOWING or _la==SparkSQLParser.PRECEDING):
                    localctx.boundType = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QualifiedNameListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def qualifiedName(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.QualifiedNameContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_qualifiedNameList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQualifiedNameList" ):
                listener.enterQualifiedNameList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQualifiedNameList" ):
                listener.exitQualifiedNameList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQualifiedNameList" ):
                return visitor.visitQualifiedNameList(self)
            else:
                return visitor.visitChildren(self)




    def qualifiedNameList(self):

        localctx = SparkSQLParser.QualifiedNameListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 248, self.RULE_qualifiedNameList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2912
            self.qualifiedName()
            self.state = 2917
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSQLParser.T__3:
                self.state = 2913
                self.match(SparkSQLParser.T__3)
                self.state = 2914
                self.qualifiedName()
                self.state = 2919
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class FunctionNameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSQLParser.QualifiedNameContext,0)


        def FILTER(self):
            return self.getToken(SparkSQLParser.FILTER, 0)

        def LEFT(self):
            return self.getToken(SparkSQLParser.LEFT, 0)

        def RIGHT(self):
            return self.getToken(SparkSQLParser.RIGHT, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_functionName

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFunctionName" ):
                listener.enterFunctionName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFunctionName" ):
                listener.exitFunctionName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFunctionName" ):
                return visitor.visitFunctionName(self)
            else:
                return visitor.visitChildren(self)




    def functionName(self):

        localctx = SparkSQLParser.FunctionNameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 250, self.RULE_functionName)
        try:
            self.state = 2924
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,375,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 2920
                self.qualifiedName()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 2921
                self.match(SparkSQLParser.FILTER)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 2922
                self.match(SparkSQLParser.LEFT)
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 2923
                self.match(SparkSQLParser.RIGHT)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QualifiedNameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSQLParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_qualifiedName

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQualifiedName" ):
                listener.enterQualifiedName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQualifiedName" ):
                listener.exitQualifiedName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQualifiedName" ):
                return visitor.visitQualifiedName(self)
            else:
                return visitor.visitChildren(self)




    def qualifiedName(self):

        localctx = SparkSQLParser.QualifiedNameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 252, self.RULE_qualifiedName)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2926
            self.identifier()
            self.state = 2931
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,376,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2927
                    self.match(SparkSQLParser.T__4)
                    self.state = 2928
                    self.identifier() 
                self.state = 2933
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,376,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class ErrorCapturingIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSQLParser.IdentifierContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_errorCapturingIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterErrorCapturingIdentifier" ):
                listener.enterErrorCapturingIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitErrorCapturingIdentifier" ):
                listener.exitErrorCapturingIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitErrorCapturingIdentifier" ):
                return visitor.visitErrorCapturingIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def errorCapturingIdentifier(self):

        localctx = SparkSQLParser.ErrorCapturingIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 254, self.RULE_errorCapturingIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2934
            self.identifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class IdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def strictIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.StrictIdentifierContext,0)


        def strictNonReserved(self):
            return self.getTypedRuleContext(SparkSQLParser.StrictNonReservedContext,0)


        def getRuleIndex(self):
            return SparkSQLParser.RULE_identifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifier" ):
                listener.enterIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifier" ):
                listener.exitIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifier" ):
                return visitor.visitIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def identifier(self):

        localctx = SparkSQLParser.IdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 256, self.RULE_identifier)
        try:
            self.state = 2938
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.ADD, SparkSQLParser.AFTER, SparkSQLParser.ALL, SparkSQLParser.ALTER, SparkSQLParser.ANALYZE, SparkSQLParser.AND, SparkSQLParser.ANY, SparkSQLParser.ARCHIVE, SparkSQLParser.ARRAY, SparkSQLParser.AS, SparkSQLParser.ASC, SparkSQLParser.AT, SparkSQLParser.AUTHORIZATION, SparkSQLParser.BETWEEN, SparkSQLParser.BOTH, SparkSQLParser.BUCKET, SparkSQLParser.BUCKETS, SparkSQLParser.BY, SparkSQLParser.CACHE, SparkSQLParser.CASCADE, SparkSQLParser.CASE, SparkSQLParser.CAST, SparkSQLParser.CHANGE, SparkSQLParser.CHECK, SparkSQLParser.CLEAR, SparkSQLParser.CLUSTER, SparkSQLParser.CLUSTERED, SparkSQLParser.CODEGEN, SparkSQLParser.COLLATE, SparkSQLParser.COLLECTION, SparkSQLParser.COLUMN, SparkSQLParser.COLUMNS, SparkSQLParser.COMMENT, SparkSQLParser.COMMIT, SparkSQLParser.COMPACT, SparkSQLParser.COMPACTIONS, SparkSQLParser.COMPUTE, SparkSQLParser.CONCATENATE, SparkSQLParser.CONSTRAINT, SparkSQLParser.COST, SparkSQLParser.CREATE, SparkSQLParser.CUBE, SparkSQLParser.CURRENT, SparkSQLParser.CURRENT_DATE, SparkSQLParser.CURRENT_TIME, SparkSQLParser.CURRENT_TIMESTAMP, SparkSQLParser.CURRENT_USER, SparkSQLParser.DATA, SparkSQLParser.DATABASE, SparkSQLParser.DATABASES, SparkSQLParser.DBPROPERTIES, SparkSQLParser.DEFINED, SparkSQLParser.DELETE, SparkSQLParser.DELIMITED, SparkSQLParser.DESC, SparkSQLParser.DESCRIBE, SparkSQLParser.DFS, SparkSQLParser.DIRECTORIES, SparkSQLParser.DIRECTORY, SparkSQLParser.DISTINCT, SparkSQLParser.DISTRIBUTE, SparkSQLParser.DIV, SparkSQLParser.DROP, SparkSQLParser.ELSE, SparkSQLParser.END, SparkSQLParser.ESCAPE, SparkSQLParser.ESCAPED, SparkSQLParser.EXCHANGE, SparkSQLParser.EXISTS, SparkSQLParser.EXPLAIN, SparkSQLParser.EXPORT, SparkSQLParser.EXTENDED, SparkSQLParser.EXTERNAL, SparkSQLParser.EXTRACT, SparkSQLParser.FALSE, SparkSQLParser.FETCH, SparkSQLParser.FIELDS, SparkSQLParser.FILTER, SparkSQLParser.FILEFORMAT, SparkSQLParser.FIRST, SparkSQLParser.FOLLOWING, SparkSQLParser.FOR, SparkSQLParser.FOREIGN, SparkSQLParser.FORMAT, SparkSQLParser.FORMATTED, SparkSQLParser.FROM, SparkSQLParser.FUNCTION, SparkSQLParser.FUNCTIONS, SparkSQLParser.GLOBAL, SparkSQLParser.GRANT, SparkSQLParser.GROUP, SparkSQLParser.GROUPING, SparkSQLParser.HAVING, SparkSQLParser.IF, SparkSQLParser.IGNORE, SparkSQLParser.IMPORT, SparkSQLParser.IN, SparkSQLParser.INDEX, SparkSQLParser.INDEXES, SparkSQLParser.INPATH, SparkSQLParser.INPUTFORMAT, SparkSQLParser.INSERT, SparkSQLParser.INTERVAL, SparkSQLParser.INTO, SparkSQLParser.IS, SparkSQLParser.ITEMS, SparkSQLParser.KEYS, SparkSQLParser.LAST, SparkSQLParser.LATERAL, SparkSQLParser.LAZY, SparkSQLParser.LEADING, SparkSQLParser.LIKE, SparkSQLParser.LIMIT, SparkSQLParser.LINES, SparkSQLParser.LIST, SparkSQLParser.LOAD, SparkSQLParser.LOCAL, SparkSQLParser.LOCATION, SparkSQLParser.LOCK, SparkSQLParser.LOCKS, SparkSQLParser.LOGICAL, SparkSQLParser.MACRO, SparkSQLParser.MAP, SparkSQLParser.MATCHED, SparkSQLParser.MERGE, SparkSQLParser.MSCK, SparkSQLParser.NAMESPACE, SparkSQLParser.NAMESPACES, SparkSQLParser.NO, SparkSQLParser.NOT, SparkSQLParser.NULL, SparkSQLParser.NULLS, SparkSQLParser.OF, SparkSQLParser.ONLY, SparkSQLParser.OPTION, SparkSQLParser.OPTIONS, SparkSQLParser.OR, SparkSQLParser.ORDER, SparkSQLParser.OUT, SparkSQLParser.OUTER, SparkSQLParser.OUTPUTFORMAT, SparkSQLParser.OVER, SparkSQLParser.OVERLAPS, SparkSQLParser.OVERLAY, SparkSQLParser.OVERWRITE, SparkSQLParser.PARTITION, SparkSQLParser.PARTITIONED, SparkSQLParser.PARTITIONS, SparkSQLParser.PERCENTLIT, SparkSQLParser.PIVOT, SparkSQLParser.PLACING, SparkSQLParser.POSITION, SparkSQLParser.PRECEDING, SparkSQLParser.PRIMARY, SparkSQLParser.PRINCIPALS, SparkSQLParser.PROPERTIES, SparkSQLParser.PURGE, SparkSQLParser.QUERY, SparkSQLParser.RANGE, SparkSQLParser.RECORDREADER, SparkSQLParser.RECORDWRITER, SparkSQLParser.RECOVER, SparkSQLParser.REDUCE, SparkSQLParser.REFERENCES, SparkSQLParser.REFRESH, SparkSQLParser.RENAME, SparkSQLParser.REPAIR, SparkSQLParser.REPLACE, SparkSQLParser.RESET, SparkSQLParser.RESTRICT, SparkSQLParser.REVOKE, SparkSQLParser.RLIKE, SparkSQLParser.ROLE, SparkSQLParser.ROLES, SparkSQLParser.ROLLBACK, SparkSQLParser.ROLLUP, SparkSQLParser.ROW, SparkSQLParser.ROWS, SparkSQLParser.SCHEMA, SparkSQLParser.SELECT, SparkSQLParser.SEPARATED, SparkSQLParser.SERDE, SparkSQLParser.SERDEPROPERTIES, SparkSQLParser.SESSION_USER, SparkSQLParser.SET, SparkSQLParser.SETS, SparkSQLParser.SHOW, SparkSQLParser.SKEWED, SparkSQLParser.SOME, SparkSQLParser.SORT, SparkSQLParser.SORTED, SparkSQLParser.START, SparkSQLParser.STATISTICS, SparkSQLParser.STORED, SparkSQLParser.STRATIFY, SparkSQLParser.STRUCT, SparkSQLParser.SUBSTR, SparkSQLParser.SUBSTRING, SparkSQLParser.TABLE, SparkSQLParser.TABLES, SparkSQLParser.TABLESAMPLE, SparkSQLParser.TBLPROPERTIES, SparkSQLParser.TEMPORARY, SparkSQLParser.TERMINATED, SparkSQLParser.THEN, SparkSQLParser.TIME, SparkSQLParser.TO, SparkSQLParser.TOUCH, SparkSQLParser.TRAILING, SparkSQLParser.TRANSACTION, SparkSQLParser.TRANSACTIONS, SparkSQLParser.TRANSFORM, SparkSQLParser.TRIM, SparkSQLParser.TRUE, SparkSQLParser.TRUNCATE, SparkSQLParser.TYPE, SparkSQLParser.UNARCHIVE, SparkSQLParser.UNBOUNDED, SparkSQLParser.UNCACHE, SparkSQLParser.UNIQUE, SparkSQLParser.UNKNOWN, SparkSQLParser.UNLOCK, SparkSQLParser.UNSET, SparkSQLParser.UPDATE, SparkSQLParser.USE, SparkSQLParser.USER, SparkSQLParser.VALUES, SparkSQLParser.VIEW, SparkSQLParser.VIEWS, SparkSQLParser.WHEN, SparkSQLParser.WHERE, SparkSQLParser.WINDOW, SparkSQLParser.WITH, SparkSQLParser.ZONE, SparkSQLParser.IDENTIFIER, SparkSQLParser.BACKQUOTED_IDENTIFIER]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2936
                self.strictIdentifier()
                pass
            elif token in [SparkSQLParser.ANTI, SparkSQLParser.CROSS, SparkSQLParser.EXCEPT, SparkSQLParser.FULL, SparkSQLParser.INNER, SparkSQLParser.INTERSECT, SparkSQLParser.JOIN, SparkSQLParser.LEFT, SparkSQLParser.NATURAL, SparkSQLParser.ON, SparkSQLParser.RIGHT, SparkSQLParser.SEMI, SparkSQLParser.SETMINUS, SparkSQLParser.UNION, SparkSQLParser.USING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2937
                self.strictNonReserved()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class StrictIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_strictIdentifier

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class QuotedIdentifierAlternativeContext(StrictIdentifierContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StrictIdentifierContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def quotedIdentifier(self):
            return self.getTypedRuleContext(SparkSQLParser.QuotedIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuotedIdentifierAlternative" ):
                listener.enterQuotedIdentifierAlternative(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuotedIdentifierAlternative" ):
                listener.exitQuotedIdentifierAlternative(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuotedIdentifierAlternative" ):
                return visitor.visitQuotedIdentifierAlternative(self)
            else:
                return visitor.visitChildren(self)


    class UnquotedIdentifierContext(StrictIdentifierContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.StrictIdentifierContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def IDENTIFIER(self):
            return self.getToken(SparkSQLParser.IDENTIFIER, 0)
        def nonReserved(self):
            return self.getTypedRuleContext(SparkSQLParser.NonReservedContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnquotedIdentifier" ):
                listener.enterUnquotedIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnquotedIdentifier" ):
                listener.exitUnquotedIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnquotedIdentifier" ):
                return visitor.visitUnquotedIdentifier(self)
            else:
                return visitor.visitChildren(self)



    def strictIdentifier(self):

        localctx = SparkSQLParser.StrictIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 258, self.RULE_strictIdentifier)
        try:
            self.state = 2943
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.IDENTIFIER]:
                localctx = SparkSQLParser.UnquotedIdentifierContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2940
                self.match(SparkSQLParser.IDENTIFIER)
                pass
            elif token in [SparkSQLParser.BACKQUOTED_IDENTIFIER]:
                localctx = SparkSQLParser.QuotedIdentifierAlternativeContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2941
                self.quotedIdentifier()
                pass
            elif token in [SparkSQLParser.ADD, SparkSQLParser.AFTER, SparkSQLParser.ALL, SparkSQLParser.ALTER, SparkSQLParser.ANALYZE, SparkSQLParser.AND, SparkSQLParser.ANY, SparkSQLParser.ARCHIVE, SparkSQLParser.ARRAY, SparkSQLParser.AS, SparkSQLParser.ASC, SparkSQLParser.AT, SparkSQLParser.AUTHORIZATION, SparkSQLParser.BETWEEN, SparkSQLParser.BOTH, SparkSQLParser.BUCKET, SparkSQLParser.BUCKETS, SparkSQLParser.BY, SparkSQLParser.CACHE, SparkSQLParser.CASCADE, SparkSQLParser.CASE, SparkSQLParser.CAST, SparkSQLParser.CHANGE, SparkSQLParser.CHECK, SparkSQLParser.CLEAR, SparkSQLParser.CLUSTER, SparkSQLParser.CLUSTERED, SparkSQLParser.CODEGEN, SparkSQLParser.COLLATE, SparkSQLParser.COLLECTION, SparkSQLParser.COLUMN, SparkSQLParser.COLUMNS, SparkSQLParser.COMMENT, SparkSQLParser.COMMIT, SparkSQLParser.COMPACT, SparkSQLParser.COMPACTIONS, SparkSQLParser.COMPUTE, SparkSQLParser.CONCATENATE, SparkSQLParser.CONSTRAINT, SparkSQLParser.COST, SparkSQLParser.CREATE, SparkSQLParser.CUBE, SparkSQLParser.CURRENT, SparkSQLParser.CURRENT_DATE, SparkSQLParser.CURRENT_TIME, SparkSQLParser.CURRENT_TIMESTAMP, SparkSQLParser.CURRENT_USER, SparkSQLParser.DATA, SparkSQLParser.DATABASE, SparkSQLParser.DATABASES, SparkSQLParser.DBPROPERTIES, SparkSQLParser.DEFINED, SparkSQLParser.DELETE, SparkSQLParser.DELIMITED, SparkSQLParser.DESC, SparkSQLParser.DESCRIBE, SparkSQLParser.DFS, SparkSQLParser.DIRECTORIES, SparkSQLParser.DIRECTORY, SparkSQLParser.DISTINCT, SparkSQLParser.DISTRIBUTE, SparkSQLParser.DIV, SparkSQLParser.DROP, SparkSQLParser.ELSE, SparkSQLParser.END, SparkSQLParser.ESCAPE, SparkSQLParser.ESCAPED, SparkSQLParser.EXCHANGE, SparkSQLParser.EXISTS, SparkSQLParser.EXPLAIN, SparkSQLParser.EXPORT, SparkSQLParser.EXTENDED, SparkSQLParser.EXTERNAL, SparkSQLParser.EXTRACT, SparkSQLParser.FALSE, SparkSQLParser.FETCH, SparkSQLParser.FIELDS, SparkSQLParser.FILTER, SparkSQLParser.FILEFORMAT, SparkSQLParser.FIRST, SparkSQLParser.FOLLOWING, SparkSQLParser.FOR, SparkSQLParser.FOREIGN, SparkSQLParser.FORMAT, SparkSQLParser.FORMATTED, SparkSQLParser.FROM, SparkSQLParser.FUNCTION, SparkSQLParser.FUNCTIONS, SparkSQLParser.GLOBAL, SparkSQLParser.GRANT, SparkSQLParser.GROUP, SparkSQLParser.GROUPING, SparkSQLParser.HAVING, SparkSQLParser.IF, SparkSQLParser.IGNORE, SparkSQLParser.IMPORT, SparkSQLParser.IN, SparkSQLParser.INDEX, SparkSQLParser.INDEXES, SparkSQLParser.INPATH, SparkSQLParser.INPUTFORMAT, SparkSQLParser.INSERT, SparkSQLParser.INTERVAL, SparkSQLParser.INTO, SparkSQLParser.IS, SparkSQLParser.ITEMS, SparkSQLParser.KEYS, SparkSQLParser.LAST, SparkSQLParser.LATERAL, SparkSQLParser.LAZY, SparkSQLParser.LEADING, SparkSQLParser.LIKE, SparkSQLParser.LIMIT, SparkSQLParser.LINES, SparkSQLParser.LIST, SparkSQLParser.LOAD, SparkSQLParser.LOCAL, SparkSQLParser.LOCATION, SparkSQLParser.LOCK, SparkSQLParser.LOCKS, SparkSQLParser.LOGICAL, SparkSQLParser.MACRO, SparkSQLParser.MAP, SparkSQLParser.MATCHED, SparkSQLParser.MERGE, SparkSQLParser.MSCK, SparkSQLParser.NAMESPACE, SparkSQLParser.NAMESPACES, SparkSQLParser.NO, SparkSQLParser.NOT, SparkSQLParser.NULL, SparkSQLParser.NULLS, SparkSQLParser.OF, SparkSQLParser.ONLY, SparkSQLParser.OPTION, SparkSQLParser.OPTIONS, SparkSQLParser.OR, SparkSQLParser.ORDER, SparkSQLParser.OUT, SparkSQLParser.OUTER, SparkSQLParser.OUTPUTFORMAT, SparkSQLParser.OVER, SparkSQLParser.OVERLAPS, SparkSQLParser.OVERLAY, SparkSQLParser.OVERWRITE, SparkSQLParser.PARTITION, SparkSQLParser.PARTITIONED, SparkSQLParser.PARTITIONS, SparkSQLParser.PERCENTLIT, SparkSQLParser.PIVOT, SparkSQLParser.PLACING, SparkSQLParser.POSITION, SparkSQLParser.PRECEDING, SparkSQLParser.PRIMARY, SparkSQLParser.PRINCIPALS, SparkSQLParser.PROPERTIES, SparkSQLParser.PURGE, SparkSQLParser.QUERY, SparkSQLParser.RANGE, SparkSQLParser.RECORDREADER, SparkSQLParser.RECORDWRITER, SparkSQLParser.RECOVER, SparkSQLParser.REDUCE, SparkSQLParser.REFERENCES, SparkSQLParser.REFRESH, SparkSQLParser.RENAME, SparkSQLParser.REPAIR, SparkSQLParser.REPLACE, SparkSQLParser.RESET, SparkSQLParser.RESTRICT, SparkSQLParser.REVOKE, SparkSQLParser.RLIKE, SparkSQLParser.ROLE, SparkSQLParser.ROLES, SparkSQLParser.ROLLBACK, SparkSQLParser.ROLLUP, SparkSQLParser.ROW, SparkSQLParser.ROWS, SparkSQLParser.SCHEMA, SparkSQLParser.SELECT, SparkSQLParser.SEPARATED, SparkSQLParser.SERDE, SparkSQLParser.SERDEPROPERTIES, SparkSQLParser.SESSION_USER, SparkSQLParser.SET, SparkSQLParser.SETS, SparkSQLParser.SHOW, SparkSQLParser.SKEWED, SparkSQLParser.SOME, SparkSQLParser.SORT, SparkSQLParser.SORTED, SparkSQLParser.START, SparkSQLParser.STATISTICS, SparkSQLParser.STORED, SparkSQLParser.STRATIFY, SparkSQLParser.STRUCT, SparkSQLParser.SUBSTR, SparkSQLParser.SUBSTRING, SparkSQLParser.TABLE, SparkSQLParser.TABLES, SparkSQLParser.TABLESAMPLE, SparkSQLParser.TBLPROPERTIES, SparkSQLParser.TEMPORARY, SparkSQLParser.TERMINATED, SparkSQLParser.THEN, SparkSQLParser.TIME, SparkSQLParser.TO, SparkSQLParser.TOUCH, SparkSQLParser.TRAILING, SparkSQLParser.TRANSACTION, SparkSQLParser.TRANSACTIONS, SparkSQLParser.TRANSFORM, SparkSQLParser.TRIM, SparkSQLParser.TRUE, SparkSQLParser.TRUNCATE, SparkSQLParser.TYPE, SparkSQLParser.UNARCHIVE, SparkSQLParser.UNBOUNDED, SparkSQLParser.UNCACHE, SparkSQLParser.UNIQUE, SparkSQLParser.UNKNOWN, SparkSQLParser.UNLOCK, SparkSQLParser.UNSET, SparkSQLParser.UPDATE, SparkSQLParser.USE, SparkSQLParser.USER, SparkSQLParser.VALUES, SparkSQLParser.VIEW, SparkSQLParser.VIEWS, SparkSQLParser.WHEN, SparkSQLParser.WHERE, SparkSQLParser.WINDOW, SparkSQLParser.WITH, SparkSQLParser.ZONE]:
                localctx = SparkSQLParser.UnquotedIdentifierContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2942
                self.nonReserved()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class QuotedIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def BACKQUOTED_IDENTIFIER(self):
            return self.getToken(SparkSQLParser.BACKQUOTED_IDENTIFIER, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_quotedIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuotedIdentifier" ):
                listener.enterQuotedIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuotedIdentifier" ):
                listener.exitQuotedIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuotedIdentifier" ):
                return visitor.visitQuotedIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def quotedIdentifier(self):

        localctx = SparkSQLParser.QuotedIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 260, self.RULE_quotedIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2945
            self.match(SparkSQLParser.BACKQUOTED_IDENTIFIER)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NumberContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSQLParser.RULE_number

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class DecimalLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DECIMAL_VALUE(self):
            return self.getToken(SparkSQLParser.DECIMAL_VALUE, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDecimalLiteral" ):
                listener.enterDecimalLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDecimalLiteral" ):
                listener.exitDecimalLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDecimalLiteral" ):
                return visitor.visitDecimalLiteral(self)
            else:
                return visitor.visitChildren(self)


    class BigIntLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def BIGINT_LITERAL(self):
            return self.getToken(SparkSQLParser.BIGINT_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBigIntLiteral" ):
                listener.enterBigIntLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBigIntLiteral" ):
                listener.exitBigIntLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBigIntLiteral" ):
                return visitor.visitBigIntLiteral(self)
            else:
                return visitor.visitChildren(self)


    class TinyIntLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def TINYINT_LITERAL(self):
            return self.getToken(SparkSQLParser.TINYINT_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTinyIntLiteral" ):
                listener.enterTinyIntLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTinyIntLiteral" ):
                listener.exitTinyIntLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTinyIntLiteral" ):
                return visitor.visitTinyIntLiteral(self)
            else:
                return visitor.visitChildren(self)


    class BigDecimalLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def BIGDECIMAL_LITERAL(self):
            return self.getToken(SparkSQLParser.BIGDECIMAL_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBigDecimalLiteral" ):
                listener.enterBigDecimalLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBigDecimalLiteral" ):
                listener.exitBigDecimalLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBigDecimalLiteral" ):
                return visitor.visitBigDecimalLiteral(self)
            else:
                return visitor.visitChildren(self)


    class ExponentLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def EXPONENT_VALUE(self):
            return self.getToken(SparkSQLParser.EXPONENT_VALUE, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExponentLiteral" ):
                listener.enterExponentLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExponentLiteral" ):
                listener.exitExponentLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExponentLiteral" ):
                return visitor.visitExponentLiteral(self)
            else:
                return visitor.visitChildren(self)


    class DoubleLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DOUBLE_LITERAL(self):
            return self.getToken(SparkSQLParser.DOUBLE_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDoubleLiteral" ):
                listener.enterDoubleLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDoubleLiteral" ):
                listener.exitDoubleLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDoubleLiteral" ):
                return visitor.visitDoubleLiteral(self)
            else:
                return visitor.visitChildren(self)


    class IntegerLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def INTEGER_VALUE(self):
            return self.getToken(SparkSQLParser.INTEGER_VALUE, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIntegerLiteral" ):
                listener.enterIntegerLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIntegerLiteral" ):
                listener.exitIntegerLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIntegerLiteral" ):
                return visitor.visitIntegerLiteral(self)
            else:
                return visitor.visitChildren(self)


    class FloatLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def FLOAT_LITERAL(self):
            return self.getToken(SparkSQLParser.FLOAT_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFloatLiteral" ):
                listener.enterFloatLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFloatLiteral" ):
                listener.exitFloatLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFloatLiteral" ):
                return visitor.visitFloatLiteral(self)
            else:
                return visitor.visitChildren(self)


    class SmallIntLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSQLParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SMALLINT_LITERAL(self):
            return self.getToken(SparkSQLParser.SMALLINT_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSQLParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSmallIntLiteral" ):
                listener.enterSmallIntLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSmallIntLiteral" ):
                listener.exitSmallIntLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSmallIntLiteral" ):
                return visitor.visitSmallIntLiteral(self)
            else:
                return visitor.visitChildren(self)



    def number(self):

        localctx = SparkSQLParser.NumberContext(self, self._ctx, self.state)
        self.enterRule(localctx, 262, self.RULE_number)
        self._la = 0 # Token type
        try:
            self.state = 2983
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,388,self._ctx)
            if la_ == 1:
                localctx = SparkSQLParser.ExponentLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2948
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2947
                    self.match(SparkSQLParser.MINUS)


                self.state = 2950
                self.match(SparkSQLParser.EXPONENT_VALUE)
                pass

            elif la_ == 2:
                localctx = SparkSQLParser.DecimalLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2952
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2951
                    self.match(SparkSQLParser.MINUS)


                self.state = 2954
                self.match(SparkSQLParser.DECIMAL_VALUE)
                pass

            elif la_ == 3:
                localctx = SparkSQLParser.IntegerLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2956
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2955
                    self.match(SparkSQLParser.MINUS)


                self.state = 2958
                self.match(SparkSQLParser.INTEGER_VALUE)
                pass

            elif la_ == 4:
                localctx = SparkSQLParser.BigIntLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2960
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2959
                    self.match(SparkSQLParser.MINUS)


                self.state = 2962
                self.match(SparkSQLParser.BIGINT_LITERAL)
                pass

            elif la_ == 5:
                localctx = SparkSQLParser.SmallIntLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 2964
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2963
                    self.match(SparkSQLParser.MINUS)


                self.state = 2966
                self.match(SparkSQLParser.SMALLINT_LITERAL)
                pass

            elif la_ == 6:
                localctx = SparkSQLParser.TinyIntLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 6)
                self.state = 2968
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2967
                    self.match(SparkSQLParser.MINUS)


                self.state = 2970
                self.match(SparkSQLParser.TINYINT_LITERAL)
                pass

            elif la_ == 7:
                localctx = SparkSQLParser.DoubleLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 7)
                self.state = 2972
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2971
                    self.match(SparkSQLParser.MINUS)


                self.state = 2974
                self.match(SparkSQLParser.DOUBLE_LITERAL)
                pass

            elif la_ == 8:
                localctx = SparkSQLParser.FloatLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 8)
                self.state = 2976
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2975
                    self.match(SparkSQLParser.MINUS)


                self.state = 2978
                self.match(SparkSQLParser.FLOAT_LITERAL)
                pass

            elif la_ == 9:
                localctx = SparkSQLParser.BigDecimalLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 9)
                self.state = 2980
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSQLParser.MINUS:
                    self.state = 2979
                    self.match(SparkSQLParser.MINUS)


                self.state = 2982
                self.match(SparkSQLParser.BIGDECIMAL_LITERAL)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class AlterColumnActionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.setOrDrop = None # Token

        def TYPE(self):
            return self.getToken(SparkSQLParser.TYPE, 0)

        def dataType(self):
            return self.getTypedRuleContext(SparkSQLParser.DataTypeContext,0)


        def commentSpec(self):
            return self.getTypedRuleContext(SparkSQLParser.CommentSpecContext,0)


        def colPosition(self):
            return self.getTypedRuleContext(SparkSQLParser.ColPositionContext,0)


        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_alterColumnAction

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAlterColumnAction" ):
                listener.enterAlterColumnAction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAlterColumnAction" ):
                listener.exitAlterColumnAction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAlterColumnAction" ):
                return visitor.visitAlterColumnAction(self)
            else:
                return visitor.visitChildren(self)




    def alterColumnAction(self):

        localctx = SparkSQLParser.AlterColumnActionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 264, self.RULE_alterColumnAction)
        self._la = 0 # Token type
        try:
            self.state = 2992
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSQLParser.TYPE]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2985
                self.match(SparkSQLParser.TYPE)
                self.state = 2986
                self.dataType()
                pass
            elif token in [SparkSQLParser.COMMENT]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2987
                self.commentSpec()
                pass
            elif token in [SparkSQLParser.AFTER, SparkSQLParser.FIRST]:
                self.enterOuterAlt(localctx, 3)
                self.state = 2988
                self.colPosition()
                pass
            elif token in [SparkSQLParser.DROP, SparkSQLParser.SET]:
                self.enterOuterAlt(localctx, 4)
                self.state = 2989
                localctx.setOrDrop = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSQLParser.DROP or _la==SparkSQLParser.SET):
                    localctx.setOrDrop = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 2990
                self.match(SparkSQLParser.NOT)
                self.state = 2991
                self.match(SparkSQLParser.NULL)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class StrictNonReservedContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ANTI(self):
            return self.getToken(SparkSQLParser.ANTI, 0)

        def CROSS(self):
            return self.getToken(SparkSQLParser.CROSS, 0)

        def EXCEPT(self):
            return self.getToken(SparkSQLParser.EXCEPT, 0)

        def FULL(self):
            return self.getToken(SparkSQLParser.FULL, 0)

        def INNER(self):
            return self.getToken(SparkSQLParser.INNER, 0)

        def INTERSECT(self):
            return self.getToken(SparkSQLParser.INTERSECT, 0)

        def JOIN(self):
            return self.getToken(SparkSQLParser.JOIN, 0)

        def LEFT(self):
            return self.getToken(SparkSQLParser.LEFT, 0)

        def NATURAL(self):
            return self.getToken(SparkSQLParser.NATURAL, 0)

        def ON(self):
            return self.getToken(SparkSQLParser.ON, 0)

        def RIGHT(self):
            return self.getToken(SparkSQLParser.RIGHT, 0)

        def SEMI(self):
            return self.getToken(SparkSQLParser.SEMI, 0)

        def SETMINUS(self):
            return self.getToken(SparkSQLParser.SETMINUS, 0)

        def UNION(self):
            return self.getToken(SparkSQLParser.UNION, 0)

        def USING(self):
            return self.getToken(SparkSQLParser.USING, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_strictNonReserved

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStrictNonReserved" ):
                listener.enterStrictNonReserved(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStrictNonReserved" ):
                listener.exitStrictNonReserved(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStrictNonReserved" ):
                return visitor.visitStrictNonReserved(self)
            else:
                return visitor.visitChildren(self)




    def strictNonReserved(self):

        localctx = SparkSQLParser.StrictNonReservedContext(self, self._ctx, self.state)
        self.enterRule(localctx, 266, self.RULE_strictNonReserved)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2994
            _la = self._input.LA(1)
            if not(((((_la - 18)) & ~0x3f) == 0 and ((1 << (_la - 18)) & ((1 << (SparkSQLParser.ANTI - 18)) | (1 << (SparkSQLParser.CROSS - 18)) | (1 << (SparkSQLParser.EXCEPT - 18)))) != 0) or ((((_la - 101)) & ~0x3f) == 0 and ((1 << (_la - 101)) & ((1 << (SparkSQLParser.FULL - 101)) | (1 << (SparkSQLParser.INNER - 101)) | (1 << (SparkSQLParser.INTERSECT - 101)) | (1 << (SparkSQLParser.JOIN - 101)) | (1 << (SparkSQLParser.LEFT - 101)) | (1 << (SparkSQLParser.NATURAL - 101)) | (1 << (SparkSQLParser.ON - 101)))) != 0) or ((((_la - 193)) & ~0x3f) == 0 and ((1 << (_la - 193)) & ((1 << (SparkSQLParser.RIGHT - 193)) | (1 << (SparkSQLParser.SEMI - 193)) | (1 << (SparkSQLParser.SETMINUS - 193)) | (1 << (SparkSQLParser.UNION - 193)) | (1 << (SparkSQLParser.USING - 193)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx


    class NonReservedContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ADD(self):
            return self.getToken(SparkSQLParser.ADD, 0)

        def AFTER(self):
            return self.getToken(SparkSQLParser.AFTER, 0)

        def ALL(self):
            return self.getToken(SparkSQLParser.ALL, 0)

        def ALTER(self):
            return self.getToken(SparkSQLParser.ALTER, 0)

        def ANALYZE(self):
            return self.getToken(SparkSQLParser.ANALYZE, 0)

        def AND(self):
            return self.getToken(SparkSQLParser.AND, 0)

        def ANY(self):
            return self.getToken(SparkSQLParser.ANY, 0)

        def ARCHIVE(self):
            return self.getToken(SparkSQLParser.ARCHIVE, 0)

        def ARRAY(self):
            return self.getToken(SparkSQLParser.ARRAY, 0)

        def AS(self):
            return self.getToken(SparkSQLParser.AS, 0)

        def ASC(self):
            return self.getToken(SparkSQLParser.ASC, 0)

        def AT(self):
            return self.getToken(SparkSQLParser.AT, 0)

        def AUTHORIZATION(self):
            return self.getToken(SparkSQLParser.AUTHORIZATION, 0)

        def BETWEEN(self):
            return self.getToken(SparkSQLParser.BETWEEN, 0)

        def BOTH(self):
            return self.getToken(SparkSQLParser.BOTH, 0)

        def BUCKET(self):
            return self.getToken(SparkSQLParser.BUCKET, 0)

        def BUCKETS(self):
            return self.getToken(SparkSQLParser.BUCKETS, 0)

        def BY(self):
            return self.getToken(SparkSQLParser.BY, 0)

        def CACHE(self):
            return self.getToken(SparkSQLParser.CACHE, 0)

        def CASCADE(self):
            return self.getToken(SparkSQLParser.CASCADE, 0)

        def CASE(self):
            return self.getToken(SparkSQLParser.CASE, 0)

        def CAST(self):
            return self.getToken(SparkSQLParser.CAST, 0)

        def CHANGE(self):
            return self.getToken(SparkSQLParser.CHANGE, 0)

        def CHECK(self):
            return self.getToken(SparkSQLParser.CHECK, 0)

        def CLEAR(self):
            return self.getToken(SparkSQLParser.CLEAR, 0)

        def CLUSTER(self):
            return self.getToken(SparkSQLParser.CLUSTER, 0)

        def CLUSTERED(self):
            return self.getToken(SparkSQLParser.CLUSTERED, 0)

        def CODEGEN(self):
            return self.getToken(SparkSQLParser.CODEGEN, 0)

        def COLLATE(self):
            return self.getToken(SparkSQLParser.COLLATE, 0)

        def COLLECTION(self):
            return self.getToken(SparkSQLParser.COLLECTION, 0)

        def COLUMN(self):
            return self.getToken(SparkSQLParser.COLUMN, 0)

        def COLUMNS(self):
            return self.getToken(SparkSQLParser.COLUMNS, 0)

        def COMMENT(self):
            return self.getToken(SparkSQLParser.COMMENT, 0)

        def COMMIT(self):
            return self.getToken(SparkSQLParser.COMMIT, 0)

        def COMPACT(self):
            return self.getToken(SparkSQLParser.COMPACT, 0)

        def COMPACTIONS(self):
            return self.getToken(SparkSQLParser.COMPACTIONS, 0)

        def COMPUTE(self):
            return self.getToken(SparkSQLParser.COMPUTE, 0)

        def CONCATENATE(self):
            return self.getToken(SparkSQLParser.CONCATENATE, 0)

        def CONSTRAINT(self):
            return self.getToken(SparkSQLParser.CONSTRAINT, 0)

        def COST(self):
            return self.getToken(SparkSQLParser.COST, 0)

        def CREATE(self):
            return self.getToken(SparkSQLParser.CREATE, 0)

        def CUBE(self):
            return self.getToken(SparkSQLParser.CUBE, 0)

        def CURRENT(self):
            return self.getToken(SparkSQLParser.CURRENT, 0)

        def CURRENT_DATE(self):
            return self.getToken(SparkSQLParser.CURRENT_DATE, 0)

        def CURRENT_TIME(self):
            return self.getToken(SparkSQLParser.CURRENT_TIME, 0)

        def CURRENT_TIMESTAMP(self):
            return self.getToken(SparkSQLParser.CURRENT_TIMESTAMP, 0)

        def CURRENT_USER(self):
            return self.getToken(SparkSQLParser.CURRENT_USER, 0)

        def DATA(self):
            return self.getToken(SparkSQLParser.DATA, 0)

        def DATABASE(self):
            return self.getToken(SparkSQLParser.DATABASE, 0)

        def DATABASES(self):
            return self.getToken(SparkSQLParser.DATABASES, 0)

        def DBPROPERTIES(self):
            return self.getToken(SparkSQLParser.DBPROPERTIES, 0)

        def DEFINED(self):
            return self.getToken(SparkSQLParser.DEFINED, 0)

        def DELETE(self):
            return self.getToken(SparkSQLParser.DELETE, 0)

        def DELIMITED(self):
            return self.getToken(SparkSQLParser.DELIMITED, 0)

        def DESC(self):
            return self.getToken(SparkSQLParser.DESC, 0)

        def DESCRIBE(self):
            return self.getToken(SparkSQLParser.DESCRIBE, 0)

        def DFS(self):
            return self.getToken(SparkSQLParser.DFS, 0)

        def DIRECTORIES(self):
            return self.getToken(SparkSQLParser.DIRECTORIES, 0)

        def DIRECTORY(self):
            return self.getToken(SparkSQLParser.DIRECTORY, 0)

        def DISTINCT(self):
            return self.getToken(SparkSQLParser.DISTINCT, 0)

        def DISTRIBUTE(self):
            return self.getToken(SparkSQLParser.DISTRIBUTE, 0)

        def DIV(self):
            return self.getToken(SparkSQLParser.DIV, 0)

        def DROP(self):
            return self.getToken(SparkSQLParser.DROP, 0)

        def ELSE(self):
            return self.getToken(SparkSQLParser.ELSE, 0)

        def END(self):
            return self.getToken(SparkSQLParser.END, 0)

        def ESCAPE(self):
            return self.getToken(SparkSQLParser.ESCAPE, 0)

        def ESCAPED(self):
            return self.getToken(SparkSQLParser.ESCAPED, 0)

        def EXCHANGE(self):
            return self.getToken(SparkSQLParser.EXCHANGE, 0)

        def EXISTS(self):
            return self.getToken(SparkSQLParser.EXISTS, 0)

        def EXPLAIN(self):
            return self.getToken(SparkSQLParser.EXPLAIN, 0)

        def EXPORT(self):
            return self.getToken(SparkSQLParser.EXPORT, 0)

        def EXTENDED(self):
            return self.getToken(SparkSQLParser.EXTENDED, 0)

        def EXTERNAL(self):
            return self.getToken(SparkSQLParser.EXTERNAL, 0)

        def EXTRACT(self):
            return self.getToken(SparkSQLParser.EXTRACT, 0)

        def FALSE(self):
            return self.getToken(SparkSQLParser.FALSE, 0)

        def FETCH(self):
            return self.getToken(SparkSQLParser.FETCH, 0)

        def FILTER(self):
            return self.getToken(SparkSQLParser.FILTER, 0)

        def FIELDS(self):
            return self.getToken(SparkSQLParser.FIELDS, 0)

        def FILEFORMAT(self):
            return self.getToken(SparkSQLParser.FILEFORMAT, 0)

        def FIRST(self):
            return self.getToken(SparkSQLParser.FIRST, 0)

        def FOLLOWING(self):
            return self.getToken(SparkSQLParser.FOLLOWING, 0)

        def FOR(self):
            return self.getToken(SparkSQLParser.FOR, 0)

        def FOREIGN(self):
            return self.getToken(SparkSQLParser.FOREIGN, 0)

        def FORMAT(self):
            return self.getToken(SparkSQLParser.FORMAT, 0)

        def FORMATTED(self):
            return self.getToken(SparkSQLParser.FORMATTED, 0)

        def FROM(self):
            return self.getToken(SparkSQLParser.FROM, 0)

        def FUNCTION(self):
            return self.getToken(SparkSQLParser.FUNCTION, 0)

        def FUNCTIONS(self):
            return self.getToken(SparkSQLParser.FUNCTIONS, 0)

        def GLOBAL(self):
            return self.getToken(SparkSQLParser.GLOBAL, 0)

        def GRANT(self):
            return self.getToken(SparkSQLParser.GRANT, 0)

        def GROUP(self):
            return self.getToken(SparkSQLParser.GROUP, 0)

        def GROUPING(self):
            return self.getToken(SparkSQLParser.GROUPING, 0)

        def HAVING(self):
            return self.getToken(SparkSQLParser.HAVING, 0)

        def IF(self):
            return self.getToken(SparkSQLParser.IF, 0)

        def IGNORE(self):
            return self.getToken(SparkSQLParser.IGNORE, 0)

        def IMPORT(self):
            return self.getToken(SparkSQLParser.IMPORT, 0)

        def IN(self):
            return self.getToken(SparkSQLParser.IN, 0)

        def INDEX(self):
            return self.getToken(SparkSQLParser.INDEX, 0)

        def INDEXES(self):
            return self.getToken(SparkSQLParser.INDEXES, 0)

        def INPATH(self):
            return self.getToken(SparkSQLParser.INPATH, 0)

        def INPUTFORMAT(self):
            return self.getToken(SparkSQLParser.INPUTFORMAT, 0)

        def INSERT(self):
            return self.getToken(SparkSQLParser.INSERT, 0)

        def INTERVAL(self):
            return self.getToken(SparkSQLParser.INTERVAL, 0)

        def INTO(self):
            return self.getToken(SparkSQLParser.INTO, 0)

        def IS(self):
            return self.getToken(SparkSQLParser.IS, 0)

        def ITEMS(self):
            return self.getToken(SparkSQLParser.ITEMS, 0)

        def KEYS(self):
            return self.getToken(SparkSQLParser.KEYS, 0)

        def LAST(self):
            return self.getToken(SparkSQLParser.LAST, 0)

        def LATERAL(self):
            return self.getToken(SparkSQLParser.LATERAL, 0)

        def LAZY(self):
            return self.getToken(SparkSQLParser.LAZY, 0)

        def LEADING(self):
            return self.getToken(SparkSQLParser.LEADING, 0)

        def LIKE(self):
            return self.getToken(SparkSQLParser.LIKE, 0)

        def LIMIT(self):
            return self.getToken(SparkSQLParser.LIMIT, 0)

        def LINES(self):
            return self.getToken(SparkSQLParser.LINES, 0)

        def LIST(self):
            return self.getToken(SparkSQLParser.LIST, 0)

        def LOAD(self):
            return self.getToken(SparkSQLParser.LOAD, 0)

        def LOCAL(self):
            return self.getToken(SparkSQLParser.LOCAL, 0)

        def LOCATION(self):
            return self.getToken(SparkSQLParser.LOCATION, 0)

        def LOCK(self):
            return self.getToken(SparkSQLParser.LOCK, 0)

        def LOCKS(self):
            return self.getToken(SparkSQLParser.LOCKS, 0)

        def LOGICAL(self):
            return self.getToken(SparkSQLParser.LOGICAL, 0)

        def MACRO(self):
            return self.getToken(SparkSQLParser.MACRO, 0)

        def MAP(self):
            return self.getToken(SparkSQLParser.MAP, 0)

        def MATCHED(self):
            return self.getToken(SparkSQLParser.MATCHED, 0)

        def MERGE(self):
            return self.getToken(SparkSQLParser.MERGE, 0)

        def MSCK(self):
            return self.getToken(SparkSQLParser.MSCK, 0)

        def NAMESPACE(self):
            return self.getToken(SparkSQLParser.NAMESPACE, 0)

        def NAMESPACES(self):
            return self.getToken(SparkSQLParser.NAMESPACES, 0)

        def NO(self):
            return self.getToken(SparkSQLParser.NO, 0)

        def NOT(self):
            return self.getToken(SparkSQLParser.NOT, 0)

        def NULL(self):
            return self.getToken(SparkSQLParser.NULL, 0)

        def NULLS(self):
            return self.getToken(SparkSQLParser.NULLS, 0)

        def OF(self):
            return self.getToken(SparkSQLParser.OF, 0)

        def ONLY(self):
            return self.getToken(SparkSQLParser.ONLY, 0)

        def OPTION(self):
            return self.getToken(SparkSQLParser.OPTION, 0)

        def OPTIONS(self):
            return self.getToken(SparkSQLParser.OPTIONS, 0)

        def OR(self):
            return self.getToken(SparkSQLParser.OR, 0)

        def ORDER(self):
            return self.getToken(SparkSQLParser.ORDER, 0)

        def OUT(self):
            return self.getToken(SparkSQLParser.OUT, 0)

        def OUTER(self):
            return self.getToken(SparkSQLParser.OUTER, 0)

        def OUTPUTFORMAT(self):
            return self.getToken(SparkSQLParser.OUTPUTFORMAT, 0)

        def OVER(self):
            return self.getToken(SparkSQLParser.OVER, 0)

        def OVERLAPS(self):
            return self.getToken(SparkSQLParser.OVERLAPS, 0)

        def OVERLAY(self):
            return self.getToken(SparkSQLParser.OVERLAY, 0)

        def OVERWRITE(self):
            return self.getToken(SparkSQLParser.OVERWRITE, 0)

        def PARTITION(self):
            return self.getToken(SparkSQLParser.PARTITION, 0)

        def PARTITIONED(self):
            return self.getToken(SparkSQLParser.PARTITIONED, 0)

        def PARTITIONS(self):
            return self.getToken(SparkSQLParser.PARTITIONS, 0)

        def PERCENTLIT(self):
            return self.getToken(SparkSQLParser.PERCENTLIT, 0)

        def PIVOT(self):
            return self.getToken(SparkSQLParser.PIVOT, 0)

        def PLACING(self):
            return self.getToken(SparkSQLParser.PLACING, 0)

        def POSITION(self):
            return self.getToken(SparkSQLParser.POSITION, 0)

        def PRECEDING(self):
            return self.getToken(SparkSQLParser.PRECEDING, 0)

        def PRIMARY(self):
            return self.getToken(SparkSQLParser.PRIMARY, 0)

        def PRINCIPALS(self):
            return self.getToken(SparkSQLParser.PRINCIPALS, 0)

        def PROPERTIES(self):
            return self.getToken(SparkSQLParser.PROPERTIES, 0)

        def PURGE(self):
            return self.getToken(SparkSQLParser.PURGE, 0)

        def QUERY(self):
            return self.getToken(SparkSQLParser.QUERY, 0)

        def RANGE(self):
            return self.getToken(SparkSQLParser.RANGE, 0)

        def RECORDREADER(self):
            return self.getToken(SparkSQLParser.RECORDREADER, 0)

        def RECORDWRITER(self):
            return self.getToken(SparkSQLParser.RECORDWRITER, 0)

        def RECOVER(self):
            return self.getToken(SparkSQLParser.RECOVER, 0)

        def REDUCE(self):
            return self.getToken(SparkSQLParser.REDUCE, 0)

        def REFERENCES(self):
            return self.getToken(SparkSQLParser.REFERENCES, 0)

        def REFRESH(self):
            return self.getToken(SparkSQLParser.REFRESH, 0)

        def RENAME(self):
            return self.getToken(SparkSQLParser.RENAME, 0)

        def REPAIR(self):
            return self.getToken(SparkSQLParser.REPAIR, 0)

        def REPLACE(self):
            return self.getToken(SparkSQLParser.REPLACE, 0)

        def RESET(self):
            return self.getToken(SparkSQLParser.RESET, 0)

        def RESTRICT(self):
            return self.getToken(SparkSQLParser.RESTRICT, 0)

        def REVOKE(self):
            return self.getToken(SparkSQLParser.REVOKE, 0)

        def RLIKE(self):
            return self.getToken(SparkSQLParser.RLIKE, 0)

        def ROLE(self):
            return self.getToken(SparkSQLParser.ROLE, 0)

        def ROLES(self):
            return self.getToken(SparkSQLParser.ROLES, 0)

        def ROLLBACK(self):
            return self.getToken(SparkSQLParser.ROLLBACK, 0)

        def ROLLUP(self):
            return self.getToken(SparkSQLParser.ROLLUP, 0)

        def ROW(self):
            return self.getToken(SparkSQLParser.ROW, 0)

        def ROWS(self):
            return self.getToken(SparkSQLParser.ROWS, 0)

        def SCHEMA(self):
            return self.getToken(SparkSQLParser.SCHEMA, 0)

        def SELECT(self):
            return self.getToken(SparkSQLParser.SELECT, 0)

        def SEPARATED(self):
            return self.getToken(SparkSQLParser.SEPARATED, 0)

        def SERDE(self):
            return self.getToken(SparkSQLParser.SERDE, 0)

        def SERDEPROPERTIES(self):
            return self.getToken(SparkSQLParser.SERDEPROPERTIES, 0)

        def SESSION_USER(self):
            return self.getToken(SparkSQLParser.SESSION_USER, 0)

        def SET(self):
            return self.getToken(SparkSQLParser.SET, 0)

        def SETS(self):
            return self.getToken(SparkSQLParser.SETS, 0)

        def SHOW(self):
            return self.getToken(SparkSQLParser.SHOW, 0)

        def SKEWED(self):
            return self.getToken(SparkSQLParser.SKEWED, 0)

        def SOME(self):
            return self.getToken(SparkSQLParser.SOME, 0)

        def SORT(self):
            return self.getToken(SparkSQLParser.SORT, 0)

        def SORTED(self):
            return self.getToken(SparkSQLParser.SORTED, 0)

        def START(self):
            return self.getToken(SparkSQLParser.START, 0)

        def STATISTICS(self):
            return self.getToken(SparkSQLParser.STATISTICS, 0)

        def STORED(self):
            return self.getToken(SparkSQLParser.STORED, 0)

        def STRATIFY(self):
            return self.getToken(SparkSQLParser.STRATIFY, 0)

        def STRUCT(self):
            return self.getToken(SparkSQLParser.STRUCT, 0)

        def SUBSTR(self):
            return self.getToken(SparkSQLParser.SUBSTR, 0)

        def SUBSTRING(self):
            return self.getToken(SparkSQLParser.SUBSTRING, 0)

        def TABLE(self):
            return self.getToken(SparkSQLParser.TABLE, 0)

        def TABLES(self):
            return self.getToken(SparkSQLParser.TABLES, 0)

        def TABLESAMPLE(self):
            return self.getToken(SparkSQLParser.TABLESAMPLE, 0)

        def TBLPROPERTIES(self):
            return self.getToken(SparkSQLParser.TBLPROPERTIES, 0)

        def TEMPORARY(self):
            return self.getToken(SparkSQLParser.TEMPORARY, 0)

        def TERMINATED(self):
            return self.getToken(SparkSQLParser.TERMINATED, 0)

        def THEN(self):
            return self.getToken(SparkSQLParser.THEN, 0)

        def TIME(self):
            return self.getToken(SparkSQLParser.TIME, 0)

        def TO(self):
            return self.getToken(SparkSQLParser.TO, 0)

        def TOUCH(self):
            return self.getToken(SparkSQLParser.TOUCH, 0)

        def TRAILING(self):
            return self.getToken(SparkSQLParser.TRAILING, 0)

        def TRANSACTION(self):
            return self.getToken(SparkSQLParser.TRANSACTION, 0)

        def TRANSACTIONS(self):
            return self.getToken(SparkSQLParser.TRANSACTIONS, 0)

        def TRANSFORM(self):
            return self.getToken(SparkSQLParser.TRANSFORM, 0)

        def TRIM(self):
            return self.getToken(SparkSQLParser.TRIM, 0)

        def TRUE(self):
            return self.getToken(SparkSQLParser.TRUE, 0)

        def TRUNCATE(self):
            return self.getToken(SparkSQLParser.TRUNCATE, 0)

        def TYPE(self):
            return self.getToken(SparkSQLParser.TYPE, 0)

        def UNARCHIVE(self):
            return self.getToken(SparkSQLParser.UNARCHIVE, 0)

        def UNBOUNDED(self):
            return self.getToken(SparkSQLParser.UNBOUNDED, 0)

        def UNCACHE(self):
            return self.getToken(SparkSQLParser.UNCACHE, 0)

        def UNIQUE(self):
            return self.getToken(SparkSQLParser.UNIQUE, 0)

        def UNKNOWN(self):
            return self.getToken(SparkSQLParser.UNKNOWN, 0)

        def UNLOCK(self):
            return self.getToken(SparkSQLParser.UNLOCK, 0)

        def UNSET(self):
            return self.getToken(SparkSQLParser.UNSET, 0)

        def UPDATE(self):
            return self.getToken(SparkSQLParser.UPDATE, 0)

        def USE(self):
            return self.getToken(SparkSQLParser.USE, 0)

        def USER(self):
            return self.getToken(SparkSQLParser.USER, 0)

        def VALUES(self):
            return self.getToken(SparkSQLParser.VALUES, 0)

        def VIEW(self):
            return self.getToken(SparkSQLParser.VIEW, 0)

        def VIEWS(self):
            return self.getToken(SparkSQLParser.VIEWS, 0)

        def WHEN(self):
            return self.getToken(SparkSQLParser.WHEN, 0)

        def WHERE(self):
            return self.getToken(SparkSQLParser.WHERE, 0)

        def WINDOW(self):
            return self.getToken(SparkSQLParser.WINDOW, 0)

        def WITH(self):
            return self.getToken(SparkSQLParser.WITH, 0)

        def ZONE(self):
            return self.getToken(SparkSQLParser.ZONE, 0)

        def getRuleIndex(self):
            return SparkSQLParser.RULE_nonReserved

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNonReserved" ):
                listener.enterNonReserved(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNonReserved" ):
                listener.exitNonReserved(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNonReserved" ):
                return visitor.visitNonReserved(self)
            else:
                return visitor.visitChildren(self)




    def nonReserved(self):

        localctx = SparkSQLParser.NonReservedContext(self, self._ctx, self.state)
        self.enterRule(localctx, 268, self.RULE_nonReserved)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2996
            _la = self._input.LA(1)
            if not(((((_la - 12)) & ~0x3f) == 0 and ((1 << (_la - 12)) & ((1 << (SparkSQLParser.ADD - 12)) | (1 << (SparkSQLParser.AFTER - 12)) | (1 << (SparkSQLParser.ALL - 12)) | (1 << (SparkSQLParser.ALTER - 12)) | (1 << (SparkSQLParser.ANALYZE - 12)) | (1 << (SparkSQLParser.AND - 12)) | (1 << (SparkSQLParser.ANY - 12)) | (1 << (SparkSQLParser.ARCHIVE - 12)) | (1 << (SparkSQLParser.ARRAY - 12)) | (1 << (SparkSQLParser.AS - 12)) | (1 << (SparkSQLParser.ASC - 12)) | (1 << (SparkSQLParser.AT - 12)) | (1 << (SparkSQLParser.AUTHORIZATION - 12)) | (1 << (SparkSQLParser.BETWEEN - 12)) | (1 << (SparkSQLParser.BOTH - 12)) | (1 << (SparkSQLParser.BUCKET - 12)) | (1 << (SparkSQLParser.BUCKETS - 12)) | (1 << (SparkSQLParser.BY - 12)) | (1 << (SparkSQLParser.CACHE - 12)) | (1 << (SparkSQLParser.CASCADE - 12)) | (1 << (SparkSQLParser.CASE - 12)) | (1 << (SparkSQLParser.CAST - 12)) | (1 << (SparkSQLParser.CHANGE - 12)) | (1 << (SparkSQLParser.CHECK - 12)) | (1 << (SparkSQLParser.CLEAR - 12)) | (1 << (SparkSQLParser.CLUSTER - 12)) | (1 << (SparkSQLParser.CLUSTERED - 12)) | (1 << (SparkSQLParser.CODEGEN - 12)) | (1 << (SparkSQLParser.COLLATE - 12)) | (1 << (SparkSQLParser.COLLECTION - 12)) | (1 << (SparkSQLParser.COLUMN - 12)) | (1 << (SparkSQLParser.COLUMNS - 12)) | (1 << (SparkSQLParser.COMMENT - 12)) | (1 << (SparkSQLParser.COMMIT - 12)) | (1 << (SparkSQLParser.COMPACT - 12)) | (1 << (SparkSQLParser.COMPACTIONS - 12)) | (1 << (SparkSQLParser.COMPUTE - 12)) | (1 << (SparkSQLParser.CONCATENATE - 12)) | (1 << (SparkSQLParser.CONSTRAINT - 12)) | (1 << (SparkSQLParser.COST - 12)) | (1 << (SparkSQLParser.CREATE - 12)) | (1 << (SparkSQLParser.CUBE - 12)) | (1 << (SparkSQLParser.CURRENT - 12)) | (1 << (SparkSQLParser.CURRENT_DATE - 12)) | (1 << (SparkSQLParser.CURRENT_TIME - 12)) | (1 << (SparkSQLParser.CURRENT_TIMESTAMP - 12)) | (1 << (SparkSQLParser.CURRENT_USER - 12)) | (1 << (SparkSQLParser.DATA - 12)) | (1 << (SparkSQLParser.DATABASE - 12)) | (1 << (SparkSQLParser.DATABASES - 12)) | (1 << (SparkSQLParser.DBPROPERTIES - 12)) | (1 << (SparkSQLParser.DEFINED - 12)) | (1 << (SparkSQLParser.DELETE - 12)) | (1 << (SparkSQLParser.DELIMITED - 12)) | (1 << (SparkSQLParser.DESC - 12)) | (1 << (SparkSQLParser.DESCRIBE - 12)) | (1 << (SparkSQLParser.DFS - 12)) | (1 << (SparkSQLParser.DIRECTORIES - 12)) | (1 << (SparkSQLParser.DIRECTORY - 12)) | (1 << (SparkSQLParser.DISTINCT - 12)) | (1 << (SparkSQLParser.DISTRIBUTE - 12)) | (1 << (SparkSQLParser.DIV - 12)))) != 0) or ((((_la - 76)) & ~0x3f) == 0 and ((1 << (_la - 76)) & ((1 << (SparkSQLParser.DROP - 76)) | (1 << (SparkSQLParser.ELSE - 76)) | (1 << (SparkSQLParser.END - 76)) | (1 << (SparkSQLParser.ESCAPE - 76)) | (1 << (SparkSQLParser.ESCAPED - 76)) | (1 << (SparkSQLParser.EXCHANGE - 76)) | (1 << (SparkSQLParser.EXISTS - 76)) | (1 << (SparkSQLParser.EXPLAIN - 76)) | (1 << (SparkSQLParser.EXPORT - 76)) | (1 << (SparkSQLParser.EXTENDED - 76)) | (1 << (SparkSQLParser.EXTERNAL - 76)) | (1 << (SparkSQLParser.EXTRACT - 76)) | (1 << (SparkSQLParser.FALSE - 76)) | (1 << (SparkSQLParser.FETCH - 76)) | (1 << (SparkSQLParser.FIELDS - 76)) | (1 << (SparkSQLParser.FILTER - 76)) | (1 << (SparkSQLParser.FILEFORMAT - 76)) | (1 << (SparkSQLParser.FIRST - 76)) | (1 << (SparkSQLParser.FOLLOWING - 76)) | (1 << (SparkSQLParser.FOR - 76)) | (1 << (SparkSQLParser.FOREIGN - 76)) | (1 << (SparkSQLParser.FORMAT - 76)) | (1 << (SparkSQLParser.FORMATTED - 76)) | (1 << (SparkSQLParser.FROM - 76)) | (1 << (SparkSQLParser.FUNCTION - 76)) | (1 << (SparkSQLParser.FUNCTIONS - 76)) | (1 << (SparkSQLParser.GLOBAL - 76)) | (1 << (SparkSQLParser.GRANT - 76)) | (1 << (SparkSQLParser.GROUP - 76)) | (1 << (SparkSQLParser.GROUPING - 76)) | (1 << (SparkSQLParser.HAVING - 76)) | (1 << (SparkSQLParser.IF - 76)) | (1 << (SparkSQLParser.IGNORE - 76)) | (1 << (SparkSQLParser.IMPORT - 76)) | (1 << (SparkSQLParser.IN - 76)) | (1 << (SparkSQLParser.INDEX - 76)) | (1 << (SparkSQLParser.INDEXES - 76)) | (1 << (SparkSQLParser.INPATH - 76)) | (1 << (SparkSQLParser.INPUTFORMAT - 76)) | (1 << (SparkSQLParser.INSERT - 76)) | (1 << (SparkSQLParser.INTERVAL - 76)) | (1 << (SparkSQLParser.INTO - 76)) | (1 << (SparkSQLParser.IS - 76)) | (1 << (SparkSQLParser.ITEMS - 76)) | (1 << (SparkSQLParser.KEYS - 76)) | (1 << (SparkSQLParser.LAST - 76)) | (1 << (SparkSQLParser.LATERAL - 76)) | (1 << (SparkSQLParser.LAZY - 76)) | (1 << (SparkSQLParser.LEADING - 76)) | (1 << (SparkSQLParser.LIKE - 76)) | (1 << (SparkSQLParser.LIMIT - 76)) | (1 << (SparkSQLParser.LINES - 76)) | (1 << (SparkSQLParser.LIST - 76)) | (1 << (SparkSQLParser.LOAD - 76)) | (1 << (SparkSQLParser.LOCAL - 76)) | (1 << (SparkSQLParser.LOCATION - 76)) | (1 << (SparkSQLParser.LOCK - 76)) | (1 << (SparkSQLParser.LOCKS - 76)))) != 0) or ((((_la - 140)) & ~0x3f) == 0 and ((1 << (_la - 140)) & ((1 << (SparkSQLParser.LOGICAL - 140)) | (1 << (SparkSQLParser.MACRO - 140)) | (1 << (SparkSQLParser.MAP - 140)) | (1 << (SparkSQLParser.MATCHED - 140)) | (1 << (SparkSQLParser.MERGE - 140)) | (1 << (SparkSQLParser.MSCK - 140)) | (1 << (SparkSQLParser.NAMESPACE - 140)) | (1 << (SparkSQLParser.NAMESPACES - 140)) | (1 << (SparkSQLParser.NO - 140)) | (1 << (SparkSQLParser.NOT - 140)) | (1 << (SparkSQLParser.NULL - 140)) | (1 << (SparkSQLParser.NULLS - 140)) | (1 << (SparkSQLParser.OF - 140)) | (1 << (SparkSQLParser.ONLY - 140)) | (1 << (SparkSQLParser.OPTION - 140)) | (1 << (SparkSQLParser.OPTIONS - 140)) | (1 << (SparkSQLParser.OR - 140)) | (1 << (SparkSQLParser.ORDER - 140)) | (1 << (SparkSQLParser.OUT - 140)) | (1 << (SparkSQLParser.OUTER - 140)) | (1 << (SparkSQLParser.OUTPUTFORMAT - 140)) | (1 << (SparkSQLParser.OVER - 140)) | (1 << (SparkSQLParser.OVERLAPS - 140)) | (1 << (SparkSQLParser.OVERLAY - 140)) | (1 << (SparkSQLParser.OVERWRITE - 140)) | (1 << (SparkSQLParser.PARTITION - 140)) | (1 << (SparkSQLParser.PARTITIONED - 140)) | (1 << (SparkSQLParser.PARTITIONS - 140)) | (1 << (SparkSQLParser.PERCENTLIT - 140)) | (1 << (SparkSQLParser.PIVOT - 140)) | (1 << (SparkSQLParser.PLACING - 140)) | (1 << (SparkSQLParser.POSITION - 140)) | (1 << (SparkSQLParser.PRECEDING - 140)) | (1 << (SparkSQLParser.PRIMARY - 140)) | (1 << (SparkSQLParser.PRINCIPALS - 140)) | (1 << (SparkSQLParser.PROPERTIES - 140)) | (1 << (SparkSQLParser.PURGE - 140)) | (1 << (SparkSQLParser.QUERY - 140)) | (1 << (SparkSQLParser.RANGE - 140)) | (1 << (SparkSQLParser.RECORDREADER - 140)) | (1 << (SparkSQLParser.RECORDWRITER - 140)) | (1 << (SparkSQLParser.RECOVER - 140)) | (1 << (SparkSQLParser.REDUCE - 140)) | (1 << (SparkSQLParser.REFERENCES - 140)) | (1 << (SparkSQLParser.REFRESH - 140)) | (1 << (SparkSQLParser.RENAME - 140)) | (1 << (SparkSQLParser.REPAIR - 140)) | (1 << (SparkSQLParser.REPLACE - 140)) | (1 << (SparkSQLParser.RESET - 140)) | (1 << (SparkSQLParser.RESTRICT - 140)) | (1 << (SparkSQLParser.REVOKE - 140)) | (1 << (SparkSQLParser.RLIKE - 140)) | (1 << (SparkSQLParser.ROLE - 140)) | (1 << (SparkSQLParser.ROLES - 140)) | (1 << (SparkSQLParser.ROLLBACK - 140)) | (1 << (SparkSQLParser.ROLLUP - 140)) | (1 << (SparkSQLParser.ROW - 140)) | (1 << (SparkSQLParser.ROWS - 140)) | (1 << (SparkSQLParser.SCHEMA - 140)) | (1 << (SparkSQLParser.SELECT - 140)))) != 0) or ((((_la - 204)) & ~0x3f) == 0 and ((1 << (_la - 204)) & ((1 << (SparkSQLParser.SEPARATED - 204)) | (1 << (SparkSQLParser.SERDE - 204)) | (1 << (SparkSQLParser.SERDEPROPERTIES - 204)) | (1 << (SparkSQLParser.SESSION_USER - 204)) | (1 << (SparkSQLParser.SET - 204)) | (1 << (SparkSQLParser.SETS - 204)) | (1 << (SparkSQLParser.SHOW - 204)) | (1 << (SparkSQLParser.SKEWED - 204)) | (1 << (SparkSQLParser.SOME - 204)) | (1 << (SparkSQLParser.SORT - 204)) | (1 << (SparkSQLParser.SORTED - 204)) | (1 << (SparkSQLParser.START - 204)) | (1 << (SparkSQLParser.STATISTICS - 204)) | (1 << (SparkSQLParser.STORED - 204)) | (1 << (SparkSQLParser.STRATIFY - 204)) | (1 << (SparkSQLParser.STRUCT - 204)) | (1 << (SparkSQLParser.SUBSTR - 204)) | (1 << (SparkSQLParser.SUBSTRING - 204)) | (1 << (SparkSQLParser.TABLE - 204)) | (1 << (SparkSQLParser.TABLES - 204)) | (1 << (SparkSQLParser.TABLESAMPLE - 204)) | (1 << (SparkSQLParser.TBLPROPERTIES - 204)) | (1 << (SparkSQLParser.TEMPORARY - 204)) | (1 << (SparkSQLParser.TERMINATED - 204)) | (1 << (SparkSQLParser.THEN - 204)) | (1 << (SparkSQLParser.TIME - 204)) | (1 << (SparkSQLParser.TO - 204)) | (1 << (SparkSQLParser.TOUCH - 204)) | (1 << (SparkSQLParser.TRAILING - 204)) | (1 << (SparkSQLParser.TRANSACTION - 204)) | (1 << (SparkSQLParser.TRANSACTIONS - 204)) | (1 << (SparkSQLParser.TRANSFORM - 204)) | (1 << (SparkSQLParser.TRIM - 204)) | (1 << (SparkSQLParser.TRUE - 204)) | (1 << (SparkSQLParser.TRUNCATE - 204)) | (1 << (SparkSQLParser.TYPE - 204)) | (1 << (SparkSQLParser.UNARCHIVE - 204)) | (1 << (SparkSQLParser.UNBOUNDED - 204)) | (1 << (SparkSQLParser.UNCACHE - 204)) | (1 << (SparkSQLParser.UNIQUE - 204)) | (1 << (SparkSQLParser.UNKNOWN - 204)) | (1 << (SparkSQLParser.UNLOCK - 204)) | (1 << (SparkSQLParser.UNSET - 204)) | (1 << (SparkSQLParser.UPDATE - 204)) | (1 << (SparkSQLParser.USE - 204)) | (1 << (SparkSQLParser.USER - 204)) | (1 << (SparkSQLParser.VALUES - 204)) | (1 << (SparkSQLParser.VIEW - 204)) | (1 << (SparkSQLParser.VIEWS - 204)) | (1 << (SparkSQLParser.WHEN - 204)) | (1 << (SparkSQLParser.WHERE - 204)) | (1 << (SparkSQLParser.WINDOW - 204)) | (1 << (SparkSQLParser.WITH - 204)) | (1 << (SparkSQLParser.ZONE - 204)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx



    def sempred(self, localctx:RuleContext, ruleIndex:int, predIndex:int):
        if self._predicates == None:
            self._predicates = dict()
        self._predicates[41] = self.queryTerm_sempred
        self._predicates[95] = self.booleanExpression_sempred
        self._predicates[97] = self.valueExpression_sempred
        self._predicates[98] = self.primaryExpression_sempred
        pred = self._predicates.get(ruleIndex, None)
        if pred is None:
            raise Exception("No predicate with index:" + str(ruleIndex))
        else:
            return pred(localctx, predIndex)

    def queryTerm_sempred(self, localctx:QueryTermContext, predIndex:int):
            if predIndex == 0:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 1:
                return self.precpred(self._ctx, 1)
         

    def booleanExpression_sempred(self, localctx:BooleanExpressionContext, predIndex:int):
            if predIndex == 2:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 3:
                return self.precpred(self._ctx, 1)
         

    def valueExpression_sempred(self, localctx:ValueExpressionContext, predIndex:int):
            if predIndex == 4:
                return self.precpred(self._ctx, 6)
         

            if predIndex == 5:
                return self.precpred(self._ctx, 5)
         

            if predIndex == 6:
                return self.precpred(self._ctx, 4)
         

            if predIndex == 7:
                return self.precpred(self._ctx, 3)
         

            if predIndex == 8:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 9:
                return self.precpred(self._ctx, 1)
         

    def primaryExpression_sempred(self, localctx:PrimaryExpressionContext, predIndex:int):
            if predIndex == 10:
                return self.precpred(self._ctx, 8)
         

            if predIndex == 11:
                return self.precpred(self._ctx, 6)
         




